# Disease-spatial patterns {#Disease}

This chapter introduces disease mapping and contains the theory for spatial lattice processes and models for performing smoothing of risks over space. From this chapter, the reader will have gained an understanding of the following topics:

- Disease mapping, where we have seen how to improve estimates of risk by borrowing strength from adjacent regions which can reduce the instability inherent in risk estimates (SMRs) based on small expected numbers.
- Seen how smoothing can be performed using either the empirical Bayes or fully Bayesian approaches.
- Been introduced to computational methods for handling areal data.
- Learned about Besag’s seminal contributions to the field of spatial statistics including the very important concept of a Markov random field.
- Explored approaches to modelling a real data including the conditional auto regressive models.
- Seen how Bayesian spatial models for lattice data use `nimble`, `stan`, `R` and `R–INLA`. 

## Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 {-}

### Nimble {-}

Following Example 6.2 we look back at the hospital admission rates for COPD, in England for 2010. We can implement the same model we used on Example 6.2 using `nimble`.

```{r Ex 9.1 nimble load, message=FALSE, echo = TRUE, warning=FALSE, message= FALSE}

# Load nimble
library(nimble)
library(sf) # to read shapefile

# Load data
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv", row.names = 1)

```

The following is the code for the Poisson-Gamma model implemented in `nimble`.

```{r Ex 9.1 nimble model, results='hide', warning=FALSE, message= FALSE}

Example9_1Nimble <- nimbleCode({
  for (i in 1:N) {
    Y[i] ~ dpois(mu[i])
    # REVIEW: There is an intercept in the book, 
    # but then we wouldn't be able to compare it to example 5.2
    mu[i] <- E[i] * theta[i]
    # REVIEW: Same as before, the example in the book has theta[i] ~ Ga(a,a)
    theta[i] ~ dgamma(a, b)
    Y.fit[i] ~ dpois(mu[i])
  }
  # Priors
  a ~ dexp(lambda_a)
  b ~ dexp(lambda_b)

})

```

Define the constants, data and initials lists for the `nimble` model.

```{r Ex 9.1 nimble set constants and inits, error = FALSE, message = FALSE, results='hide'}

# observations
y <- observed$Y2010
# offset
E <- expected$E2010
N <- length(y)
# parameter of exponential prior for a
lambda_a <- 1
# parameter of exponential prior for b
lambda_b <- 1
# constants list
constants <- list(
  N = N,
  E = E,
  lambda_a = lambda_a,
  lambda_b = lambda_b
)
# data list
ex.data <- list(Y = y)
# initial values list
inits <-
    list(
      theta = rgamma(N, 1, 1),
      a = rexp(1, lambda_a),
      b = rexp(1, lambda_b),
      Y.fit = rpois(N, E)
    )
# parameters to monitor
params <- c("theta", "a", "b", "Y.fit")

# Run model in nimble
mcmc.out <- nimbleMCMC(
  code = Example9_1Nimble,
  constants = constants,
  data = ex.data,
  inits = inits,
  monitors = params,
  niter = 50000,
  nburnin = 20000,
  thin = 30,
  WAIC = TRUE,
  nchains = 2,
  summary = TRUE,
  samplesAsCodaMCMC = TRUE
)

```



Show the WAIC, effective sample size, and trace plots for some of the parameters.

```{r Ex 9.1 nimble WAIC, ESS and traceplots}

mcmc.out$WAIC
min(coda::effectiveSize(mcmc.out$samples))
mvSamples <- mcmc.out$samples

# trace plot of a
plot(mvSamples[, c("a")], bty = "n")
# trace plot of b
plot(mvSamples[, c("b")], bty = "n")
# trace plots of theta
for (i in 1:3)
  plot(mvSamples[, c(paste("theta[", i, "]", sep = ""))], bty = "n")

```

Now that we have checked the convergence of the chains we can plot the posterior mean and 95% CIs for each of the parameters.


```{r Ex 9.1 nimble posterior summary}
# Print posterior summary for parameters a and b
summary(mvSamples[, c("a", "b")])
# posterior summaries of theta_i
post_summary <- mcmc.out$summary$all.chains |> as.data.frame() |>
  tibble::rownames_to_column("variable")
# pot the mean and 95% CIs for the thetas
post_theta <-  post_summary[grepl("theta\\[", post_summary$variable), ]

par(mfrow = c(1, 1))
plot(
  post_theta$Mean,
  pch = 19,
  cex = 0.8,
  bty = "n",
  xlab = "Borough",
  ylab = "Posterior Summary Rate",
  ylim = c(min(post_theta$`95%CI_low`), max(post_theta$`95%CI_upp`))
)
for (i in 1:N)
  segments(i, post_theta$`95%CI_low`[i], i, post_theta$`95%CI_upp`[i])
abline(h = 1, lwd = 2, lty = 2)

# posterior summary of fitted values
post_fitted <-  post_summary[grepl("Y.fit\\[", post_summary$variable), ]
# plot mean and 95% CIs for the fitted values
par(mfrow = c(1, 1))
plot(
  y,
  post_fitted$Mean,
  ylim = c(min(post_fitted$`95%CI_low`), max(post_fitted$`95%CI_upp`)),
  xlab = "Observed",
  ylab = "Fitted",
  pch = 19,
  cex = 0.7,
  bty = "n"
)
for (i in 1:N)
  segments(y[i], post_fitted$`95%CI_low`[i], y[i], post_fitted$`95%CI_upp`[i])
abline(a = 0, b = 1)

```

### Stan {-}

Load necessary libraries and data.  


```{r Ex 9.1 clean, include = FALSE}
rm(list=ls())
```

```{r Ex 9.1 stan load, message=FALSE, echo = TRUE, warning=FALSE, message= FALSE}

library(rstan) 
library(sf) # to read shapefile
library(loo) # To calculate WAIC
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load data
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv", row.names = 1)

```

Write the `stan` model. This model is in a separate file called `Example8_1.stan` that will be called later.

```{r engine='bash', comment='', echo = FALSE}
cat functions/Example9_1.stan
``` 


Define the data for the model, similar to `nimble`. 

```{r Ex 8.1 stan define data, error = FALSE, message = FALSE, results='hide'}
# observations
y <- observed$Y2010
# offset
E <- expected$E2010
N <- length(y)
# data list 
ex.data <- list(
  N = length(y),
  Y = y,
  E = E,
  lambda_a = 1,
  lambda_b = 1
)
# Run the model in Stan
Ex9_1Stan <- stan(
  file = "functions/Example9_1.stan",
  data = ex.data,
  chains = 3,
  iter = 10000,
  warmup = 3000,
  thin = 14,
  # QUESTION: should we explain this?
  control = list(adapt_delta = 0.8, max_treedepth = 15), 
  init = "random",
  pars = c("a", "b", "theta", "log_lik", "yfit"),
  include = TRUE
)

```

Compute the WAIC, show the trace plots and posterior summaries of the parameters.

```{r Ex 8.1 stan waic and traceplots, message= FALSE, warning=FALSE}

loglik0 <- extract_log_lik(Ex9_1Stan)
waic0 <- waic(loglik0)
waic0

# traceplots of parameters a and b
traceplot(Ex9_1Stan, pars = c("a", "b"))
# traceplots of parameter theta
traceplot(Ex9_1Stan, pars = c("theta[1]", "theta[2]", "theta[3]"))

```


```{r Ex 8.1 stan posterior summary}

summary_theta <-
  summary(Ex9_1Stan, pars = c("theta"), probs = c(0.05, 0.95))$summary |> as.data.frame()

par(mfrow = c(1, 1))
plot(
  summary_theta$mean,
  pch = 19,
  cex = 0.8,
  bty = "n",
  xlab = "Borough",
  ylab = "Posterior Summary Rate",
  ylim = c(min(summary_theta$`5%`), max(summary_theta$`95%`))
)
for (i in 1:N)
  segments(i, summary_theta$`5%`[i], i, summary_theta$`95%`[i])
abline(h = 1, lwd = 2, lty = 2)

# Posterior summary of fitted values
summary_fit <-
  summary(Ex9_1Stan, pars = c("yfit"), probs = c(0.05, 0.95))$summary |> as.data.frame()
# Plot mean and 95% CIs for the fitted values
par(mfrow = c(1, 1))
plot(
  y,
  summary_fit$mean,
  ylim = c(min(summary_fit$`5%`), max(summary_fit$`95%`)),
  xlab = "Observed",
  ylab = "Fitted",
  pch = 19,
  cex = 0.7,
  bty = "n"
)
for (i in 1:N)
  segments(y[i], summary_fit$`5%`[i], y[i], summary_fit$`95%`[i])
abline(a = 0, b = 1)
```

## Example 9.3: Fitting a conditional spatial model in `nimble` and `stan` {-}

### Nimble {-}

In this example we revisit the COPD data from example 6.3.

```{r Ex 9.3 nimble clean, include = FALSE}
rm(list=ls())
```


```{r Ex 9.3 nimble load, message=FALSE, warning=FALSE, message= FALSE}

library(ggplot2) # to plot map
library(spdep) # read the shapefile (read_sf) and build neighbors list (poly2nb)
library(nimble)
# Add a theme to clear axes and background in ggplot (optional)
theme_clear <- function(){
  theme(
    panel.grid.major = element_blank(), # remove background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank(), # remove grey background
    axis.line = element_line(colour = "black") # remove axis line
  )
}

# Load data
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copdavgscore.csv")
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )

```

Analyze the relationship between COPD data and the average deprivation score which is a measure of the socioeconomic status of the patients. 

```{r Ex 9.3 plot copd and avg score}

copd_df$SMR2010 = copd_df$Y2010 / copd_df$E2010

ggplot(copd_df) + geom_point(aes(x = Average.Score, y = SMR2010)) + theme_clear() + ylab("SMR 2010") + xlab("Average deprivation score")

```


Define a function to obtain the number of neighbors. 

```{r Ex 9.3 nimble munge functions, echo = TRUE}

adjlist = function(W, N) {
  adj = 0
  for (i in 1:N) {
    for (j in 1:N) {
      if (W[i, j] == 1) {
        adj = append(adj, j)
      }
    }
  }
  adj = adj[-1]
  return(adj)
}

```

Define the adjacency matrix and indexes for `stan` using the `nb2mat` and `adjlist`

```{r Ex 9.3 nimble neighborhood}

# Create the neighborhood
W.nb <-
  poly2nb(england, row.names = rownames(england))
# Creates a matrix for following function call
W.mat <- nb2mat(W.nb, style = "B")
# Define the spatial structure to use in stan
N <- length(unique(england$ID))
neigh  <- adjlist(W.mat, N)
numneigh  <- apply(W.mat,2,sum)

```

```{r Ex 9.3 nimble car model, warning=FALSE}

Example9_3Nimble <- nimbleCode({
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dpois(lambda[i])
    log(lambda[i]) <- log(E[i]) + b[i] + beta0 + beta1*x[i]
  }
  
  # Priors
  beta0 ~ dnorm(0, sd = 1)
  beta1 ~ dnorm(0, sd = 1)
  b[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1)
  tau <- 1 / (sigma_b ^ 2)
  sigma_b ~ T(dnorm(0, sd = 1), 0,)
  # Fitted values and likelihood for WAIC
  for (i in 1:N) {
    fitted[i] ~ dpois(lambda[i])
  }
})

```

Define the constants, data and initial values lists and run the model. 

```{r Ex 9.3 nimble car run, error = FALSE, message = FALSE, results='hide'}
# constants list
constants <- list(
  N = N,
  E = copd_df$E2010,
  L = length(neigh),
  adj = neigh,
  weights = rep(1, length(neigh)),
  num = as.vector(numneigh),
  p = 3
)
# data list
ex.data <-
  list(y = copd_df$Y2010, 
       x = as.vector(scale(copd_df$Average.Score))) # vector of covariates
inits <-  list(
  beta0 = rnorm(1),
  beta1 = rnorm(1),
  fitted = rpois(N, 2),
  sigma_b = 1,
  b = rnorm(N)
)
params <- c("beta0","beta1", "fitted", "b", "sigma_b")
# Run model in nimble
mcmc.out <- nimbleMCMC(
  code = Example9_3Nimble,
  constants = constants,
  data = ex.data,
  inits = inits,
  monitors = params,
  niter = 40000,
  nburnin = 20000,
  thin = 80,
  WAIC = TRUE,
  nchains = 2,
  summary = TRUE,
  samplesAsCodaMCMC = TRUE
)

```

Show the WAIC, effective sample size, and trace plots for some of the parameters.


```{r Ex 9.3 WAIC, ESS and traceplots }

mcmc.out$WAIC
min(coda::effectiveSize(mcmc.out$samples))
plot(mcmc.out$samples[, c("beta0")], bty = "n")
plot(mcmc.out$samples[, c("beta1")], bty = "n")
plot(mcmc.out$samples[, c("b[2]")], bty = "n")
plot(mcmc.out$samples[, c("sigma_b")], bty = "n")
```


```{r Ex 8.3 nimble CAR summary, echo = TRUE}

# Extract samples
variables <- c("beta0", "beta1","sigma_b")
summary_CAR_nimble <- mcmc.out$summary$all.chains
summary_CAR_nimble[variables,]

```

Map the mean of the posterior estimate for the latent effect.

```{r plot map nimble}

samples_CAR_b <-
  summary_CAR_nimble[grepl("b\\[", rownames(summary_CAR_nimble)), ] |> as.data.frame()
observed <- tibble::rownames_to_column(observed, "ID")
samples_CAR_b$ID <- observed$ID
CAR_nimble_merge <- merge(england, samples_CAR_b, by = "ID")
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = CAR_nimble_merge, aes(fill = Mean)) +
  # Change legend's label
  labs(fill = 'Latent effects nimble') +
  # Clear background and plot borders
 theme_void()
```


### Stan {-}

Here, we implement the CAR model using `stan`.

```{r Ex 9.3 stan clean, include = FALSE}
rm(list=ls())
```


```{r Ex 9.3 stan load, message=FALSE, echo = FALSE, warning=FALSE, message= FALSE}

library(ggplot2)
library(loo) # To calculate WAIC and loo later
library(rstan) 
library(spdep) 
# Add a theme to clear axes and background in ggplot (optional)
theme_clear <- function(){
  theme(
    panel.grid.major = element_blank(), # remove background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank(), # remove grey background
    axis.line = element_blank() # remove axis line
  )
}
# Additional settings for stan
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

# Load data
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copdavgscore.csv")
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )

```


We will need two functions to structure the matrix of neighbors that will be needed in `stan`.

```{r Ex 9.3 stan munge functions, echo = TRUE}

adjlist = function(W, N) {
  adj = 0
  for (i in 1:N) {
    for (j in 1:N) {
      if (W[i, j] == 1) {
        adj = append(adj, j)
      }
    }
  }
  adj = adj[-1]
  return(adj)
}

mungeCARdata4stan = function(adjBUGS, numBUGS) {
  N = length(numBUGS)
  nn = numBUGS
  N_edges = length(adjBUGS) / 2
  node1 = vector(mode = "numeric", length = N_edges)
  node2 = vector(mode = "numeric", length = N_edges)
  iAdj = 0
  iEdge = 0
  
  for (i in 1:N) {
    for (j in 1:nn[i]) {
      iAdj = iAdj + 1
      if (i < adjBUGS[iAdj]) {
        iEdge = iEdge + 1
        node1[iEdge] = i
        node2[iEdge] = adjBUGS[iAdj]
      }
    }
  }
  return (list(
    "N" = N,
    "N_edges" = N_edges,
    "node1" = node1,
    "node2" = node2
  ))
  
}
```


<!-- REVIEW: I am missing the adjacency matrix but I calculated like they did for the CARBayes example, is that right? -->

Define the adjacency matrix and indexes for `stan` using the `nb2mat` and `adjlist`

```{r Ex 9.3 stan neighborhood}

# Create the neighborhood
W.nb <-
  poly2nb(england, row.names = rownames(england))
# Creates a matrix for following function call
W.mat <- nb2mat(W.nb, style = "B")
# Define the spatial structure to use in stan
N <- length(unique(england$ID))
neigh <- adjlist(W.mat, N)
numneigh  <-  apply(W.mat, 2, sum)
nbs <- mungeCARdata4stan(neigh, numneigh)
N <- nbs$N
node1 <- nbs$node1
node2 <- nbs$node2
N_edges <- nbs$N_edges

```


`Stan` model for CAR effects. 
This model is in a separate file called `Example8_3.stan` that will be called later.

```{r engine='bash', comment='', echo = FALSE}
cat functions/Example9_3.stan
``` 


```{r Ex 9.3 stan car effects, warning=FALSE}

N_edges <- N_edges
node1 <- node1
node2 <- node2
N <- N
y <- copd_df$Y2010
E <- copd_df$E2010
X <- as.numeric(scale(copd_df$Average.Score))

ex.data <- list(
  N = N,
  y = y,
  E = E,
  p = 1,
  X = as.matrix(X),
  N_edges = N_edges,
  node1 = node1,
  node2 = node2
)

Example9_3Stan  <- stan(
  file = "functions/Example9_3.stan",
  data = ex.data,
  warmup = 10000,
  iter = 20000,
  chains = 2,
  thin = 10,
  pars = c("beta0", "beta","sigma_s", "b", "log_lik"),
  include = TRUE
)

```


Show traceplots.

```{r Ex 9.3 stan CAR traceplots}
#computing WAIC using the package loo

loglikcar <- extract_log_lik(Example9_3Stan)
waiccar <- waic(loglikcar)
waiccar

traceplot(Example9_3Stan, pars = c("beta0","beta","sigma_s"))

```

Show the posterior summary for the parameters if interest. 
Keep in mind that in the stan model the we are sampling the standard deviation of the random effect unlike `CARBayes` where the variance is obtained.

```{r Ex 9.3 stan CAR results, echo = TRUE}

# Extract samples
summary_CAR_stan <-
  summary(
    Example9_3Stan,
    pars = c("beta0", "beta","sigma_s"),
    probs = c(0.025, 0.975)
  )

summary_CAR_stan$summary

```



Map the mean of the posterior estimate for the latent effect.

```{r Ex 9.3 plot map stan}

summary_CAR_stan_b <-
  summary(
    Example9_3Stan,
    pars = c("b"),
    probs = c(0.025, 0.975)
  )

observed <- tibble::rownames_to_column(observed, "ID")

summary_CAR_stan_b$ID <- observed$ID

CAR_stan_merge <- merge(england, summary_CAR_stan_b, by = "ID") 

ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = CAR_stan_merge, aes(fill = summary.mean)) + 
  # Change legend's label
  labs(fill = 'Latent effects stan') +
  # Clear background and plot borders
  theme_void()


```

<!-- IDEA: Maybe we should agree on what output we will show for every method -->

## Example 9.4: Fitting a conditional spatial model using CARBayes {-}


```{r Ex 9.4 clean, include=FALSE}
rm(list=ls())
```

Load the necessary libraries and the COPD data. As in example 5.2 `englandlocalauthority.shp` and it's related files contain the location, shape, and attributes of English local authorities. 


```{r Ex 9.4 carbayes load, message=FALSE, warning=FALSE, message= FALSE}

library(CARBayes)
library(ggplot2)
library(sf)
library(spdep)
# Add a theme to clear axes and background in ggplot (optional)
theme_clear <- function(){
  theme(
    panel.grid.major = element_blank(), # remove background grid
    panel.grid.minor = element_blank(),
    panel.background = element_blank(), # remove grey background
    axis.line = element_blank()# remove axis line
  )
}
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copdavgscore.csv") 
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )
copd_df$Average.Score <- as.numeric(scale(copd_df$Average.Score))

```


To calculate the smoother SMRs, we first need to create a *neighborhood* structure. The functions `poly2nb()` and `nb2mat()` from the `spdep` package can be used to create this

```{r Ex 9.4 carbayes neighborhood}

# Create the neighborhood
W.nb <-
  poly2nb(england, row.names = rownames(england))

# Creates a matrix for following function call
W.mat <- nb2mat(W.nb, style = "B")

```

Here, we use *first neighbors* to define the structure, so any local authority sharing a border are considered neighbors.

The function `S.CARleroux()` allows us to use the neighborhood structure and performs a Bayesian analysis to create a smoothed set of observed values.


```{r Ex 9.4 carbayes run, results='hide', message = FALSE}

# Running smoothing model
Ex9_4 <-
  S.CARleroux(
    # Model Formula
    formula = Y2010 ~ offset(log(E2010)) + Average.Score,
    # data frame with data
    data = copd_df,
    # Choosing Poisson Regression
    family = "poisson",
    # Neighborhood matrix
    W = W.mat,
    # Number of burn in samples
    burnin = 20000,
    # Number of MCMC samples
    n.sample = 100000,
    thin = 10,
    rho = 1
  )

```

We can extract the new smoother values from the model output and divide them by the expected values in order to compare both methods. 


```{r Ex 9.4 carbayes smoothed SMRs, class.source = 'foldable' }

# Creating a dataset with smoothed SMRs in 2010
SMR2010 <- Ex9_4$fitted.values /  copd_df$E2010
SMR_smooth <- as.data.frame(SMR2010, row.names = rownames(observed))
# Printing first six rows of smoothed SMRs
head(SMR_smooth)
# Summarizing smoothed SMRs
summary(SMR_smooth)
# Summary of the parameters under the CAR model.
Ex9_4$summary.results[]
```

<!-- QUESTION: What do we want the latent effect or the fitted values? -->

Use `ggplot()` and `geom_sf()` to plot the map of the latent spatial effect.

```{r Ex 9.4 carbayes map smoothed SMRs, class.source = 'foldable'}

observed <- tibble::rownames_to_column(observed, "ID")
phi_car <-  Ex9_4$samples$phi

latent_car_df  <-  data.frame(
  phi_mean = apply(phi_car, 2, mean),
  phi_sd = apply(phi_car, 2, sd)
)
# Combine latent spatial effect with england dataset
latent_car_df$ID <- observed$ID
latent_car_england <-  merge(england, latent_car_df, by = "ID")
# Creating map of smoothed SMRs in England in 2010
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = latent_car_england, aes(fill = phi_mean)) +
  labs(fill = 'Latent effects CARbayes') +
  # Clear background and plot borders
  theme_void()
```

## Example 9.5: Fitting a conditional model using INLA {-}


```{r Ex 9.5 clean, include = FALSE}
rm(list = ls())
```

```{r Ex 9.5 inla load, message=FALSE, echo = FALSE, warning=FALSE, message= FALSE}

library(INLA)
library(spdep) 
# Reading in borders
england <- read_sf("data/englandlocalauthority.shp")
# Reading in data
observed <-
  read.csv(file = "data/copdmortalityobserved.csv", row.names = 1)
expected <-
  read.csv(file = "data/copdmortalityexpected.csv")
covariates <- read.csv(file = "data/copdavgscore.csv") 
# Merge everything into one data frame
copd_df <-
  cbind(observed,  expected) |> merge(
    covariates,
    by.x = "code",
    by.y = "LA.CODE",
    all.x = TRUE,
    all.y = FALSE
  )
copd_df$Average.Score <- as.numeric(scale(copd_df$Average.Score))
# Create areas IDs to match the values in UK.adj
copd_df$ID <- 1:324

# Create the neighborhood matrix
W.nb <- poly2nb (england , row.names = england$ID)
W.list <- nb2listw (W.nb , style = "B")
W.mat <- nb2mat (W.nb , style = "B")

# Convert the adjacency matrix into a file in the INLA format
nb2INLA("UK.adj", W.nb)

```

<!-- TODO: Show output for this model  -->


```{r Ex 9.5 inla run}
# run the INLA model
Ex9_5 <- inla(
  Y2010 ~ Average.Score + f(ID  , model = "besag", graph = "UK.adj"),
  family = "poisson",
  E = E2010,
  data = copd_df,
  control.predictor = list(compute = TRUE)
)
```


Summarize the results

```{r Ex 9.5 inla summary}
# Summarizing smoothed SMRs
summary(Ex9_5)

```


```{r Ex 9.5 inla smoothed SMRs, class.source = 'foldable'}

observed <- tibble::rownames_to_column(observed, "ID")
phi_car <-  Ex9_5$summary.random$ID

latent_car_df  <-  data.frame(
  phi_mean = phi_car$mean,
  phi_sd = phi_car$sd
)
# Combine latent spatial effect with england dataset
latent_car_df$ID <- observed$ID
latent_car_england <-  merge(england, latent_car_df, by = "ID")
# Creating map of smoothed SMRs in England in 2010
ggplot() +
  # Choose spatial object and column for plotting
  geom_sf(data = latent_car_england, aes(fill = phi_mean)) +
  labs(fill = 'Latent effects INLA') +
  # Clear background and plot borders
  theme_void()

```

