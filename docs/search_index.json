[["index.html", "Spatio-temporal methods in environmental epidemiology Introduction", " Spatio-temporal methods in environmental epidemiology Gavin Shaddick, James V. Zidek, and Alexandra M. Schmidt 2023-03-31 Introduction This is the online companion for the book Spatio-temporal methods in environmental epidemiology published in Chapman and Hall/CRC. All the codes used for the examples in the book are presented here to ensure the material is reproducible, transparent, and accessible. Please feel free to contact us if you find any typos, or errors in our code Errare humanum est. We would like to thank Yang ‘Seagle’ Liu, Yiping Dou, and Yi Liu for allowing us to include their code and other material in the book and in the online resources. We would also like to thank Sara Zapata-Marin, Johnny Xi, Paritosh Kumar Roy, and Mariana Carmona Baez for their help developing some of the additional examples and exercises for this edition. "],["preface-2nd-edition.html", "Preface (2nd edition)", " Preface (2nd edition) Spatio-Temporal Methods in Environmental Epidemiology using R is the first book of its kind to specifically address the interface between environmental epidemiology and spatio-temporal modeling. In response to the growing need for collaboration between statisticians and environmental epidemiologists, the book links recent developments in spatio-temporal methodology with epidemiological applications. Drawing on real-life problems, it provides the tools required to exploit advances in methodology when assessing the health risks associated with environmental hazards. Clear guidelines are given to enable the implementation of methodology and estimation of risks in practice. Designed for graduate students in both epidemiology and statistics, the text covers a wide range of topics, from an introduction to epidemiological principles and the foundations of spatio-temporal modeling to new directions for research. It describes traditional and Bayesian approaches and presents the theory of spatial, temporal, and spatio-temporal modeling in the context of its application to environmental epidemiology. The text includes practical examples together with embedded R code and details of specific R packages and the use of other software including Nimble, Stan, and INLA. Online resources associated with the book provide additional code, data, examples, exercises, lab projects, and more. Representing a major new direction in environmental epidemiology, this book—in full color throughout—underscores the increasing need to consider dependencies in both space and time when modeling epidemiological data. Students will learn how to identify and model patterns in spatio-temporal data and to exploit dependencies over both space and time in order to reduce bias and inefficiency. From this book the reader will have gained an understanding of the following topics: The basic concepts of epidemiology and the estimation of risks associated with environmental hazards. Hierarchical modelling with a Bayesian framework. The theory of spatial, temporal and spatio–temporal process needed for environmental health risk analysis. Fundamental questions related to the nature and role of uncertainty in environmental epidemiology and methods which may help answer those questions. Important areas of application within environmental epidemiology together with strategies for building the models that are needed and coping with challenges that arise. Methods and software for the analysis and visualisation of environmental and health. - Examples of R and WinBUGS code are given throughout the book and, together with data for the examples the code, are included in the online resources. Offer a variety of exercises, both theoretical and practical, to assist in the development of the skills needed to perform spatio–temporal analyses. New frontiers and areas of current and future research. "],["why.html", "Chapter 1 Why spatio-temporal epidemiology? Example 1.5", " Chapter 1 Why spatio-temporal epidemiology? This chapter provides a overview of methods for spatio-temporal modelling and their use in epidemiological studies Example 1.5 # Loading relevant libraries library(ggmap) library(sp) library(rgdal) # Load Meuse river data(meuse) # Assign a reference system used in the Netherlands coordinates(meuse) &lt;- ~ x + y proj4string(meuse) &lt;- CRS(&#39;+init=epsg:28992&#39;) # Convert it to latitude - longitude scale meuse_ll &lt;- spTransform(meuse,CRS(&quot;+proj=longlat +datum=WGS84&quot;)) ## Warning: PROJ support is provided by the sf and terra packages among others meuse_ll_df &lt;- as.data.frame(meuse_ll) # Specify the bounding box latLongBox = bbox(meuse_ll) location = c(latLongBox[1, 1] - 0.01, latLongBox[2, 1] - 0.01, latLongBox[1, 2] + 0.01, latLongBox[2, 2] + 0.01) # Create map with location dots marked on it in MeuseMap &lt;- get_stamenmap( bbox = location, zoom = 14 ) ## ℹ Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL. ggmap(MeuseMap) + geom_point( data = meuse_ll_df, aes( x = x, y = y, color = &quot;red&quot; )) + theme_void() + theme(legend.position=&quot;none&quot;) # Finally we write the result that can be read by Google maps writeOGR(meuse_ll, &quot;meuse.kml&quot;, &quot;meuse&quot;, driver=&quot;KML&quot;) "],["basics.html", "Chapter 2 Epidemiology-the basics Example 2.9: Estimating the SMR using a Poisson GLM Example 2.10: Estimating the SMR using quasi-likelihood Example 2.11: Modelling differences in SMRs in relation to differences in exposures Example 2.12: Modelling the risks associated with lagged effects of air pollution Example 2.13: Estimating the odds ratio in a case-control study using a logistic model Example 2.14: Estimating the odds ratio of asthma associated with proximity to roads", " Chapter 2 Epidemiology-the basics This chapter contains the basic principles of epidemiological analysis and how estimates of the risks associated with exposures can be obtained. From this chapter, the reader will have gained an understanding of the following topics: Methods for expressing risk and their use with different types of epidemiological study. Calculating risks based on calculations of the expected number of health counts in an area, allowing for the age–sex structure of the underlying population. The use of generalised linear models (GLMS) to model counts of disease and case–control indicators. Modelling the effect of exposures on health and allowing for the possible effects of covariates. Cumulative exposures to environmental hazards. Example 2.9: Estimating the SMR using a Poisson GLM # Finding MLE and SE of log(SMR) = beta0 on one single area y &lt;- 29 # Total observed death E &lt;- 19.88 # Expected deaths summary(glm(y ~ offset(log(E)), family = &quot;poisson&quot;)) ## ## Call: ## glm(formula = y ~ offset(log(E)), family = &quot;poisson&quot;) ## ## Deviance Residuals: ## [1] 0 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.3776 0.1857 2.033 0.042 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 6.6613e-16 on 0 degrees of freedom ## Residual deviance: 4.4409e-15 on 0 degrees of freedom ## AIC: 7.2109 ## ## Number of Fisher Scoring iterations: 3 # Finding MLE and SE of log(SMR) = beta0 over multiple areas summary(glm(Y ~ offset(log(E)),family=&quot;poisson&quot;, data=data)) Example 2.10: Estimating the SMR using quasi-likelihood # Using quasi-likelihood to find the MLE and standard error of log(SMR) = beta0 summary(glm(y ~ offset(log(E)), family=&quot;quasipoisson&quot;), data = data) Example 2.11: Modelling differences in SMRs in relation to differences in exposures # Fitting a model to estimate the relative risk associated with air pollution summary(glm(Y ~ offset(log(E)) + X1, family=&quot;poisson&quot;, data=data)) # Fitting a model to estimate the relative risk associated with air pollution # using a Quasi-Poisson approach summary(glm(Y ~ offset(log(E)) + X1, family = &quot;quasipoisson&quot;, data = data)) #Fitting a Poisson GLM with air pollution and deprivation summary(glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;poisson&quot;, data = data)) # Fitting a Quasi-Poisson GLM with air pollution and deprivation summary(glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data)) Perform tests between the deviances of two models. ## Test 1: Effect on Quasi-Poisson models with and without deprivation anova( glm(Y ~ offset(log(E)) + X1, family = &quot;quasipoisson&quot;, data = data), # Model 1 glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data), # Model2 test = &quot;Chisq&quot; ) # Chi-Squared test ## Test 2: Effect on Quasi-Poisson models with and without air pollution anova( glm(Y ~ offset(log(E)) + X1 + X2, family = &quot;quasipoisson&quot;, data = data), # Model 1 glm(Y ~ offset(log(E)) + X2, family = &quot;quasipoisson&quot;, data = data), # Model 2 test = &quot;Chisq&quot; ) # Chi-Squared test Example 2.12: Modelling the risks associated with lagged effects of air pollution ## Fitting quasi-poisson model glm( formula = Y ~ offset(log(E)) + X1 + X1t1 + X1t2, family = &quot;quasipoisson&quot;, data = data ) Example 2.13: Estimating the odds ratio in a case-control study using a logistic model ## Fitting Odds Ratio glm(formula = Y ~ 1, family = &quot;binomial&quot;, data = data) Example 2.14: Estimating the odds ratio of asthma associated with proximity to roads ## Fitting Odds Ratio glm(Y ~ X, family = &quot;binomial&quot;, data = data) "],["uncertainty.html", "Chapter 3 Uncertainty and its reduction", " Chapter 3 Uncertainty and its reduction This chapter contains a discussion of uncertainty, both in terms of statistical modelling and quantification but also in the wider setting of sources of uncertainty outside those normally encountered in statistics. From this chapter, the reader will have gained an understanding of the following topics: Uncertainty can be dichotomised as either qualitative or quantitative, with the former allowing consideration of a wide variety of sources of uncertainty that would be difficult, if not impossible, to quantify mathematically. Quantitative uncertainty can be thought of as comprising both aleatory and epistemic components, the former representing stochastic uncertainty and the latter subjective uncertainty. Methods for assessing uncertainty including eliciting prior information from experts and sensitivity analysis. Indexing quantitative uncertainty using the variance and entropy of the distribution of a random quantity. Uncertainty in post-normal science derives from a wide variety of issues and can lead to high levels of that uncertainty with serious consequences. Understanding uncertainty is therefore a vital feature of modern environmental epidemiology. "],["data.html", "Chapter 4 Data: increasing information and reducing uncertainty", " Chapter 4 Data: increasing information and reducing uncertainty This is a new chapter "],["embracing.html", "Chapter 5 Embracing uncertainty: the Bayesian approach", " Chapter 5 Embracing uncertainty: the Bayesian approach This chapter introduces the Bayesian approach which provides a natural framework for dealing with uncertainty and also for fitting the models that will be encountered later in the book. From this chapter, the reader will have gained an understanding of the following topics: The use of prior distributions to capture beliefs before data are observed. The combination of prior beliefs and information from data to obtain posterior beliefs. The manipulation of prior distributions with likelihoods to formulate posterior distributions and why conjugate priors are useful in this regard. The difference between informative and non-informative priors. The use of the posterior distribution for inference and methods for calculating summary measures. "],["tactics.html", "Chapter 6 Tactics-implementing uncertainty models Example 6.2: Chronic obstructive pulmonary disease (COPD) in England Example 6.3: Fitting a Poisson regression model", " Chapter 6 Tactics-implementing uncertainty models This chapter describes methods for implementing Bayesian models when their complexity means that simple, analytic solutions may not be available. From this chapter, the reader will have gained an understanding of the following topics: Analytical approximations to the posterior distribution. Using samples from a posterior distribution for inference and Monte Carlo integration. Methods for direct sampling such as importance and rejection sampling. Markov Chain Monte Carlo (MCMC) and methods for obtaining samples from the required posterior distribution including Metropolis–Hastings and Gibbs algorithms. Using nimble and stan to fit Bayesian models using Gibbs sampling. Integrated Nested Laplace Approximations (INLA) as a method for performing efficient Bayesian inference including the use of R–INLA to implement a wide variety of latent process models. Example 6.2: Chronic obstructive pulmonary disease (COPD) in England We now look at example into the hospital admission rates for chronic obstructive pulmonary disease (COPD) in England between 2001–2010. In England, there are 324 local authority administrative areas each with an observed and expected number of cases. The expected numbers were calculated using indirect standardization by applying the age–sex specific rates for the whole of England to the age–sex population profile of each of the areas. For this example, the following packages are needed ggplot2 and sf. Load the necessary packages. library(ggplot2) library(sf) To create SMR maps, we need to read in the relevant shapefiles. englandlocalauthority.shp and englandlocalauthority.dbf contain the location, shape, and attributes of English local authorities. The function read_sf() from the sf package will read these shapefiles into R. copdmortalityobserved.csv contains the observed number of hospital admissions in England by local authority. copdmortalityexpected.csv contains the expected number of hospital admissions in England by local authority. # Reading in borders england &lt;- read_sf(&quot;data/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copdmortalityexpected.csv&quot;, row.names = 1) Print summaries of the observed and expected counts. # Printing first six rows of the observed counts head(observed) ## name Y2001 Y2002 Y2003 Y2004 Y2005 Y2006 Y2007 Y2008 ## 00AA City of London LB 2 0 3 1 1 1 5 1 ## 00AB Barking and Dagenham LB 100 100 122 93 136 97 91 96 ## 00AC Barnet LB 110 102 106 89 99 97 72 84 ## 00AD Bexley LB 109 113 113 96 113 97 94 89 ## 00AE Brent LB 69 89 70 59 61 48 53 46 ## 00AF Bromley LB 120 129 135 124 128 117 120 106 ## Y2009 Y2010 ## 00AA 0 1 ## 00AB 101 78 ## 00AC 78 89 ## 00AD 93 93 ## 00AE 55 43 ## 00AF 107 113 # Printing first six rows of the expected counts head(expected) ## E2001 E2002 E2003 E2004 E2005 E2006 ## 00AA 2.648915 2.68106 2.727112 2.749562 2.808655 2.915977 ## 00AB 63.946730 63.41700 62.567863 61.444884 60.677119 59.678672 ## 00AC 121.795213 121.91534 122.451050 123.201898 124.449563 125.982868 ## 00AD 90.201336 91.24645 91.949050 92.754781 93.674540 94.598593 ## 00AE 76.876437 77.18529 78.017980 78.967493 80.422828 81.785325 ## 00AF 131.182934 132.30521 133.257442 134.520920 136.441229 137.382528 ## E2007 E2008 E2009 E2010 ## 00AA 3.021586 3.114696 3.237998 3.237998 ## 00AB 58.487583 57.701932 57.250524 57.250524 ## 00AC 127.088805 128.825149 131.374946 131.374946 ## 00AD 95.447131 96.832061 97.651369 97.651369 ## 00AE 83.651266 85.265264 87.089119 87.089119 ## 00AF 138.634021 139.508507 140.634084 140.634084 # Summarising the observed counts summary(observed) ## name Y2001 Y2002 Y2003 ## Length:324 Min. : 2.00 Min. : 0.00 Min. : 3.00 ## Class :character 1st Qu.: 35.00 1st Qu.: 38.00 1st Qu.: 38.00 ## Mode :character Median : 50.00 Median : 52.00 Median : 52.00 ## Mean : 68.01 Mean : 69.63 Mean : 73.44 ## 3rd Qu.: 83.50 3rd Qu.: 80.75 3rd Qu.: 83.25 ## Max. :445.00 Max. :438.00 Max. :480.00 ## Y2004 Y2005 Y2006 Y2007 ## Min. : 1.00 Min. : 1.00 Min. : 1.00 Min. : 5.00 ## 1st Qu.: 35.00 1st Qu.: 37.00 1st Qu.: 35.00 1st Qu.: 37.00 ## Median : 49.50 Median : 51.00 Median : 49.00 Median : 50.00 ## Mean : 66.67 Mean : 69.37 Mean : 67.07 Mean : 68.17 ## 3rd Qu.: 81.25 3rd Qu.: 80.50 3rd Qu.: 81.00 3rd Qu.: 79.00 ## Max. :428.00 Max. :395.00 Max. :428.00 Max. :456.00 ## Y2008 Y2009 Y2010 ## Min. : 1.00 Min. : 0.00 Min. : 1.00 ## 1st Qu.: 37.00 1st Qu.: 36.00 1st Qu.: 38.00 ## Median : 51.00 Median : 50.00 Median : 51.00 ## Mean : 71.40 Mean : 67.04 Mean : 68.81 ## 3rd Qu.: 84.25 3rd Qu.: 78.00 3rd Qu.: 81.25 ## Max. :463.00 Max. :394.00 Max. :441.00 # Summarising the expected counts summary(expected) ## E2001 E2002 E2003 E2004 ## Min. : 2.649 Min. : 2.681 Min. : 2.727 Min. : 2.75 ## 1st Qu.: 39.066 1st Qu.: 39.456 1st Qu.: 39.849 1st Qu.: 40.60 ## Median : 51.766 Median : 52.671 Median : 53.487 Median : 54.29 ## Mean : 62.944 Mean : 63.589 Mean : 64.139 Mean : 64.72 ## 3rd Qu.: 74.292 3rd Qu.: 74.974 3rd Qu.: 74.701 3rd Qu.: 74.02 ## Max. :370.913 Max. :371.271 Max. :369.861 Max. :368.87 ## E2005 E2006 E2007 E2008 ## Min. : 2.809 Min. : 2.916 Min. : 3.022 Min. : 3.115 ## 1st Qu.: 41.646 1st Qu.: 42.497 1st Qu.: 43.203 1st Qu.: 44.262 ## Median : 54.765 Median : 55.506 Median : 56.552 Median : 57.522 ## Mean : 65.440 Mean : 66.180 Mean : 67.022 Mean : 67.950 ## 3rd Qu.: 75.003 3rd Qu.: 75.260 3rd Qu.: 75.790 3rd Qu.: 76.935 ## Max. :368.565 Max. :367.838 Max. :368.026 Max. :368.291 ## E2009 E2010 ## Min. : 3.238 Min. : 3.238 ## 1st Qu.: 45.062 1st Qu.: 45.062 ## Median : 58.077 Median : 58.077 ## Mean : 68.901 Mean : 68.901 ## 3rd Qu.: 78.166 3rd Qu.: 78.166 ## Max. :368.940 Max. :368.940 Modelling the raw standardized mortality rates (SMRs) Calculate the raw SMRs as. \\[ \\text{SMR} = \\dfrac{observed}{expected}\\] SMR_raw &lt;- observed[, -1] / expected # Rename columns names(SMR_raw) &lt;- c( &quot;SMR2001&quot;, &quot;SMR2002&quot;, &quot;SMR2003&quot;, &quot;SMR2004&quot;, &quot;SMR2005&quot;, &quot;SMR2006&quot;, &quot;SMR2007&quot;, &quot;SMR2008&quot;, &quot;SMR2009&quot;, &quot;SMR2010&quot; ) # Printing first six rows of raw SMRs head(SMR_raw) ## SMR2001 SMR2002 SMR2003 SMR2004 SMR2005 SMR2006 SMR2007 ## 00AA 0.7550261 0.0000000 1.1000648 0.3636943 0.3560423 0.3429382 1.6547601 ## 00AB 1.5638016 1.5768644 1.9498828 1.5135516 2.2413721 1.6253713 1.5558858 ## 00AC 0.9031554 0.8366462 0.8656520 0.7223915 0.7955030 0.7699460 0.5665330 ## 00AD 1.2084078 1.2384043 1.2289415 1.0349871 1.2063043 1.0253852 0.9848384 ## 00AE 0.8975442 1.1530694 0.8972291 0.7471429 0.7584911 0.5869024 0.6335828 ## 00AF 0.9147531 0.9750183 1.0130766 0.9217897 0.9381329 0.8516367 0.8655884 ## SMR2008 SMR2009 SMR2010 ## 00AA 0.3210586 0.0000000 0.3088328 ## 00AB 1.6637225 1.7641760 1.3624329 ## 00AC 0.6520466 0.5937205 0.6774503 ## 00AD 0.9191171 0.9523676 0.9523676 ## 00AE 0.5394928 0.6315370 0.4937471 ## 00AF 0.7598103 0.7608397 0.8035037 # Summarising raw SMRs summary(SMR_raw) ## SMR2001 SMR2002 SMR2003 SMR2004 ## Min. :0.3883 Min. :0.0000 Min. :0.3616 Min. :0.2778 ## 1st Qu.:0.7900 1st Qu.:0.8272 1st Qu.:0.8519 1st Qu.:0.7636 ## Median :0.9496 Median :1.0168 Median :1.0209 Median :0.9266 ## Mean :1.0349 Mean :1.0508 Mean :1.0895 Mean :0.9812 ## 3rd Qu.:1.2526 3rd Qu.:1.2364 3rd Qu.:1.3071 3rd Qu.:1.1858 ## Max. :1.9861 Max. :2.2181 Max. :2.2483 Max. :1.9811 ## SMR2005 SMR2006 SMR2007 SMR2008 ## Min. :0.3326 Min. :0.3429 Min. :0.3509 Min. :0.3211 ## 1st Qu.:0.7592 1st Qu.:0.7415 1st Qu.:0.7533 1st Qu.:0.7695 ## Median :0.9573 Median :0.9101 Median :0.9305 Median :0.9404 ## Mean :1.0126 Mean :0.9726 Mean :0.9743 Mean :1.0069 ## 3rd Qu.:1.2083 3rd Qu.:1.1586 3rd Qu.:1.1679 3rd Qu.:1.1979 ## Max. :2.2414 Max. :2.0805 Max. :1.8528 Max. :2.0567 ## SMR2009 SMR2010 ## Min. :0.0000 Min. :0.3088 ## 1st Qu.:0.7452 1st Qu.:0.7682 ## Median :0.8777 Median :0.9337 ## Mean :0.9328 Mean :0.9639 ## 3rd Qu.:1.0934 3rd Qu.:1.1335 ## Max. :1.8507 Max. :2.3856 Attach the values of the raw SMRs to the shapefiles. The function merge() allows us to combine a data frame with a shapefile to plot later. # Convert row names to ID column SMR_raw &lt;- tibble::rownames_to_column(SMR_raw, &quot;ID&quot;) # Combine raw SMRs and shapefiles SMRspatial_raw &lt;- merge(england, SMR_raw, by = &quot;ID&quot;) Use ggplot() and geom_sf() to create a map which colours the local authorities by the raw SMR estimate. # Creating breaks for legend in plot range &lt;- seq(min(SMR_raw$SMR2010) - 0.01, max(SMR_raw$SMR2010) + 0.01, length.out = 11) # Creating map of Raw SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = SMRspatial_raw, aes(fill = SMR2010)) + # Break points for colours scale_y_continuous(breaks = range) + # Clear background and plot borders theme( axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), rect = element_blank() ) Modelling a Poisson-Gamma with an MCMC implemented in R The following code shows how to implement the MCMC using only R for the COPD example. First, the constants are defined and the necessary vectors are initialized. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 # Number of MCMC iterations L &lt;- 80000 ## Initialize objects used in MCMC # Matrix for sampled values of parameter theta_i theta &lt;- matrix(ncol = length(y), nrow = L) # Matrix for fitted values fitted &lt;- theta # Vector for sampled values of hyper-parameter a a &lt;- c() # Vector for sampled values of hyper-parameter b b &lt;- c() ## Define constants # Sample size N &lt;- length(y) # Parameter of exponential prior for a lambda_a &lt;- 1 # Parameter of exponential prior for b lambda_b &lt;- 1 # standard deviation of the proposal distribution of log a u &lt;- 0.5 # Initialize theta theta[1, ] &lt;- y / E # Initial value sampled from the prior for a # REVIEW: In the example of the book theta ~ Ga(a,a) not Ga(a,b) a &lt;- rexp(1, lambda_a) # Initial value sampled from the prior for b b &lt;- rexp(1, lambda_b) fitted[1, ] &lt;- rpois(N, E * theta[1, ]) # Once all the constants and initial values are set we can run the MCMC. # The following code shows the MCMC implementation of a Poisson-Gamma model using only `R`. # Starting from l=2 as l=1 contains the initial values for(l in 2:L) { # Sampling from the posterior full conditional of each theta_i for (i in 1:N) theta[l, i] &lt;- rgamma(1, (y[i] + a[(l - 1)]), rate = (E[i] + b[(l - 1)])) # Sampling from the posterior full conditional of b # b[l] &lt;- b[l-1] # REVIEW: is this part of the comment? b[l] &lt;- rgamma(1, (N * a[(l - 1)] + 1), rate = (sum(theta[l, ]) + lambda_b)) # Metropolis-Hastings step to sample from the full conditional of &quot;a&quot; # the new value receives the current value in case the proposed # value is rejected a[l] &lt;- a[l - 1] # Proposal in the log-scale laprop &lt;- rnorm(1, log(a[l - 1]), u) aprop &lt;- exp(laprop) num &lt;- N * (aprop * (log(b[l])) - lgamma(aprop)) + (aprop - 1) * sum(log(theta[l, ])) - aprop * lambda_a + log(aprop) den &lt;- N * (a[l - 1] * (log(b[l])) - lgamma(a[l - 1])) + (a[(l - 1)] - 1) * sum(log(theta[l, ])) - a[(l - 1)] * lambda_a + log(a[(l - 1)]) ratio &lt;- exp(num - den) unif &lt;- runif(1) # Change the current value if the proposed value is accepted if (unif &lt; ratio) a[l] &lt;- aprop fitted[l,] &lt;- rpois(N, E * theta[l,]) } After running the MCMC, we should check if the chains have converged # Number of burn in samples burnin &lt;- 20000 thin &lt;- 30 # MCMC samples seqaux &lt;- seq(burnin, L, by = thin) # Trace-plots of the parameters xx &lt;- seq(0, 6, length = 2000) par(mfrow = c(2, 2)) # Plot for &quot;a&quot; plot(a[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;) hist(a[seqaux], prob = 1, main = &quot;&quot;) lines(xx, dexp(xx, lambda_a), col = 2, lwd = 2) # COMBAK: This chains are not looking great # Plot for &quot;b&quot; plot(b[seqaux], type = &quot;l&quot;, bty = &quot;n&quot;) hist(b[seqaux], prob = 1, main = &quot;&quot;) lines(xx, dexp(xx, lambda_b), col = 2, lwd = 2) # Traceplots of theta&#39;s par(mfrow = c(3, 3)) for (i in 1:9) plot(theta[seqaux, i], type = &quot;l&quot;, bty = &quot;n&quot;) Given the convergence issues, we can also check the estimated sampled size and Rhat using the package coda. paste0(&quot;ESS a: &quot;, coda::effectiveSize(a[seqaux])) ## [1] &quot;ESS a: 50.0892164646561&quot; paste0(&quot;ESS b: &quot;, coda::effectiveSize(b[seqaux])) ## [1] &quot;ESS b: 54.1012433947017&quot; paste0(&quot;ESS theta[1]: &quot;, coda::effectiveSize(theta[seqaux, 1])) ## [1] &quot;ESS theta[1]: 2001&quot; paste0(&quot;ESS theta[10]: &quot;,coda::effectiveSize(theta[seqaux, 10])) ## [1] &quot;ESS theta[10]: 2001&quot; The variances for the parameters a and b is lower than the recommended minimum of 100. We will go back to this in Chapter 8. Now that we have guaranteed the convergence of the chains, we can look at the posterior summaries. # Posterior summaries of theta_i meantheta &lt;- apply(theta, 2, mean) q025theta &lt;- apply(theta, 2, function(x) quantile(x, 0.025)) q975theta &lt;- apply(theta, 2, function(x) quantile(x, 0.975)) # Plot the mean and 95% CIs for the thetas par(mfrow = c(1, 1)) plot( meantheta, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(q025theta), max(q975theta)) ) for (i in 1:N) segments(i, q025theta[i], i, q975theta[i]) abline(h = 1, lwd = 2, lty = 2) # Posterior summary of fitted values meanfit &lt;- apply(fitted, 2, mean) q025fit &lt;- apply(fitted, 2, function(x) quantile(x, 0.025)) q975fit &lt;- apply(fitted, 2, function(x) quantile(x, 0.975)) # Plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, meanfit, ylim = c(min(q025fit), max(q975fit)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], q025fit[i], y[i], q975fit[i]) abline(a = 0, b = 1) Example 6.3: Fitting a Poisson regression model Nimble Load nimble package library(&quot;nimble&quot;) The following code is used to fit the Poisson log-linear model seen in Chapter 2 Section … using Nimble First, define the model in Nimble. # Define the model Example5_3Code &lt;- nimbleCode({ for (i in 1:N) { Y[i] ~ dpois(mu[i]) log(mu[i]) &lt;- log(E[i]) + beta0 + beta1 * X1[i] + beta2 * X2[i] } # Priors beta0 ~ dnorm (0 , sd = 100) beta1 ~ dnorm (0 , sd = 100) beta2 ~ dnorm (0 , sd = 100) # Functions of interest: base &lt;- exp(beta0) RR &lt;- exp(beta1) }) # Read the data and define the constants, data and initials lists for the `Nimble` model. # REVIEW: Is this another version of the COPD data? Is there a data dictionary for this dataset? data &lt;- read.csv(&quot;data/DataExample53.csv&quot;, sep = &quot;,&quot;) ex.const &lt;- list( N = nrow(data), E = data$exp_lungc65pls, X1 = as.vector(scale(data$k3)), X2 = as.vector(scale(data$k2)) ) ex.data &lt;- list(Y = data$lungc65pls) inits &lt;- function() list(beta0 = rnorm(1), beta1 = rnorm(1), beta2 = rnorm(1)) # Define parameters to monitor and run the model params &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;base&quot;, &quot;RR&quot;) mcmc.out &lt;- nimbleMCMC( code = Example5_3Code, data = ex.data, constants = ex.const, inits = inits, monitors = params, niter = 22000, nburnin = 2000, thin = 10, WAIC = TRUE, nchains = 2, samplesAsCodaMCMC = TRUE ) Check the WAIC. mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] 3049.327 ## Field &quot;lppd&quot;: ## [1] -1514.466 ## Field &quot;pWAIC&quot;: ## [1] 10.19779 Show the trace plots and posterior summaries for each of the parameters. mvSamples &lt;- mcmc.out$samples #trace plots of beta1 plot(mvSamples[, c(&quot;beta1&quot;)]) #trace plots of base plot(mvSamples[, c(&quot;base&quot;)]) #trace plots of RR plot(mvSamples[, c(&quot;RR&quot;)]) #posterior summary of base summary(mvSamples[, c(&quot;base&quot;)]) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 2 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 1.1288621 0.0123903 0.0001959 0.0001959 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 1.104 1.121 1.129 1.137 1.154 #posterior summary of RR summary(mvSamples[, c(&quot;RR&quot;)]) ## ## Iterations = 1:2000 ## Thinning interval = 1 ## Number of chains = 2 ## Sample size per chain = 2000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## 1.0147062 0.0174258 0.0002755 0.0004217 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## 0.9806 1.0029 1.0143 1.0267 1.0486 Stan Load stan package with options library(rstan) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) data { int&lt;lower=0&gt; N; vector[N] E; // expected cases? vector[N] X1; // what are these covariates? vector[N] X2; int Y[N] ; } parameters { real beta0; real beta1; real beta2; } transformed parameters{ real base = exp(beta0); real RR = exp(beta1); } model { vector[N] mu; for(i in 1:N){ mu[i] = log(E[i])+ beta0 + beta1*X1[i] + beta2*X2[i]; Y[i] ~ poisson_log(mu[i]); } beta0 ~ normal(0 , 100); beta1 ~ normal(0 , 100); beta2 ~ normal(0 , 100); } data &lt;- read.csv(&quot;data/DataExample53.csv&quot;, sep = &quot;,&quot;) stan_data &lt;- list( N = nrow(data), E = data$exp_lungc65pls, X1 = as.vector(scale(data$k3)), X2 = as.vector(scale(data$k2)), Y = data$lungc65pls ) Example6_3_Stan &lt;- stan( file = &quot;functions/Example6_3.stan&quot;, data = stan_data, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) rstan::traceplot(Example6_3_Stan, pars = c(&quot;beta1&quot;, &quot;base&quot;, &quot;RR&quot;)) stan_summary &lt;- summary(Example6_3_Stan, pars = c(&quot;RR&quot;, &quot;base&quot;)) stan_summary$summary ## mean se_mean sd 2.5% 25% 50% 75% ## RR 1.015277 0.0002199196 0.01758635 0.9810819 1.003394 1.015282 1.027100 ## base 1.128823 0.0001406853 0.01272524 1.1041418 1.120047 1.128823 1.137338 ## 97.5% n_eff Rhat ## RR 1.050307 6394.753 1.000196 ## base 1.153955 8181.535 1.000037 "],["Strategies.html", "Chapter 7 Strategies implementing uncertainty models", " Chapter 7 Strategies implementing uncertainty models This chapter considers both some of the wider issues related to modelling and the generalisability of results and more technical material on the effect of covariates and model selection. From this chapter, the reader will have gained an understanding of the following topics: Why having contrasts in the variables of interest is important in assessing the effects they have on the response variable. The biases that may arise in the presence of covariates and how covariates can affect variable selection and model choice. Hierarchical models and how that can be used to acknowledge dependence between observations. There are issues with using p–values as measures of evidence against a null hypothesis. Basing scientific conclusions on it can lead to non-reproducible results. The use of predictions from exposure models including acknowledging the additional uncertainty involved when using predictions as inputs to a health model. Methods for performing model selection, including the pros and cons of automatic selection procedures. Model selection within the Bayesian setting and how the models themselves can be incorporated into the estimation process using Bayesian Model Averaging. "],["trusted.html", "Chapter 8 But can the data be trusted Solutions to Selected Exercises", " Chapter 8 But can the data be trusted This chapter considers some of the issues that will arise when dealing with ‘real data’. Data will commonly have missing values and may be measured with error. This error might be random or may be due to systematic patterns in how it was collected. From this chapter, the reader will have gained an understanding of the following topics: Classification of missing values into missing at random or not at random. Methods for imputing missing values. Various measurement models including classical and Berkson. The attenuation of regression coefficients under measurement error. Preferential sampling, where the process that determines the locations of monitoring sites and the process being modelled are in some ways dependent. How preferential sampling can bias the measurements that arise from environmental monitoring networks Solutions to Selected Exercises PLACEHOLDER: Repeat the Black Smoke analysis conducted in Watson et al. 2019. in the “naive” model. If INLA is not already installed, it needs to be done manually (https://www.r-inla.org/download-install): install.packages( &quot;INLA&quot;, repos = c(getOption(&quot;repos&quot;), INLA = &quot;https://inla.r-inla-download.org/R/stable&quot;), dep = TRUE ) After INLA is installed load all the necessary packages. library(INLA) ## Loading required package: Matrix ## Loading required package: foreach ## Loading required package: parallel ## This is INLA_22.12.16 built 2022-12-23 13:24:10 UTC. ## - See www.r-inla.org/contact-us for how to get help. library(mvtnorm) library(boot) ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:nimble&#39;: ## ## logit library(geoR) ## -------------------------------------------------------------- ## Analysis of Geostatistical Data ## For an Introduction to geoR go to http://www.leg.ufpr.br/geoR ## geoR version 1.9-2 (built on 2022-08-09) is now loaded ## -------------------------------------------------------------- library(reshape2) library(sp) library(ggplot2) Assuming the data is held in the file BlackSmokePrefData.csv in a /data directory: BlackSmokePrefData &lt;- read.csv(&quot;data/BlackSmokePrefData.csv&quot;) head(BlackSmokePrefData) ## site east north AMEAN66 AMEAN67 AMEAN68 AMEAN69 AMEAN70 AMEAN71 ## 1 ADDLESTONE 1 505200 164600 92 103 68 47 47 45 ## 2 BARNSLEY 8 434800 409400 NA NA NA NA NA NA ## 3 BARNSLEY 9 437000 405500 NA NA NA NA NA NA ## 4 BOLTON 24 371500 409200 132 108 75 77 78 74 ## 5 BRADFORD 6 416300 432900 202 151 122 110 90 118 ## 6 CARDIFF 12 319300 177300 NA NA NA NA NA NA ## AMEAN72 AMEAN73 AMEAN74 AMEAN75 AMEAN76 AMEAN77 AMEAN78 AMEAN79 AMEAN80 ## 1 52 41 36 31 33 24 20 17 11 ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA 23 18 19 19 12 ## 4 149 137 NA NA NA NA NA NA NA ## 5 172 191 74 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA 24 19 12 ## AMEAN81 AMEAN82 AMEAN83 AMEAN84 AMEAN85 AMEAN86 AMEAN87 AMEAN88 AMEAN89 ## 1 15 9 8 15 12 10 13 7 9 ## 2 NA NA NA NA NA NA NA NA NA ## 3 19 NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 10 11 16 19 18 14 11 9 11 ## AMEAN90 AMEAN91 AMEAN92 AMEAN93 AMEAN94 AMEAN95 AMEAN96 ## 1 11 6 NA NA NA NA NA ## 2 NA 13 6 11 6 6 5 ## 3 NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA ## 6 12 12 9 11 9 19 11 ## standardize location coordinates sd_x &lt;- sd(BlackSmokePrefData[, c(2)]) sd_y &lt;- sd(BlackSmokePrefData[, c(3)]) BlackSmokePrefData[, 2] &lt;- BlackSmokePrefData[, 2] / sd_x BlackSmokePrefData[, 3] &lt;- BlackSmokePrefData[, 3] / sd_y We need to reshape the data to have one observation per row, as required by INLA. yearmeans = colMeans(BlackSmokePrefData[,-c(1, 2, 3)], na.rm = T) ## save means for each year BlackSmokePrefData &lt;- melt( BlackSmokePrefData, id.vars = c(1, 2, 3), variable.name = &#39;year&#39;, value.name = &#39;bsmoke&#39; ) BlackSmokePrefData$year = as.numeric(as.character(factor(BlackSmokePrefData$year, labels = 66:96))) ## standardize by mean of mean of each year to make it unitless BlackSmokePrefData$bsmoke = BlackSmokePrefData$bsmoke / mean(yearmeans) ## log-transform to eliminate right skew BlackSmokePrefData$bsmoke = log(BlackSmokePrefData$bsmoke) Producing the 2-d mesh of the UK: no_sites = as.numeric(length(unique(BlackSmokePrefData$site))) # number of sites no_T = as.numeric(length(unique(BlackSmokePrefData$year))) # number of years ncol = 100 # grid for projection nrow = 100 L = nrow * ncol # number of grid sites # Form the regular mesh # # Create a rough convex boundary for the UK # # Form the grid independently from the sites to avoid preferential grid selection # UK_domain = cbind(c(2, 7.7, 7.7, 6, 4, 1), c(0.5, 0.5, 6, 13.5, 13.5, 12)) hull = inla.nonconvex.hull(cbind(BlackSmokePrefData$east, BlackSmokePrefData$north)) cutoff_dist = 16000 / sd_x # 16km as min edge max.edge = 100000 / sd_x # 100km max edge as in Shaddick and Zidek # NOTE: JOHNNY, I HAD TO COMMENT THE CUTOFF VARIABLE CAUSE OTHERWISE IT WOULDN&#39;T WORK mesh = inla.mesh.2d( loc = cbind(BlackSmokePrefData$east, BlackSmokePrefData$north), boundary = hull, offset = c(0.1, 0.2), max.edge = c(cutoff_dist, 0.5), #cutoff = c(cutoff_dist, 1), min.angle = 26 ) plot(mesh) points(BlackSmokePrefData$east, BlackSmokePrefData$north, col = &quot;red&quot;) Now we define and fit our INLA model. spde_obj = inla.spde2.pcmatern( mesh = mesh, alpha = 2, prior.range = c(0.04, 0.05), prior.sigma = c(1, 0.01), constr = T ) A_proj = inla.spde.make.A( mesh = mesh, loc = as.matrix(cbind( BlackSmokePrefData$east, BlackSmokePrefData$north )), group = BlackSmokePrefData$year - 65, # group membership needs to be 1:no_T n.group = no_T ) s_index = inla.spde.make.index(name = &quot;spatial.field&quot;, n.spde = spde_obj$n.spde, n.group = no_T) time = (1:no_T) / no_T time2 = time ^ 2 # create the stack object for estimating observation process y # cov_y = data.frame( year = rep(time, each = no_sites), year_2 = rep(time2, each = no_sites), spatial_ind = rep(1:no_sites, times = no_T), spatial_ind2 = no_sites + rep(1:no_sites, times = no_T) ) # site-specific random intercepts # Needs copies to include covariate twice s_index_copy = s_index names(s_index_copy) = c(&#39;spatial.field.copy&#39;, &quot;spatial.field.group.copy&quot;, &quot;spatial.field.repl.copy&quot;) s_index_copy2 = s_index names(s_index_copy2) = c(&#39;spatial.field.copy2&#39;, &quot;spatial.field.group.copy2&quot;, &quot;spatial.field.repl.copy2&quot;) stack_y_est = inla.stack( data = list( y = BlackSmokePrefData$bsmoke, #single model alldata = cbind(BlackSmokePrefData$bsmoke, NA, NA), Ntrials = rep(0, times = length(BlackSmokePrefData$bsmoke)) ), #joint model A = list(A_proj, A_proj, A_proj, 1), effects = list( c(s_index, list(Intercept = 1)), c(s_index_copy, list(Intercept_copy = 1)), c(s_index_copy2, list(Intercept_copy2 = 1)), cov_y ), tag = &#39;y_est&#39; ) formula_naive = y ~ -1 + Intercept + f(spatial.field, model = spde_obj) + f(spatial.field.copy, I(spatial.field.group.copy / no_T), model = spde_obj) + f(spatial.field.copy2, I((spatial.field.group.copy2 / no_T) ^ 2), model = spde_obj) + I(spatial.field.group / no_T) + I((spatial.field.group / no_T) ^ 2) + f(year, model = &#39;ar1&#39;) + f(spatial_ind, model = &quot;iid2d&quot;, n = no_sites * 2, constr = TRUE) + #random site-specific intercepts f(spatial_ind2, year, copy = &quot;spatial_ind&quot;) theta.ini = c( 1.597900, -1.277423, -0.443820, -1.441220, 0.036510, -1.441336, 0.016919, 4.462918, 1.437147, 4, 4, 4 ) out.naive = inla( formula_naive, family = &#39;gaussian&#39;, data = inla.stack.data(stack_y_est), control.predictor = list(A = inla.stack.A(stack_y_est), compute = F), control.compute = list(dic = F, config = T, cpo = F), control.inla = list(strategy = &quot;gaussian&quot;, int.strategy = &#39;eb&#39;), control.mode = list(theta = theta.ini, restart = T), verbose = T, num.threads = 2 ) We can now take posteriors from the model. m_samples = 50 samp &lt;- inla.posterior.sample(m_samples, out.naive) for (i in 1:m_samples) { samp[[i]]$latent = samp[[i]]$latent[-c(grep(&#39;Predictor&#39;, rownames(samp[[i]]$latent), fixed = T)), ] } Finally, summarize and plot the outputs: stepsize = 5000 / sd_x nxy = round(c(diff(range( BlackSmokePrefData$east )), diff(range( BlackSmokePrefData$north ))) / stepsize) ## how many points? proj_grid = inla.mesh.projector( mesh, xlim = range(BlackSmokePrefData$east), ylim = range(BlackSmokePrefData$north), dims = nxy ) A.grid = inla.spde.make.A(mesh, loc = cbind(BlackSmokePrefData$east, BlackSmokePrefData$north)) ## define placeholders tmp2 = matrix(0, nrow = no_T, ncol = mesh$n) year = (1:no_T) / no_T year2 = year ^ 2 Post_Array2 = array(0, dim = c(m_samples, no_T, mesh$n)) Post_Sites_Array2 = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) residuals_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) RI_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) # random intercepts RS_array = array(0, dim = c(m_samples, no_T, dim(BlackSmokePrefData)[1])) # random slopes for (i in 1:m_samples) { # Creating predictions for Y process for (j in 1:no_T) # loop over the years { Post_Array2[i, j, ] &lt;- samp[[i]]$latent[&#39;Intercept:1&#39;] + # Intercept samp[[i]]$latent[which(startsWith(prefix = &#39;year:&#39;, x = names(samp[[i]]$latent)))][j] * year[j] + # linear fixed effect #samp[[i]]$latent[&#39;year_2&#39;] * year2[j] + # quadratic fixed effect as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial.field:&#39;, x = names(samp[[i]]$latent)))]) + # Spatial Intercept as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial.field.copy:&#39;, x = names(samp[[i]]$latent)))]) * year[j] + # Spatial linear slope as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial.field.copy2:&#39;, x = names(samp[[i]]$latent)))]) * year2[j] # Spatial quadratic slope # Project onto the site locations and add random site-specific effects # Post_Sites_Array2[i, j, ] = as.numeric(A.grid %*% Post_Array2[i, j, ]) + as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial_ind:&#39;, x = names(samp[[i]]$latent)))[1:no_sites]]) + # random Intercept as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial_ind2:&#39;, x = names(samp[[i]]$latent)))[no_sites + (1:no_sites)]]) * year[j] # random slope # Extract the random intercepts # RI_array[i, j, ] = as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial_ind:&#39;, x = names(samp[[i]]$latent)))[1:no_sites]]) # Extract the random slopes # RS_array[i, j, ] = as.numeric(samp[[i]]$latent[which(startsWith(prefix = &#39;spatial_ind2:&#39;, x = names(samp[[i]]$latent)))[no_sites + (1:no_sites)]]) } } ## summarizing Post_mean2 &lt;- t(apply(Post_Array2, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) Post_sd2 &lt;- t(apply(Post_Array2, c(2, 3), function(x) { sd(x, na.rm = TRUE) })) Post_LCL2 &lt;- apply((apply(Post_Array2, c(1, 2), function(x) { mean(x, na.rm = TRUE) })), 2, quantile, probs = c(0.025)) Post_UCL2 &lt;- apply((apply(Post_Array2, c(1, 2), function(x) { mean(x, na.rm = TRUE) })), 2, quantile, probs = c(0.975)) RI_array &lt;- t(apply(RI_array, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) RS_array &lt;- t(apply(RS_array, c(2, 3), function(x) { mean(x, na.rm = TRUE) })) ## placeholders for uniform grid posterior blacksmoke values (not necessarily in dataset) post_Matrix_grid_mean2 = array(NA, dim = c(nxy[1], nxy[2], no_T)) post_Matrix_grid_sd2 = array(NA, dim = c(nxy[1], nxy[2], no_T)) for (i in 1:no_T) { post_Matrix_grid_mean2[, , i] = inla.mesh.project(proj_grid, Post_mean2[, i]) post_Matrix_grid_sd2[, , i] = inla.mesh.project(proj_grid, Post_sd2[, i]) } ## placeholders for observed (R1 = active, R0 = inactive) site blacksmoke values R1_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(R1_results2) = c(&#39;predciction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) R0_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(R0_results2) = c(&#39;predciction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) Grid_results2 = matrix(NA, nrow = no_T, ncol = 4) # prediction means, sd, LCL, UCL colnames(Grid_results2) = c(&#39;predciction mean&#39;, &#39;prediction sd&#39;, &#39;LCL&#39;, &#39;UCL&#39;) BlackSmokePrefData_original &lt;- read.csv(&quot;data/BlackSmokePrefData.csv&quot;) ## need original columns here for (j in 1:no_T) { R1_results2[j, 1] = mean(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], na.rm = T) R1_results2[j, 2] = sd(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], c(1), mean, na.rm = T)) R1_results2[j, 3] = quantile(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.025) R1_results2[j, 4] = quantile(apply(Post_Sites_Array2[, j, which(!is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.975) R0_results2[j, 1] = mean(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], na.rm = T) R0_results2[j, 2] = sd(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], c(1), mean, na.rm = T)) R0_results2[j, 3] = quantile(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.025) R0_results2[j, 4] = quantile(apply(Post_Sites_Array2[, j, which(is.na(BlackSmokePrefData_original[, 3 + j]))], 1, mean, na.rm = T), probs = 0.975) } for (i in 1:no_T) { Grid_results2[i, 1] = mean(post_Matrix_grid_mean2[, , i], na.rm = T) Grid_results2[i, 2] = mean(post_Matrix_grid_sd2[, , i], na.rm = T) } Grid_results2[, 3] = Post_LCL2 Grid_results2[, 4] = Post_UCL2 ## Revert to the original scale (exponentiating and scaling up) Grid_df_2 = data.frame( y = exp(c( Grid_results2[, 1], R1_results2[, 1], R0_results2[, 1] )) * mean(yearmeans), x = rep(66:96, times = 3), ymin = exp(c(Post_LCL2, R1_results2[, 3], R0_results2[, 3])) * mean(yearmeans), ymax = exp(c(Post_UCL2, R1_results2[, 4], R0_results2[, 4])) * mean(yearmeans), group = c( rep(&#39;Whole UK&#39;, times = 31), rep(&#39;R1&#39;, times = 31), rep(&#39;R0&#39;, times = 31) ) ) ## plotting Grid_plot_2 = ggplot(aes( x = x, y = y, ymin = ymin, ymax = ymax, alpha = 0.05 ), data = Grid_df_2) + geom_ribbon(aes( colour = group, alpha = 0.05, fill = group )) + xlab(&#39;year&#39;) + ylab(&#39;posterior mean bs on transformed scale&#39;) + ggtitle( &#39;Naive posterior means of BS at the selected, unselected sites and across the UK (R1, R0 and Whole UK resp.). &#39; ) Grid_plot_2 "],["Disease.html", "Chapter 9 Disease-spatial patterns Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 Example 9.3: Fitting a conditional spatial model in nimble and stan Example 9.4: Fitting a conditional spatial model using CARBayes Example 9.5: Fitting a conditional model using INLA", " Chapter 9 Disease-spatial patterns This chapter introduces disease mapping and contains the theory for spatial lattice processes and models for performing smoothing of risks over space. From this chapter, the reader will have gained an understanding of the following topics: Disease mapping, where we have seen how to improve estimates of risk by borrowing strength from adjacent regions which can reduce the instability inherent in risk estimates (SMRs) based on small expected numbers. Seen how smoothing can be performed using either the empirical Bayes or fully Bayesian approaches. Been introduced to computational methods for handling areal data. Learned about Besag’s seminal contributions to the field of spatial statistics including the very important concept of a Markov random field. Explored approaches to modelling a real data including the conditional auto regressive models. Seen how Bayesian spatial models for lattice data use nimble, stan, R and R–INLA. Example 9.1: Empirical Bayes and Bayes smoothing of COPD mortality for 2010 Nimble Following Example 6.2 we look back at the hospital admission rates for COPD, in England for 2010. We can implement the same model we used on Example 6.2 using nimble. # Load nimble library(nimble) library(sf) # to read shapefile # Load data # Reading in borders england &lt;- read_sf(&quot;data/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copdmortalityexpected.csv&quot;, row.names = 1) The following is the code for the Poisson-Gamma model implemented in nimble. Example9_1Nimble &lt;- nimbleCode({ for (i in 1:N) { Y[i] ~ dpois(mu[i]) # REVIEW: There is an intercept in the book, # but then we wouldn&#39;t be able to compare it to example 5.2 mu[i] &lt;- E[i]*exp(beta0)* theta[i] #mu[i] &lt;- E[i]* theta[i] # REVIEW: Same as before, the example in the book has theta[i] ~ Ga(a,a) theta[i] ~ dgamma(a, b) Y.fit[i] ~ dpois(mu[i]) } # Priors a ~ dexp(lambda_a) b ~ dexp(lambda_b) beta0 ~ dnorm(0, 10) }) # Define the constants, data and initials lists for the `nimble` model. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 N &lt;- length(y) # parameter of exponential prior for a lambda_a &lt;- 1 # parameter of exponential prior for b lambda_b &lt;- 1 # constants list constants &lt;- list( N = N, E = E, lambda_a = lambda_a, lambda_b = lambda_b ) # data list ex.data &lt;- list(Y = y) # initial values list inits &lt;- list( theta = rgamma(N, 1, 1), a = rexp(1, lambda_a), b = rexp(1, lambda_b), beta0 = 0, Y.fit = rpois(N, E) ) # parameters to monitor params &lt;- c(&quot;theta&quot;, &quot;a&quot;, &quot;b&quot;, &quot;beta0&quot;, &quot;Y.fit&quot;) # Run model in nimble mcmc.out &lt;- nimbleMCMC( code = Example9_1Nimble, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 50000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) Show the WAIC, effective sample size, and trace plots for some of the parameters. mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] 2419.274 ## Field &quot;lppd&quot;: ## [1] -1060.599 ## Field &quot;pWAIC&quot;: ## [1] 149.0374 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 9.069211 mvSamples &lt;- mcmc.out$samples # trace plot of a plot(mvSamples[, c(&quot;a&quot;)], bty = &quot;n&quot;) # trace plot of b plot(mvSamples[, c(&quot;b&quot;)], bty = &quot;n&quot;) # trace plot of beta0 plot(mvSamples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;) # trace plots of theta for (i in 1:3) plot(mvSamples[, c(paste(&quot;theta[&quot;, i, &quot;]&quot;, sep = &quot;&quot;))], bty = &quot;n&quot;) Now that we have checked the convergence of the chains we can plot the posterior mean and 95% CIs for each of the parameters. # Print posterior summary for parameters a and b summary(mvSamples[, c(&quot;a&quot;, &quot;b&quot;, &quot;beta0&quot;)]) ## ## Iterations = 1:2142 ## Thinning interval = 1 ## Number of chains = 2 ## Sample size per chain = 2142 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## a 12.1411 1.1075 0.016921 0.07589 ## b 8.2041 1.6788 0.025650 0.31808 ## beta0 -0.4367 0.1938 0.002961 0.06678 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## a 10.0376 11.3859 12.0940 12.8783 1.446e+01 ## b 5.6611 7.0104 7.9780 9.1322 1.242e+01 ## beta0 -0.7968 -0.5641 -0.4542 -0.3308 -6.206e-06 # posterior summaries of theta_i post_summary &lt;- mcmc.out$summary$all.chains |&gt; as.data.frame() |&gt; tibble::rownames_to_column(&quot;variable&quot;) # plot the mean and 95% CIs for the thetas post_theta &lt;- post_summary[grepl(&quot;theta\\\\[&quot;, post_summary$variable), ] par(mfrow = c(1, 1)) plot( post_theta$Mean, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(post_theta$`95%CI_low`), max(post_theta$`95%CI_upp`)) ) for (i in 1:N) segments(i, post_theta$`95%CI_low`[i], i, post_theta$`95%CI_upp`[i]) abline(h = 1, lwd = 2, lty = 2) # posterior summary of fitted values post_fitted &lt;- post_summary[grepl(&quot;Y.fit\\\\[&quot;, post_summary$variable), ] # plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, post_fitted$Mean, ylim = c(min(post_fitted$`95%CI_low`), max(post_fitted$`95%CI_upp`)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], post_fitted$`95%CI_low`[i], y[i], post_fitted$`95%CI_upp`[i]) abline(a = 0, b = 1) Stan Load necessary libraries and data. library(rstan) library(sf) # to read shapefile library(loo) # To calculate WAIC options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Load data # Reading in borders england &lt;- read_sf(&quot;data/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copdmortalityexpected.csv&quot;, row.names = 1) Write the stan model. This model is in a separate file called Example8_1.stan that will be called later. data { int&lt;lower=0&gt; N; real&lt;lower=0&gt; E[N]; // need to indicate that variable is strictly positive int&lt;lower=0&gt; Y[N]; real&lt;lower=0&gt;lambda_a; real&lt;lower=0&gt;lambda_b; } parameters { real&lt;lower=0&gt; theta[N]; real&lt;lower=0&gt; a; real&lt;lower=0&gt; b; } transformed parameters{ real &lt;lower=0&gt; mu[N]; for(i in 1:N){ mu[i]=E[i]*theta[i]; } } model { // likelihood function and prior for theta for(i in 1:N){ Y[i] ~ poisson(mu[i]); theta[i]~gamma(a,b); } a~exponential(lambda_a); b~exponential(lambda_b); } generated quantities { vector [N] log_lik; int&lt;lower=0&gt; yfit [N]; //computing the log_likelihood for each value of the mean mu and the fitted values for(i in 1:N){ log_lik[i]=poisson_lpmf(Y[i] |mu[i]); yfit[i]=poisson_rng(mu[i]); } } Define the data for the model, similar to nimble. # observations y &lt;- observed$Y2010 # offset E &lt;- expected$E2010 N &lt;- length(y) # data list ex.data &lt;- list( N = length(y), Y = y, E = E, lambda_a = 1, lambda_b = 1 ) # Run the model in Stan Ex9_1Stan &lt;- stan( file = &quot;functions/Example9_1.stan&quot;, data = ex.data, chains = 3, iter = 10000, warmup = 3000, thin = 14, # QUESTION: should we explain this? control = list(adapt_delta = 0.8, max_treedepth = 15), init = &quot;random&quot;, pars = c(&quot;a&quot;, &quot;b&quot;, &quot;theta&quot;, &quot;log_lik&quot;, &quot;yfit&quot;), include = TRUE ) Compute the WAIC, show the trace plots and posterior summaries of the parameters. loglik0 &lt;- extract_log_lik(Ex9_1Stan) waic0 &lt;- waic(loglik0) waic0 ## ## Computed from 1500 by 324 log-likelihood matrix ## ## Estimate SE ## elpd_waic -1208.2 7.6 ## p_waic 147.9 3.5 ## waic 2416.3 15.2 ## ## 177 (54.6%) p_waic estimates greater than 0.4. We recommend trying loo instead. # traceplots of parameters a and b rstan::traceplot(Ex9_1Stan, pars = c(&quot;a&quot;, &quot;b&quot;)) # traceplots of parameter theta rstan::traceplot(Ex9_1Stan, pars = c(&quot;theta[1]&quot;, &quot;theta[2]&quot;, &quot;theta[3]&quot;)) summary_theta &lt;- summary(Ex9_1Stan, pars = c(&quot;theta&quot;), probs = c(0.05, 0.95))$summary |&gt; as.data.frame() par(mfrow = c(1, 1)) plot( summary_theta$mean, pch = 19, cex = 0.8, bty = &quot;n&quot;, xlab = &quot;Borough&quot;, ylab = &quot;Posterior Summary Rate&quot;, ylim = c(min(summary_theta$`5%`), max(summary_theta$`95%`)) ) for (i in 1:N) segments(i, summary_theta$`5%`[i], i, summary_theta$`95%`[i]) abline(h = 1, lwd = 2, lty = 2) # Posterior summary of fitted values summary_fit &lt;- summary(Ex9_1Stan, pars = c(&quot;yfit&quot;), probs = c(0.05, 0.95))$summary |&gt; as.data.frame() # Plot mean and 95% CIs for the fitted values par(mfrow = c(1, 1)) plot( y, summary_fit$mean, ylim = c(min(summary_fit$`5%`), max(summary_fit$`95%`)), xlab = &quot;Observed&quot;, ylab = &quot;Fitted&quot;, pch = 19, cex = 0.7, bty = &quot;n&quot; ) for (i in 1:N) segments(y[i], summary_fit$`5%`[i], y[i], summary_fit$`95%`[i]) abline(a = 0, b = 1) Example 9.3: Fitting a conditional spatial model in nimble and stan Nimble In this example we revisit the COPD data from example 6.3. library(ggplot2) # to plot map library(spdep) # read the shapefile (read_sf) and build neighbors list (poly2nb) library(nimble) # Add a theme to clear axes and background in ggplot (optional) theme_clear &lt;- function(){ theme( panel.grid.major = element_blank(), # remove background grid panel.grid.minor = element_blank(), panel.background = element_blank(), # remove grey background axis.line = element_line(colour = &quot;black&quot;) # remove axis line ) } # Load data # Reading in borders england &lt;- read_sf(&quot;data/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copdmortalityexpected.csv&quot;) covariates &lt;- read.csv(file = &quot;data/copdavgscore.csv&quot;) # Merge everything into one data frame copd_df &lt;- cbind(observed, expected) |&gt; merge( covariates, by.x = &quot;code&quot;, by.y = &quot;LA.CODE&quot;, all.x = TRUE, all.y = FALSE ) Analyze the relationship between COPD data and the average deprivation score which is a measure of the socioeconomic status of the patients. copd_df$SMR2010 = copd_df$Y2010 / copd_df$E2010 ggplot(copd_df) + geom_point(aes(x = Average.Score, y = SMR2010)) + theme_clear() + ylab(&quot;SMR 2010&quot;) + xlab(&quot;Average deprivation score&quot;) Define a function to obtain the number of neighbors. adjlist = function(W, N) { adj = 0 for (i in 1:N) { for (j in 1:N) { if (W[i, j] == 1) { adj = append(adj, j) } } } adj = adj[-1] return(adj) } Define the adjacency matrix and indexes for stan using the nb2mat and adjlist # Create the neighborhood W.nb &lt;- poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) # Define the spatial structure to use in stan N &lt;- length(unique(england$ID)) neigh &lt;- adjlist(W.mat, N) numneigh &lt;- apply(W.mat,2,sum) Example9_3Nimble &lt;- nimbleCode({ # Likelihood for (i in 1:N) { y[i] ~ dpois(lambda[i]) log(lambda[i]) &lt;- log(E[i]) + b[i] + beta0 + beta1*x[i] } # Priors beta0 ~ dnorm(0, sd = 1) beta1 ~ dnorm(0, sd = 1) b[1:N] ~ dcar_normal(adj[1:L], weights[1:L], num[1:N], tau, zero_mean = 1) tau &lt;- 1 / (sigma_b ^ 2) sigma_b ~ T(dnorm(0, sd = 1), 0,) # Fitted values and likelihood for WAIC for (i in 1:N) { fitted[i] ~ dpois(lambda[i]) } }) # Define the constants, data and initial values lists and run the model. # constants list constants &lt;- list( N = N, E = copd_df$E2010, L = length(neigh), adj = neigh, weights = rep(1, length(neigh)), num = as.vector(numneigh), p = 3 ) # data list ex.data &lt;- list(y = copd_df$Y2010, x = as.vector(scale(copd_df$Average.Score))) # vector of covariates inits &lt;- list( beta0 = rnorm(1), beta1 = rnorm(1), fitted = rpois(N, 2), sigma_b = 1, b = rnorm(N) ) params &lt;- c(&quot;beta0&quot;,&quot;beta1&quot;, &quot;fitted&quot;, &quot;b&quot;, &quot;sigma_b&quot;) # Run model in nimble mcmc.out &lt;- nimbleMCMC( code = Example9_3Nimble, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 80, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) Show the WAIC, effective sample size, and trace plots for some of the parameters. mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] 2389.864 ## Field &quot;lppd&quot;: ## [1] -1079.574 ## Field &quot;pWAIC&quot;: ## [1] 115.3584 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 335.8356 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;) plot(mcmc.out$samples[, c(&quot;b[2]&quot;)], bty = &quot;n&quot;) plot(mcmc.out$samples[, c(&quot;sigma_b&quot;)], bty = &quot;n&quot;) # Extract samples variables &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;,&quot;sigma_b&quot;) summary_CAR_nimble &lt;- mcmc.out$summary$all.chains summary_CAR_nimble[variables,] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 -0.06825928 -0.06846789 0.007621251 -0.08371931 -0.05366343 ## beta1 0.18241740 0.18265397 0.012631226 0.15729454 0.20713464 ## sigma_b 0.25842306 0.25805987 0.020903231 0.21990912 0.30148624 Map the mean of the posterior estimate for the latent effect. samples_CAR_b &lt;- summary_CAR_nimble[grepl(&quot;b\\\\[&quot;, rownames(summary_CAR_nimble)), ] |&gt; as.data.frame() observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) samples_CAR_b$ID &lt;- observed$ID CAR_nimble_merge &lt;- merge(england, samples_CAR_b, by = &quot;ID&quot;) ggplot() + # Choose spatial object and column for plotting geom_sf(data = CAR_nimble_merge, aes(fill = Mean)) + # Change legend&#39;s label labs(fill = &#39;Latent effects nimble&#39;) + # Clear background and plot borders theme_void() Stan Here, we implement the CAR model using stan. We will need two functions to structure the matrix of neighbors that will be needed in stan. adjlist = function(W, N) { adj = 0 for (i in 1:N) { for (j in 1:N) { if (W[i, j] == 1) { adj = append(adj, j) } } } adj = adj[-1] return(adj) } mungeCARdata4stan = function(adjBUGS, numBUGS) { N = length(numBUGS) nn = numBUGS N_edges = length(adjBUGS) / 2 node1 = vector(mode = &quot;numeric&quot;, length = N_edges) node2 = vector(mode = &quot;numeric&quot;, length = N_edges) iAdj = 0 iEdge = 0 for (i in 1:N) { for (j in 1:nn[i]) { iAdj = iAdj + 1 if (i &lt; adjBUGS[iAdj]) { iEdge = iEdge + 1 node1[iEdge] = i node2[iEdge] = adjBUGS[iAdj] } } } return (list( &quot;N&quot; = N, &quot;N_edges&quot; = N_edges, &quot;node1&quot; = node1, &quot;node2&quot; = node2 )) } This model is in a separate file called Example8_3.stan that will be called later. data { int&lt;lower=1&gt; N; int&lt;lower=1&gt; N_edges; int&lt;lower=1&gt; p; matrix[N,p] X; int&lt;lower=1, upper=N&gt; node1[N_edges]; // node1[i] adjacent to node2[i] int&lt;lower=1, upper=N&gt; node2[N_edges]; // and node1[i] &lt; node2[i] int&lt;lower=0&gt; y[N]; // count outcomes vector&lt;lower=0&gt;[N] E; // exposure } transformed data { vector[N] log_E = log(E); } parameters { real beta0; // intercept vector[p] beta; vector[N] s; // spatial effects real&lt;lower=0&gt; sigma_s; // marginal standard deviation of spatial effects } transformed parameters { vector[N] b; // latent effect b = sigma_s*s; } model { y ~ poisson_log(log_E + beta0 + X*beta + b); // This is the prior for s! (up to proportionality) target += -0.5 * dot_self(s[node1] - s[node2]); sum(s) ~ normal(0, 0.001 * N); beta0 ~ normal(0.0, 10.0); for(j in 1:p){ beta[j] ~ normal(0.0, 10.0); } sigma_s ~ normal(0.0,1.0); } generated quantities { vector[N] mu=exp(log_E + beta0 + X*beta + b); vector[N] lik; vector[N] log_lik; for(i in 1:N){ lik[i] = exp(poisson_lpmf(y[i] | mu[i] )); log_lik[i] = poisson_lpmf(y[i] | mu[i] ); } } Define the adjacency matrix and indexes for stan using the nb2mat and adjlist # Create the neighborhood W.nb &lt;- poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) # Define the spatial structure to use in stan N &lt;- length(unique(england$ID)) neigh &lt;- adjlist(W.mat, N) numneigh &lt;- apply(W.mat, 2, sum) nbs &lt;- mungeCARdata4stan(neigh, numneigh) # Define data and variables for Stan model y &lt;- copd_df$Y2010 E &lt;- copd_df$E2010 X &lt;- as.numeric(scale(copd_df$Average.Score)) ex.data &lt;- list( N = nbs$N, y = y, E = E, p = 1, X = as.matrix(X), N_edges = nbs$N_edges, node1 = nbs$node1, node2 = nbs$node2 ) Example9_3Stan &lt;- stan( file = &quot;functions/Example9_3.stan&quot;, data = ex.data, warmup = 10000, iter = 20000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;beta&quot;,&quot;sigma_s&quot;, &quot;b&quot;, &quot;log_lik&quot;), include = TRUE ) Show traceplots. #computing WAIC using the package loo loglikcar &lt;- extract_log_lik(Example9_3Stan) waiccar &lt;- waic(loglikcar) ## Warning: ## 88 (27.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. waiccar ## ## Computed from 2000 by 324 log-likelihood matrix ## ## Estimate SE ## elpd_waic -1194.8 14.3 ## p_waic 115.4 7.1 ## waic 2389.6 28.6 ## ## 88 (27.2%) p_waic estimates greater than 0.4. We recommend trying loo instead. traceplot(Example9_3Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma_s&quot;)) Show the posterior summary for the parameters if interest. Keep in mind that in the stan model the we are sampling the standard deviation of the random effect unlike CARBayes where the variance is obtained. # Extract samples summary_CAR_stan &lt;- summary( Example9_3Stan, pars = c(&quot;beta0&quot;, &quot;beta&quot;,&quot;sigma_s&quot;), probs = c(0.025, 0.975) ) summary_CAR_stan$summary ## mean se_mean sd 2.5% 97.5% n_eff ## beta0 -0.06934503 0.0001667032 0.007407282 -0.08437349 -0.05517127 1974.377 ## beta[1] 0.18210647 0.0002901973 0.012190021 0.15845312 0.20630585 1764.502 ## sigma_s 0.25884831 0.0004901313 0.020429237 0.22026038 0.29995291 1737.318 ## Rhat ## beta0 0.9992447 ## beta[1] 0.9995802 ## sigma_s 1.0018920 Map the mean of the posterior estimate for the latent effect. summary_CAR_stan_b &lt;- summary( Example9_3Stan, pars = c(&quot;b&quot;), probs = c(0.025, 0.975) ) observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) summary_CAR_stan_b$ID &lt;- observed$ID CAR_stan_merge &lt;- merge(england, summary_CAR_stan_b, by = &quot;ID&quot;) ggplot() + # Choose spatial object and column for plotting geom_sf(data = CAR_stan_merge, aes(fill = summary.mean)) + # Change legend&#39;s label labs(fill = &#39;Latent effects stan&#39;) + # Clear background and plot borders theme_void() Example 9.4: Fitting a conditional spatial model using CARBayes Load the necessary libraries and the COPD data. As in example 5.2 englandlocalauthority.shp and it’s related files contain the location, shape, and attributes of English local authorities. library(CARBayes) library(ggplot2) library(sf) library(spdep) # Add a theme to clear axes and background in ggplot (optional) theme_clear &lt;- function(){ theme( panel.grid.major = element_blank(), # remove background grid panel.grid.minor = element_blank(), panel.background = element_blank(), # remove grey background axis.line = element_blank()# remove axis line ) } # Reading in borders england &lt;- read_sf(&quot;data/englandlocalauthority.shp&quot;) # Reading in data observed &lt;- read.csv(file = &quot;data/copdmortalityobserved.csv&quot;, row.names = 1) expected &lt;- read.csv(file = &quot;data/copdmortalityexpected.csv&quot;) covariates &lt;- read.csv(file = &quot;data/copdavgscore.csv&quot;) # Merge everything into one data frame copd_df &lt;- cbind(observed, expected) |&gt; merge( covariates, by.x = &quot;code&quot;, by.y = &quot;LA.CODE&quot;, all.x = TRUE, all.y = FALSE ) copd_df$Average.Score &lt;- as.numeric(scale(copd_df$Average.Score)) To calculate the smoother SMRs, we first need to create a neighborhood structure. The functions poly2nb() and nb2mat() from the spdep package can be used to create this # Create the neighborhood W.nb &lt;- poly2nb(england, row.names = rownames(england)) # Creates a matrix for following function call W.mat &lt;- nb2mat(W.nb, style = &quot;B&quot;) Here, we use first neighbors to define the structure, so any local authority sharing a border are considered neighbors. The function S.CARleroux() allows us to use the neighborhood structure and performs a Bayesian analysis to create a smoothed set of observed values. # Running smoothing model Ex9_4 &lt;- S.CARleroux( # Model Formula formula = Y2010 ~ offset(log(E2010)) + Average.Score, # data frame with data data = copd_df, # Choosing Poisson Regression family = &quot;poisson&quot;, # Neighborhood matrix W = W.mat, # Number of burn in samples burnin = 20000, # Number of MCMC samples n.sample = 100000, thin = 10, rho = 1 ) We can extract the new smoother values from the model output and divide them by the expected values in order to compare both methods. # Creating a dataset with smoothed SMRs in 2010 SMR2010 &lt;- Ex9_4$fitted.values / copd_df$E2010 SMR_smooth &lt;- as.data.frame(SMR2010, row.names = rownames(observed)) # Printing first six rows of smoothed SMRs head(SMR_smooth) ## SMR2010 ## 00AA 0.6750542 ## 00AB 1.3192050 ## 00AC 0.6654749 ## 00AD 0.9436303 ## 00AE 0.7471261 ## 00AF 0.8293328 # Summarizing smoothed SMRs summary(SMR_smooth) ## SMR2010 ## Min. :0.6033 ## 1st Qu.:0.7897 ## Median :0.9013 ## Mean :0.9638 ## 3rd Qu.:1.0815 ## Max. :1.7969 # Summary of the parameters under the CAR model. Ex9_4$summary.results[] ## Mean 2.5% 97.5% n.sample % accept n.effective Geweke.diag ## (Intercept) -0.0688 -0.0840 -0.0536 8000 31.5 7908.9 0.7 ## Average.Score 0.1816 0.1578 0.2052 8000 31.5 2366.9 -0.7 ## tau2 0.0649 0.0464 0.0867 8000 100.0 3379.8 0.2 ## rho 1.0000 1.0000 1.0000 NA NA NA NA Use ggplot() and geom_sf() to plot the map of the latent spatial effect. observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) phi_car &lt;- Ex9_4$samples$phi latent_car_df &lt;- data.frame( phi_mean = apply(phi_car, 2, mean), phi_sd = apply(phi_car, 2, sd) ) # Combine latent spatial effect with england dataset latent_car_df$ID &lt;- observed$ID latent_car_england &lt;- merge(england, latent_car_df, by = &quot;ID&quot;) # Creating map of smoothed SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = latent_car_england, aes(fill = phi_mean)) + labs(fill = &#39;Latent effects CARbayes&#39;) + # Clear background and plot borders theme_void() Example 9.5: Fitting a conditional model using INLA # run the INLA model Ex9_5 &lt;- inla( Y2010 ~ Average.Score + f(ID , model = &quot;besag&quot;, graph = &quot;UK.adj&quot;), family = &quot;poisson&quot;, E = E2010, data = copd_df, control.predictor = list(compute = TRUE) ) Summarize the results # Summarizing smoothed SMRs summary(Ex9_5) ## ## Call: ## c(&quot;inla.core(formula = formula, family = family, contrasts = contrasts, ## &quot;, &quot; data = data, quantiles = quantiles, E = E, offset = offset, &quot;, &quot; ## scale = scale, weights = weights, Ntrials = Ntrials, strata = strata, ## &quot;, &quot; lp.scale = lp.scale, link.covariates = link.covariates, verbose = ## verbose, &quot;, &quot; lincomb = lincomb, selection = selection, control.compute ## = control.compute, &quot;, &quot; control.predictor = control.predictor, ## control.family = control.family, &quot;, &quot; control.inla = control.inla, ## control.fixed = control.fixed, &quot;, &quot; control.mode = control.mode, ## control.expert = control.expert, &quot;, &quot; control.hazard = control.hazard, ## control.lincomb = control.lincomb, &quot;, &quot; control.update = ## control.update, control.lp.scale = control.lp.scale, &quot;, &quot; ## control.pardiso = control.pardiso, only.hyperparam = only.hyperparam, ## &quot;, &quot; inla.call = inla.call, inla.arg = inla.arg, num.threads = ## num.threads, &quot;, &quot; blas.num.threads = blas.num.threads, keep = keep, ## working.directory = working.directory, &quot;, &quot; silent = silent, inla.mode ## = inla.mode, safe = FALSE, debug = debug, &quot;, &quot; .parent.frame = ## .parent.frame)&quot;) ## Time used: ## Pre = 1.06, Running = 0.288, Post = 0.0499, Total = 1.4 ## Fixed effects: ## mean sd 0.025quant 0.5quant 0.975quant mode kld ## (Intercept) -0.069 0.008 -0.084 -0.069 -0.054 -0.069 0 ## Average.Score 0.182 0.012 0.158 0.182 0.206 0.182 0 ## ## Random effects: ## Name Model ## ID Besags ICAR model ## ## Model hyperparameters: ## mean sd 0.025quant 0.5quant 0.975quant mode ## Precision for ID 15.94 2.62 11.52 15.70 21.75 15.23 ## ## Marginal log-Likelihood: -1493.27 ## is computed ## Posterior summaries for the linear predictor and the fitted values are computed ## (Posterior marginals needs also &#39;control.compute=list(return.marginals.predictor=TRUE)&#39;) observed &lt;- tibble::rownames_to_column(observed, &quot;ID&quot;) phi_car &lt;- Ex9_5$summary.random$ID latent_car_df &lt;- data.frame( phi_mean = phi_car$mean, phi_sd = phi_car$sd ) # Combine latent spatial effect with england dataset latent_car_df$ID &lt;- observed$ID latent_car_england &lt;- merge(england, latent_car_df, by = &quot;ID&quot;) # Creating map of smoothed SMRs in England in 2010 ggplot() + # Choose spatial object and column for plotting geom_sf(data = latent_car_england, aes(fill = phi_mean)) + labs(fill = &#39;Latent effects INLA&#39;) + # Clear background and plot borders theme_void() "],["hazards.html", "Chapter 10 Environmental hazards-spatial models Example 10.1 Spatial patterns of benzene concentrations in Montreal, QC, Canada Example 10.2: Examining the log concentrations of benzene in Montreal Example 10.3: Mapping the locations of ozone monitoring sites in New York State Example 10.5: Variogram Example 10.6 Basic spatial modelling and prediction of benzene in Montreal Example 10.9 Spatial modelling of Benzene in Montreal Example 10.10: Spatial predictions Example 10.11: INLA, creating a mesh Example 10.12: Fitting an SPDE model using R–INLA: benzene concentration in Montreal Example 10.13: Directional variograms Example 10.14: Spatial modeling of malaria in Gambia", " Chapter 10 Environmental hazards-spatial models This chapter contains the basic theory for spatial processes and a number of approaches to modelling point-referenced spatial data. From this chapter, the reader will have gained an understanding of the following topics: Visualization techniques needed for both exploring and analyzing spatial data and communicating its features through the use of maps. Exploring the underlying structure of spatial data and methods for characterizing dependence over space. Second-order theory for spatial processes including the covariance. The variogram for measuring spatial associations. Stationarity and isotropy. Methods for spatial prediction, using both classical methods (kriging) as well as modern methods (Bayesian kriging). Non-stationarity fields. Example 10.1 Spatial patterns of benzene concentrations in Montreal, QC, Canada Map the locations of the monitoring stations in Montreal. library(cowplot) library(geoR) library(ggmap) library(spdep) # Load data on benzene concentration in Montreal benzene &lt;- read.csv(&quot;data/montreal_benzene_apr.csv&quot;) # TODO: Add description of the data, this is the April campaign # create a new variable in &quot;sp&quot; format and define coordinates benzene_geo &lt;- benzene coordinates(benzene_geo) &lt;- ~ lon + lat proj4string(benzene_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) # specify the bounding box latLongBox = bbox(benzene_geo) location = c(latLongBox[1, 1] - 0.05, latLongBox[2, 1] - 0.05, latLongBox[1, 2] + 0.1, latLongBox[2, 2] + 0.05) # create map with location dots marked on it in MontrelBenzeneMap &lt;- get_stamenmap(bbox = location, zoom = 10) ggmap(MontrelBenzeneMap) + geom_point(data = benzene, aes(x = lon, y = lat, size = Benzene), col = &quot;#011f4b&quot;, alpha = 0.45) + theme_void() Using the geoR package we can also plot the following: the locations of the sampling sites the concentrations of ozone in relation to the x and y coordinates and a histogram of the concentrations indicating the distribution of concentrations together with an estimate of the density. # convert data to utm coordinates benzene_utm &lt;- spTransform(benzene_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_utm) colnames(benzene_utm_df) &lt;- c(&quot;Benzene&quot;, &quot;X&quot;, &quot;Y&quot;) # Save as geodata to generate the geodata plot benzene_geodata &lt;- as.geodata(benzene_utm_df, coords.col = 2:3, data.col = 1) plot(benzene_geodata) Example 10.2: Examining the log concentrations of benzene in Montreal # Histogram for the log of the benzene concentration par(mfrow=c(1,2)) log_histogram &lt;- hist(log(benzene$Benzene), main = &quot;&quot;, xlab = &quot;log(Benzene)&quot;) qqnorm(log(benzene$Benzene), bty = &quot;n&quot;) qqline(log(benzene$Benzene)) Example 10.3: Mapping the locations of ozone monitoring sites in New York State # Load the metadata giving the site coordinates ny_data &lt;- read.csv(&quot;data/NY_metadata.txt&quot;, sep=&quot;&quot;) # Now copy ny_data into ny_data_sp and convert data to &quot;sp&quot; format ny_data_sp &lt;- ny_data coordinates(ny_data_sp) &lt;- ~Longitude+Latitude # assign a reference system to ny_data_sp proj4string(ny_data_sp) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) # We next specify a bounding box - a 2 x 2 matrix of corners of the geographic # area. Then specify the range of locations within the box. # Note: location must bounding box format be in left -bottom -right -top latLongBox &lt;- bbox(ny_data_sp) location &lt;- c(latLongBox [1, 1] - 0.2 , latLongBox [2, 1] - 0.2, latLongBox [1, 2] + 0.2 , latLongBox [2, 2] + 0.2) # Now create the map with location dots NYmap &lt;- get_stamenmap( bbox = location, zoom = 8 ) ggmap(NYmap) + geom_point( data = ny_data, aes(x = Longitude , y = Latitude), size = 4, color = &quot;darkred&quot; ) + theme_void() Example 10.5: Variogram library(gstat) benzene_utm_geo &lt;- benzene_utm_df # get the coordinates in kms and turn into a Spatial object benzene_utm_geo[,c(&quot;X&quot;, &quot;Y&quot;)] &lt;- benzene_utm_geo[,c(&quot;X&quot;, &quot;Y&quot;)]/1000 coordinates(benzene_utm_geo) &lt;- ~ X + Y # Estimate variogram intercept only benzene_inter_vgm &lt;- gstat::variogram(log(Benzene)~ 1, data = benzene_utm_geo, cutoff = 20, # cutoff distance width = 20/10 # bins width ) benzene_inter_vgm_fit &lt;- fit.variogram(benzene_inter_vgm, model = vgm(0.1, &quot;Exp&quot;, 15, 0.02)) benzene_inter_vgm_fit ## model psill range ## 1 Nug 0.01616521 0.00000 ## 2 Exp 0.17301314 23.97153 # Estimate variogram using coordinates benzene_vgm &lt;- variogram(log(Benzene)~ X + Y, data = benzene_utm_geo, cutoff = 20, # cutoff distance width = 20/10 # bins width ) benzene_vgm_fit &lt;- fit.variogram(benzene_vgm, model = vgm(0.1, &quot;Exp&quot;, 3, 0.02)) benzene_vgm_fit ## model psill range ## 1 Nug 0.01638881 0.000000 ## 2 Exp 0.05598753 7.365469 plot_inter_variog &lt;- plot(benzene_inter_vgm, benzene_inter_vgm_fit, bty = &quot;n&quot;) plot_coord_variog &lt;- plot(benzene_vgm, benzene_vgm_fit, bty = &quot;n&quot;) plot_grid(plot_inter_variog, plot_coord_variog, labels = &quot;auto&quot;) Example 10.6 Basic spatial modelling and prediction of benzene in Montreal # Generate grid MtlPred &lt;- expand.grid(seq (580, 615 , 0.5), seq (5020, 5060 , 0.5) ) # change names grid names(MtlPred)[ names(MtlPred)==&quot;Var1&quot;] &lt;- &quot;X&quot; names(MtlPred)[ names(MtlPred)==&quot;Var2&quot;] &lt;- &quot;Y&quot; # make the grid a Spatial object coordinates (MtlPred) = ~ X + Y gridded(MtlPred) = TRUE # define the model based on the variogram fit from the previous example mod &lt;- vgm (0.053 , &quot;Exp&quot;, 5.54, 0.0122) # use ordinary kriging to predict values in the grid x &lt;- krige(log(Benzene) ~ X + Y, benzene_utm_geo , MtlPred , model = mod ) ## [using universal kriging] # Plot the ordinary kriging predictions and their variance monitor_loc &lt;- list(&#39;sp.points&#39;, benzene_utm_geo, pch=19, cex=.8, col=&#39;cornsilk4&#39;) krig_pred &lt;- spplot ( x[&quot;var1.pred&quot;], main = &quot;Ordinary kriging predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) krig_var &lt;- spplot ( x[&quot;var1.var&quot;], main = &quot;Ordinary kriging variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(krig_pred, krig_var, labels = &quot;auto&quot;) Example 10.9 Spatial modelling of Benzene in Montreal Nimble library(coda) library(geoR) library(magrittr) library(nimble) library(spdep) library(tidyverse) library(tidybayes) # Load data on benzene concentration in Montreal benzene &lt;- read.csv(&quot;data/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_geo &lt;- benzene coordinates(benzene_geo) &lt;- ~ lon + lat proj4string(benzene_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_utm &lt;- spTransform(benzene_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_utm) # Change coordinates to kilometers and observations to the log scale Mtl_benzene_sample &lt;- data.frame( y = log(benzene_utm_df$Benzene), easting = benzene_utm_df$lon / 1000, northing = benzene_utm_df$lat / 1000 ) # Compute the distance matrix obsCoords &lt;- unname(as.matrix(Mtl_benzene_sample[,c(&quot;easting&quot;, &quot;northing&quot;)])) obsDist &lt;- fields::rdist(obsCoords) Example10_9Code &lt;- nimbleCode ({ # Covariance matrix spatial effect Sigma[1:n, 1:n] &lt;- sigma_sq * exp(-distMatrix[1:n, 1:n] / phi) + tau_sq * identityMatrix(d = n) for (site in 1:n) { mean.site[site] &lt;- beta0 + beta1 * easting[site] + beta2 * northing[site] } y[1:n] ~ dmnorm(mean.site[1:n], cov = Sigma[1:n, 1:n]) # Set up the priors for the spatial model sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma^2 tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau_sq &lt;- tau^2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv # prior for the coefficients beta0 ~ dnorm (0, 10) beta1 ~ dnorm (0, 10) beta2 ~ dnorm (0, 10) }) # Define the constants, data, parameters and initial values set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) constants &lt;- list(n = nrow(Mtl_benzene_sample)) ex.data &lt;- list(y = Mtl_benzene_sample$y, easting = easting_scaled, northing = northing_scaled, distMatrix = obsDist) params &lt;- c( &quot;beta0&quot;, &quot;beta1&quot;,&quot;beta2&quot;, &quot;phi&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;tau_sq&quot;, &quot;sigma_sq&quot;) inits &lt;- list( sigma = 0.1, phi_inv = 6/max(obsDist), tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out &lt;- nimbleMCMC( code = Example10_9Code, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] -32.37684 ## Field &quot;lppd&quot;: ## [1] 19.03084 ## Field &quot;pWAIC&quot;: ## [1] 2.842421 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 985.2176 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out$samples[, c(&quot;beta2&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) plot(mcmc.out$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) mcmc.out$summary$all.chains[c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), ] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 0.143589662 0.15187505 0.081381727 -4.284934e-02 0.27866526 ## beta1 0.051028164 0.04806334 0.060619017 -6.338295e-02 0.17328888 ## beta2 0.077775628 0.08250220 0.068519017 -7.013084e-02 0.20323633 ## sigma_sq 0.056087887 0.05202343 0.022624353 2.770244e-02 0.11140808 ## tau_sq 0.009128758 0.00841011 0.006709036 5.744089e-05 0.02380432 ## phi 3.697646345 3.09115863 2.228726265 1.463844e+00 9.40053808 # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) Following the posterior predictive distribution we have to define a model for the predictions. # Extract samples from nimble model tidy_post_samples &lt;- mcmc.out$samples %&gt;% tidy_draws() # Extract posterior samples for each of the parameters of interest post_beta0 &lt;- tidy_post_samples$beta0 post_beta1 &lt;- tidy_post_samples$beta1 post_beta2 &lt;- tidy_post_samples$beta2 post_sigmasq &lt;- tidy_post_samples$sigma_sq post_phi &lt;- tidy_post_samples$phi post_tausq &lt;- tidy_post_samples$tau_sq # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) n0 &lt;- nrow(predCoords_sc) L &lt;- length(post_tausq) obsMu &lt;- cbind(1, easting_scaled, northing_scaled) %*% t(cbind(post_beta0, post_beta1, post_beta2)) predMu &lt;- cbind(1, as.matrix(predCoords_sc)) %*% t(cbind(post_beta0, post_beta1, post_beta2)) pred2obsDist &lt;- fields::rdist(predCoords, obsCoords) Rcpp::sourceCpp(&quot;functions/prediction_marginal_gp12.cpp&quot;) args(prediction_marginal_gp12) ## function (y, obsMu, predMu, obsDistMat, pred2ObsDistMat, sigmasq, ## phi, tausq, iterprint) ## NULL system.time( pred_samples &lt;- prediction_marginal_gp12( y = Mtl_benzene_sample$y, obsMu = obsMu, predMu = predMu, obsDistMat = obsDist, pred2ObsDistMat = pred2obsDist, sigmasq = post_sigmasq, phi = post_phi, tausq = post_tausq, iterprint = 1000 ) ) ## Prediction upto the 0th MCMC sample is completed ## Prediction upto the 1000th MCMC sample is completed ## Prediction upto the 2000th MCMC sample is completed ## user system elapsed ## 201.33 0.37 283.21 str(pred_samples) ## num [1:5751, 1:2856] -0.116 -0.805 -0.7 -0.327 -0.402 ... predict_res_dt &lt;- data.frame( xcoord = predCoords[, 1], ycoord = predCoords[, 2], post.mean = apply(pred_samples, 1, mean), post.var = apply(pred_samples, 1, var), q2.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.025)), q50 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.5)), q97.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.975)) ) x$nimble.pred &lt;- predict_res_dt$post.mean x$nimble.var &lt;- predict_res_dt$post.var nimble_pred &lt;- spplot ( x[&quot;nimble.pred&quot;], main = &quot;Nimble spatial predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) nimble_var &lt;- spplot ( x[&quot;nimble.var&quot;], main = &quot;Nimble predictions variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(nimble_pred, nimble_var, labels = &quot;auto&quot;) Stan benzene_data &lt;- read.csv(&quot;data/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_data_geo &lt;- benzene_data coordinates(benzene_data_geo) &lt;- ~ lon + lat proj4string(benzene_data_geo) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_data_utm &lt;- spTransform(benzene_data_geo, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame benzene_utm_df &lt;- as.data.frame(benzene_data_utm) # change the observed values to the log scale and the coordinates to km&#39;s Mtl_benzene_sample &lt;- data.frame( y = log(benzene_utm_df$Benzene), easting = benzene_utm_df$lon / 1000, northing = benzene_utm_df$lat / 1000 ) # Compute the distance matrix obsCoords &lt;- unname(as.matrix(Mtl_benzene_sample[,c(&quot;easting&quot;, &quot;northing&quot;)])) obsDist &lt;- fields::rdist(obsCoords) data { int&lt;lower=1&gt; N; int&lt;lower=0&gt; p; vector[N] y; matrix[N,N] dist_matrix; matrix[N,p] X; } parameters { real&lt;lower=0&gt; phi; real&lt;lower=0&gt; tau; real&lt;lower=0&gt; sigma; vector[p] beta; real beta0; } transformed parameters{ real&lt;lower=0&gt; sigma_sq = square(sigma); real&lt;lower=0&gt; tau_sq = square(tau); } model { vector[N] mu; matrix[N, N] L; matrix[N, N] Sigma; for(i in 1:(N-1)){ for(j in (i+1):N){ Sigma[i,j] = sigma_sq*exp(-dist_matrix[i,j]/phi); Sigma[j,i] = Sigma[i,j]; } } // diagonal elements for(i in 1:N) { Sigma[i,i] = sigma_sq + tau_sq; mu[i] = beta0 + X[i,]*beta; } L = cholesky_decompose(Sigma); beta0 ~ normal(0,10); beta ~ normal(0,10); phi ~ inv_gamma(5, 5); tau ~ cauchy(0,1); sigma ~ cauchy(0,1); y ~ multi_normal_cholesky(mu, L); } set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) N &lt;- nrow(benzene_utm_df) # Change coordinates to kilometers X &lt;- data.frame(easting = easting_scaled, northing = northing_scaled) ex.data &lt;- list( N = N, p = 2, y = log(benzene_utm_df$Benzene), dist_matrix = obsDist, X = as.matrix(X) ) Example10_9Stan &lt;- stan( file = &quot;functions/Example10_9.stan&quot;, data = ex.data, warmup = 10000, iter = 20000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;beta&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), include = TRUE ) rstan::traceplot(Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;,&quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;)) summary_exp_stan &lt;- summary( Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;, &quot;phi&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;), probs = c(0.025, 0.975) ) summary_exp_stan$summary ## mean se_mean sd 2.5% 97.5% n_eff ## beta0 0.153151957 0.0017454630 0.082542739 -0.0280454381 0.30104542 2236.333 ## beta[1] 0.048260898 0.0014210620 0.060763462 -0.0628368700 0.17050377 1828.349 ## beta[2] 0.079958055 0.0015912122 0.068248714 -0.0763104700 0.19962566 1839.640 ## phi 3.577291239 0.0500854445 2.111837928 1.5099031977 9.06193864 1777.862 ## sigma_sq 0.055305541 0.0004872241 0.021205779 0.0277406700 0.10511771 1894.310 ## tau_sq 0.009140308 0.0001587922 0.006624496 0.0001113095 0.02330228 1740.394 ## Rhat ## beta0 1.0000400 ## beta[1] 1.0010770 ## beta[2] 0.9997490 ## phi 0.9997624 ## sigma_sq 1.0022418 ## tau_sq 0.9998860 # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) # Extract samples from nimble model stan_post_samples &lt;- rstan::extract(Example10_9Stan, pars = c(&quot;beta0&quot;,&quot;beta&quot;, &quot;phi&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;)) # Extract posterior samples for each of the parameters of interest post_beta0 &lt;- stan_post_samples$beta0 post_beta &lt;- stan_post_samples$beta # post_beta2 &lt;- stan_post_samples$beta2 post_sigmasq &lt;- stan_post_samples$sigma_sq post_phi &lt;- stan_post_samples$phi post_tausq &lt;- stan_post_samples$tau_sq # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) n0 &lt;- nrow(predCoords_sc) L &lt;- length(post_tausq) obsMu &lt;- cbind(1, easting_scaled, northing_scaled) %*% t(cbind(post_beta0, post_beta)) predMu &lt;- cbind(1, as.matrix(predCoords_sc)) %*% t(cbind(post_beta0, post_beta)) pred2obsDist &lt;- fields::rdist(predCoords, obsCoords) Rcpp::sourceCpp(&quot;functions/prediction_marginal_gp12.cpp&quot;) args(prediction_marginal_gp12) ## function (y, obsMu, predMu, obsDistMat, pred2ObsDistMat, sigmasq, ## phi, tausq, iterprint) ## NULL system.time( pred_samples &lt;- prediction_marginal_gp12( y = Mtl_benzene_sample$y, obsMu = obsMu, predMu = predMu, obsDistMat = obsDist, pred2ObsDistMat = pred2obsDist, sigmasq = post_sigmasq, phi = post_phi, tausq = post_tausq, iterprint = 1000 ) ) ## Prediction upto the 0th MCMC sample is completed ## Prediction upto the 1000th MCMC sample is completed ## user system elapsed ## 142.29 0.16 196.44 predict_res_dt &lt;- data.frame( xcoord = predCoords[, 1], ycoord = predCoords[, 2], post.mean = apply(pred_samples, 1, mean), post.var = apply(pred_samples, 1, var), q2.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.025)), q50 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.5)), q97.5 = apply(pred_samples, 1, function(x) quantile(x, prob = 0.975)) ) x$stan.pred &lt;- predict_res_dt$post.mean x$stan.var &lt;- predict_res_dt$post.var stan_pred &lt;- spplot ( x[&quot;stan.pred&quot;], main = &quot;Stan spatial predictions &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(-0.3, 0.8, 0.02) ) stan_var &lt;- spplot ( x[&quot;stan.var&quot;], main = &quot;Stan predictions variance &quot;, col.regions = viridis::plasma(60), sp.layout = list(monitor_loc), at = seq(0, 0.2, 0.01) ) plot_grid(stan_pred, stan_var, labels = &quot;auto&quot;) Example 10.10: Spatial predictions NOTE: To Alex, I just added the spatial predictions for nimble, do we want it for Stan too? Also note that I am just doing for 5 locations (please double-check my equations) and it instead of running in 2 minutes it runs in 6 (in case you want to highlight that). But we should discuss the order of this cause I am doing the predictions with Paritosh’s code in the previous section. # Obtain coordinates for predictions # note that we are using the same coordinates as the one generated for # the kriging example Mtl_centroids_df &lt;- as.data.frame(MtlPred) predCoords &lt;- unname(as.matrix(Mtl_centroids_df)) # Scale coordinates for predictive locations predCoords_sc &lt;- predCoords predCoords_sc[,1] &lt;- (predCoords_sc[,1] - mean(Mtl_benzene_sample$easting)) / sd(Mtl_benzene_sample$easting) predCoords_sc[,2] &lt;- (predCoords_sc[,2] - mean(Mtl_benzene_sample$northing)) / sd(Mtl_benzene_sample$northing) # As an example we are only going to predict 5 locations nu &lt;- 5 predCoords_five &lt;- predCoords_sc[1:nu,] obs2predDist &lt;- fields::rdist(obsCoords, predCoords[1:nu,]) pred2predDist &lt;- fields::rdist(predCoords[1:nu,]) Example10_10Code &lt;- nimbleCode ({ # Covariance matrix spatial effect Sigma_obs[1:n, 1:n] &lt;- sigma_sq * exp(-distMatrix[1:n, 1:n] / phi) + tau_sq * identityMatrix(d = n) Sigma_unobs[1:nu, 1:nu] &lt;- sigma_sq * exp(-distMatrixUnobs[1:nu, 1:nu] / phi) + tau_sq * identityMatrix(d = nu) Sigma_obs_unobs[1:n, 1:nu] &lt;- sigma_sq * exp(-distMatrixObsUnobs[1:n, 1:nu] / phi) for (site in 1:n) { mean.site[site] &lt;- beta0 + beta1 * easting[site] + beta2 * northing[site] } y[1:n] ~ dmnorm(mean.site[1:n], cov = Sigma_obs[1:n, 1:n]) # Spatial predictions for (usite in 1:nu) { mean.unobs.site[usite] &lt;- beta0 + beta1 * easting_unobs[usite] + beta2 * northing_unobs[usite] } mean_sigma[1:nu] &lt;- t(Sigma_obs_unobs[1:n, 1:nu])%*% inverse(Sigma_obs[1:n, 1:n])%*%(y[1:n] - mean.site[1:n]) mu_unobs[1:nu] &lt;- mean.unobs.site[1:nu] + mean_sigma[1:nu] cov_unobs[1:nu, 1:nu] &lt;- Sigma_unobs[1:nu, 1:nu] - t(Sigma_obs_unobs[1:n, 1:nu]) %*% inverse(Sigma_obs[1:n, 1:n]) %*% Sigma_obs_unobs[1:n, 1:nu] y_unobs[1:nu] ~ dmnorm(mu_unobs[1:nu], cov = cov_unobs[1:nu, 1:nu]) # Set up the priors for the spatial model sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma ^ 2 tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) tau_sq &lt;- tau ^ 2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv # prior for the coefficients beta0 ~ dnorm (0, 10) beta1 ~ dnorm (0, 10) beta2 ~ dnorm (0, 10) }) # Define the constants, data, parameters and initial values set.seed(1) easting_scaled &lt;- as.vector(scale(Mtl_benzene_sample$easting)) northing_scaled &lt;- as.vector(scale(Mtl_benzene_sample$northing)) constants &lt;- list(n = nrow(Mtl_benzene_sample), nu = nu) ex.data &lt;- list( y = Mtl_benzene_sample$y, easting = easting_scaled, northing = northing_scaled, easting_unobs = predCoords_five[,1], northing_unobs = predCoords_five[,2], distMatrix = obsDist, distMatrixUnobs = pred2predDist, distMatrixObsUnobs = obs2predDist ) params &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;phi&quot;, &quot;tau&quot;, &quot;sigma&quot;, &quot;tau_sq&quot;, &quot;y_unobs&quot;, &quot;sigma_sq&quot;) inits &lt;- list(sigma = 0.1, phi_inv = 6 / max(obsDist), tau = 0.1) # Run model in nimble start_time &lt;- Sys.time() mcmc.out &lt;- nimbleMCMC( code = Example10_10Code, constants = constants, data = ex.data, inits = inits, monitors = params, niter = 40000, nburnin = 20000, thin = 14, WAIC = TRUE, nchains = 2, summary = TRUE, samplesAsCodaMCMC = TRUE ) ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| ## |-------------|-------------|-------------|-------------| ## |-------------------------------------------------------| ## [Warning] There are individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes. end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time ## Time difference of 7.693944 mins mcmc.out$WAIC ## nimbleList object of type waicList ## Field &quot;WAIC&quot;: ## [1] -32.28445 ## Field &quot;lppd&quot;: ## [1] 19.03747 ## Field &quot;pWAIC&quot;: ## [1] 2.895244 min(coda::effectiveSize(mcmc.out$samples)) ## [1] 849.3096 plot(mcmc.out$samples[, c(&quot;beta0&quot;)], bty = &quot;n&quot;, main = &quot;beta0&quot;) plot(mcmc.out$samples[, c(&quot;beta1&quot;)], bty = &quot;n&quot;, main = &quot;beta1&quot;) plot(mcmc.out$samples[, c(&quot;beta2&quot;)], bty = &quot;n&quot;, main = &quot;beta2&quot;) plot(mcmc.out$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(mcmc.out$samples[, c(&quot;tau&quot;)], bty = &quot;n&quot;, main = &quot;tau&quot;) plot(mcmc.out$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) mcmc.out$summary$all.chains[c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;, &quot;sigma_sq&quot;, &quot;tau_sq&quot;, &quot;phi&quot;), ] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## beta0 0.143282027 0.151098526 0.080210901 -4.320255e-02 0.28188251 ## beta1 0.047132468 0.045714893 0.059074146 -7.036740e-02 0.16808769 ## beta2 0.078236153 0.083129502 0.067977770 -6.927811e-02 0.19791770 ## sigma_sq 0.056033096 0.051955467 0.021212336 2.847411e-02 0.10938656 ## tau_sq 0.009124676 0.008454582 0.006622246 2.839674e-05 0.02346751 ## phi 3.672082520 3.089047660 2.158752561 1.484596e+00 9.20444422 Example 10.11: INLA, creating a mesh library(geoR) library(INLA) library(spdep) benzene_data &lt;- read.csv(&quot;data/montreal_benzene_apr.csv&quot;) # create a new variable in &quot;sp&quot; format and define coordinates benzene_data_loc &lt;- benzene_data coordinates(benzene_data_loc) &lt;- ~ lon + lat proj4string(benzene_data_loc) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) benzene_data_utm &lt;- spTransform(benzene_data_loc, CRS(&quot;+proj=utm +zone=18 +ellps=WGS72&quot;)) # Save the utm as a data frame locations_df &lt;- as.data.frame(benzene_data_utm@coords) benzene_utm_df &lt;- data.frame( ID = 1:nrow(benzene_data), X = benzene_data_utm@coords[,1]/1000, Y = benzene_data_utm@coords[,2]/1000, logbenzene = log(benzene_data_utm$Benzene)) # change the observed values to the log scale and the coordinates to km&#39;s mesh = inla.mesh.create(locations_df, cutoff = 0.01, refine =(list(min.angle =20))) plot(mesh , col=&quot;gray&quot;, main=&quot;&quot;) Example 10.12: Fitting an SPDE model using R–INLA: benzene concentration in Montreal # Field std . dev . for theta =0 sigma0 = 1 # find the range of the location data size = min(c(diff(range(mesh$loc[, 1])), diff (range(mesh$loc[, 2])))) # A fifth of the approximate domain width . range0 = size/5 kappa0 = sqrt(8)/range0 tau0 = 1/(sqrt (4*pi)*kappa0*sigma0) spde = inla.spde2.matern ( mesh, B.tau = cbind(log (tau0), -1, +1), B.kappa = cbind(log (kappa0), 0, -1), theta.prior.mean = c(0 , 0), constr = TRUE ) formula = logbenzene ~ 1 + X + Y + f(ID , model = spde) model = inla( formula, family = &quot;gaussian&quot;, data = benzene_utm_df , control.predictor = list(compute = TRUE), control.compute = list(dic = TRUE , config = TRUE) ) model$summary.fixed ## mean sd 0.025quant 0.5quant 0.975quant ## (Intercept) 8.263107109 118.01744834 -189.26686123 -5.4724133487 298.73720629 ## X 0.014532810 0.02130267 -0.02563718 0.0132910509 0.06340354 ## Y -0.003363563 0.02371941 -0.06108419 -0.0006481981 0.03642514 ## mode kld ## (Intercept) -20.057620990 5.350437e-05 ## X 0.011915185 7.755183e-05 ## Y 0.002407428 3.418641e-05 Example 10.13: Directional variograms ### Compute and plot the directional variogram CA.geo &lt;- as.geodata( benzene_utm_df , coords.col = 2:3 , data.col=1) CA.vario4 &lt;- variog4(CA.geo ) plot(CA.vario4) Example 10.14: Spatial modeling of malaria in Gambia Note: There might be a warning when loading the \\(\\texttt{raster}\\) package, if necessary, uninstall and re-install the package. For this example we are using the same \\(\\texttt{gambia}\\) data set from the \\(\\texttt{geoR}\\) package but an \\(\\texttt{id_area}\\) column was added using QGIS to differentiate the different areas as in the original paper. library(dplyr) # to manipulate the data library(geoR) # to get the dataset library(ggmap) # to plot the map library(nimble) # for modeling #library(raster) # to get the environmental data library(rgdal) # for adding and transforming coordinates library(sf) # manipulate spatial data library(sp) # for manipulating spatial data library(stringr) # to analyze posterior library(viridis) # for a more cheerful color palette # Note, we are using the same Gambia dataset as in the geoR package but an id_area # attribute has been added using QGIS to a gambia &lt;- read.csv(&quot;data/gambia_area.csv&quot;) # gambia dataset from geoR package theme_clear &lt;- function(){ theme( panel.grid.major = element_blank(), # remove background grid panel.grid.minor = element_blank(), panel.background = element_blank(), # remove grey background axis.line = element_line(colour = &quot;black&quot;), # keep axis line text=element_text(size=12) ) } Since the data is given at the individual level, we want to aggregate the malaria tests by village. If we explore the data frame we see that there are 2035 individuals at 65 villages. head(gambia) ## x y pos age netuse treated green phc id_area ## 1 349631.3 1458055 1 1783 0 0 40.85 1 1 ## 2 349631.3 1458055 0 404 1 0 40.85 1 1 ## 3 349631.3 1458055 0 452 1 0 40.85 1 1 ## 4 349631.3 1458055 1 566 1 0 40.85 1 1 ## 5 349631.3 1458055 0 598 1 0 40.85 1 1 ## 6 349631.3 1458055 1 590 1 0 40.85 1 1 dim(gambia) ## [1] 2035 9 dim(unique(gambia[, c(&quot;x&quot;, &quot;y&quot;)])) ## [1] 65 2 We create a new data frame aggregated by village containing the coordinates, the number of malaria tests, and the prevalence. malaria_village &lt;- group_by(gambia, x, y) |&gt; summarize(total = n(), positive = sum(pos), prev = positive / total) |&gt; as.data.frame() ## `summarise()` has grouped output by &#39;x&#39;. You can override using the `.groups` ## argument. head(malaria_village) ## x y total positive prev ## 1 349631.3 1458055 33 17 0.5151515 ## 2 358543.1 1460112 63 19 0.3015873 ## 3 360308.1 1460026 17 7 0.4117647 ## 4 363795.7 1496919 24 8 0.3333333 ## 5 366400.5 1460248 26 10 0.3846154 ## 6 366687.5 1463002 18 7 0.3888889 # create a new variable in &quot;sp&quot; format and define coordinates malaria_utm &lt;- malaria_village coordinates(malaria_utm) &lt;- ~ x + y proj4string(malaria_utm) &lt;- CRS(&quot;+proj=utm +zone=28&quot;) # convert to long lat malaria_geo &lt;- spTransform(malaria_utm, CRS(&quot;+proj=longlat +datum=WGS84&quot;)) # add long lat coordinates to malaria dataframe malaria_village[, c(&quot;long&quot;, &quot;lat&quot;)] &lt;- coordinates(malaria_geo) # specify the bounding box latLongBox = bbox(malaria_geo) location = c(latLongBox[1, 1] - 0.05, latLongBox[2, 1] - 0.05, latLongBox[1, 2] + 0.05, latLongBox[2, 2] + 0.05) # create map with location dots marked on it in GambiaMap &lt;- get_stamenmap(bbox = location, zoom = 11, type = terrain-background) ggmap(GambiaMap) + geom_point(data = malaria_village, aes(x = long, y = lat, col = prev), size = 2) + scale_color_viridis() + theme_void() Nimble Example10_14_Nimble &lt;- nimbleCode({ # Define priors sigma ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) sigma_sq &lt;- sigma ^ 2 phi_inv ~ dgamma(shape = 5, rate = 5) phi &lt;- 1 / phi_inv Sigma[1:N, 1:N] &lt;- sigma_sq*(1 + (sqrt(3)*obs_dist_mat[1:N, 1:N])/phi) * exp(-sqrt(3)*obs_dist_mat[1:N, 1:N] / phi) #b0[1:N] ~ dmnorm(zeroes[1:N], cov = Id10[1:N, 1:N]) #mean_S[1:N] &lt;- nimRep(b0, N) for(i in 1:N){ mean_S[i] &lt;- b0 } #S[1:N] ~ dmnorm(mean_S[1:N], cov = Sigma[1:N,1:N]) S[1:N] ~ dmnorm(zeroes[1:N], cov = Sigma[1:N,1:N]) for (j in 1:n) { # by child logit(p[j]) &lt;- b0 + inprod(b[1:k], X[j,1:k]) + S[index_village[j]] y[j] ~ dbern(p[j]) } for (l in 1:k){ b[l] ~ dnorm(0, sd = 5) } b0 ~ dnorm(0, sd = 5) }) # distance specification coords_sf &lt;- sf::st_as_sf(malaria_village[,c(&quot;long&quot;,&quot;lat&quot;)], coords = c(&quot;long&quot;,&quot;lat&quot;)) |&gt; sf::st_set_crs(4326) obs_dist_mat &lt;- sf::st_distance(coords_sf) obs_dist_mat &lt;- units::set_units(obs_dist_mat, km) obs_dist_mat &lt;- units::set_units(obs_dist_mat, NULL) # define indicator variables for each village gambia_df &lt;- mutate( gambia, id_child = 1:nrow(gambia), # add an id for each child value = 1, # this will be needed later for the villages id_village = as.numeric(interaction( # add an id for the villages x, y, drop = TRUE, lex.order = TRUE )) ) |&gt; tidyr::spread(id_area, value, fill = 0) |&gt; rename(area1 = &#39;1&#39;, area2 = &#39;2&#39;, area3 = &#39;3&#39;, area4 = &#39;4&#39;, area5 = &#39;5&#39;) ## Model specification # Variables matrix X &lt;- data.frame(age = scale(gambia_df[,&quot;age&quot;], center = TRUE, scale = FALSE), netuse = gambia_df[,&quot;netuse&quot;], treated = gambia_df[,&quot;treated&quot;], green = scale(gambia_df[,&quot;green&quot;], center = TRUE, scale = FALSE), phc = gambia_df[,&quot;phc&quot;], area2 = gambia_df[,&quot;area2&quot;], area3 = gambia_df[,&quot;area3&quot;], area4 = gambia_df[,&quot;area4&quot;], area5 = gambia_df[,&quot;area5&quot;] ) index_village &lt;- gambia_df[,&quot;id_village&quot;] n &lt;- nrow(X) # child number N &lt;- nrow(malaria_village) # number of villages zeroes &lt;- rep(0, N) # auxiliary vector of zeroes for model ones &lt;- rep(1, N) const_list &lt;- list(n = n, # number of childs N = N, # number of villages zeroes = zeroes, # vector of zeroes prior_max_dist = max(obs_dist_mat)/6, # max dist for phi prior k = ncol(X),# number of predictors index_village = index_village, Id10 = 10*diag(N)) dat_list &lt;- list(y = gambia_df$pos, # malaria positive test obs_dist_mat = obs_dist_mat, # distance matrix in km X = X # predictors matrix ) init_list &lt;- list(sigma = 0.5, p = rep(expit(rnorm(1, 0, 1)), n), phi_inv = 6/max(obs_dist_mat), b = rep(0, ncol(X)), b0 = rnorm(1, 0, 1), S = rnorm(N, 0, 1)) #init_list &lt;- list(p = runif(n, 0, 1), b = rnorm(ncol(X), 0, 1), b0 = rnorm(1, 0, 1)) Rmodel &lt;- nimbleModel( Example10_14_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c( &quot;b0&quot;, &quot;b&quot;, &quot;p&quot;, &quot;S&quot;, &quot;sigma&quot;, &quot;phi&quot;)) # conf$removeSamplers(c(&#39;S&#39;)) # conf$addSampler(target = c(&#39;S&#39;), type = &#39;AF_slice&#39;) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = TRUE ) niters &lt;- 80000 nburnins &lt;- 0.5 * niters nchains &lt;- 2 nthins &lt;- 14 post_samples &lt;- runMCMC( Cmcmc, niter = niters, nburnin = nburnins, thin = nthins, nchains = nchains, samplesAsCodaMCMC = TRUE, summary = TRUE ) plot(post_samples$samples[, c(&quot;b0&quot;)], bty = &quot;n&quot;, main = &quot;b0&quot;) plot(post_samples$samples[, c(&quot;b[1]&quot;)], bty = &quot;n&quot;, main = &quot;b[1]&quot;) plot(post_samples$samples[, c(&quot;b[2]&quot;)], bty = &quot;n&quot;, main = &quot;b[2]&quot;) plot(post_samples$samples[, c(&quot;b[3]&quot;)], bty = &quot;n&quot;, main = &quot;b[3]&quot;) plot(post_samples$samples[, c(&quot;b[4]&quot;)], bty = &quot;n&quot;, main = &quot;b[4]&quot;) plot(post_samples$samples[, c(&quot;b[5]&quot;)], bty = &quot;n&quot;, main = &quot;b[5]&quot;) plot(post_samples$samples[, c(&quot;b[6]&quot;)], bty = &quot;n&quot;, main = &quot;b[6]&quot;) plot(post_samples$samples[, c(&quot;sigma&quot;)], bty = &quot;n&quot;, main = &quot;sigma&quot;) plot(post_samples$samples[, c(&quot;phi&quot;)], bty = &quot;n&quot;, main = &quot;phi&quot;) plot(post_samples$samples[, c(&quot;S[1]&quot;)], bty = &quot;n&quot;, main = &quot;S[1]&quot;) plot(post_samples$samples[, c(&quot;S[24]&quot;)], bty = &quot;n&quot;, main = &quot;S[24]&quot;) plot(post_samples$samples[, c(&quot;S[54]&quot;)], bty = &quot;n&quot;, main = &quot;S[54]&quot;) plot(post_samples$samples[, c(&quot;p[1]&quot;)], bty = &quot;n&quot;, main = &quot;p[1]&quot;) plot(post_samples$samples[, c(&quot;p[1805]&quot;)], bty = &quot;n&quot;, main = &quot;p[1805]&quot;) # Get minimum effective size (ESS) and which variable has the min ESS min(coda::effectiveSize(post_samples$samples)) ## [1] 105.8247 mcmc_variable_names &lt;- colnames(post_samples$samples$chain1) mcmc_variable_names[which(coda::effectiveSize(post_samples$samples) == min(coda::effectiveSize(post_samples$samples)))] ## [1] &quot;S[42]&quot; # Extract samples variables &lt;- c(&quot;b0&quot;, &quot;b[1]&quot;, &quot;b[2]&quot;, &quot;b[3]&quot;, &quot;b[4]&quot;, &quot;b[5]&quot;, &quot;b[6]&quot;, &quot;b[7]&quot;, &quot;b[8]&quot;, &quot;b[9]&quot; ,&quot;sigma&quot;, &quot;phi&quot;) summary_nimble &lt;- post_samples$summary$all.chains summary_nimble[variables,] ## Mean Median St.Dev. 95%CI_low 95%CI_upp ## b0 0.2851703918 0.2834992672 0.2456832933 -0.2044658793 0.7638609644 ## b[1] 0.0006802616 0.0006797914 0.0001236685 0.0004385325 0.0009295574 ## b[2] -0.3771503811 -0.3731644148 0.1639276480 -0.7022601066 -0.0540773855 ## b[3] -0.3816675119 -0.3791494375 0.2043240487 -0.7950621733 0.0125879431 ## b[4] 0.0464739713 0.0464605869 0.0195124345 0.0077296436 0.0842387452 ## b[5] -0.1848605191 -0.1868942111 0.2403559916 -0.6420245178 0.2915476363 ## b[6] -0.4317379130 -0.4541505313 0.3638741683 -1.0758480871 0.3215201355 ## b[7] -1.3364754763 -1.3359642780 0.2994289798 -1.9291158461 -0.7356573529 ## b[8] -0.8667274155 -0.8612057876 0.4276471629 -1.7246044041 -0.0264550662 ## b[9] 0.1314094431 0.1370149239 0.4278604126 -0.6997430545 0.9514398295 ## sigma 0.7359290971 0.7285333807 0.1062490704 0.5497333338 0.9605755352 ## phi 1.1485902540 1.0448100484 0.5023104793 0.4824973257 2.3870365213 # Plot posterior summary for the spatial random effect by village post_summary &lt;- post_samples$summary$all.chains post_sum_S &lt;- as.data.frame(post_summary) |&gt; tibble::rownames_to_column() |&gt; filter(str_detect(rowname, &quot;S&quot;)) |&gt; dplyr::select(rowname, `95%CI_low`, Mean, `95%CI_upp`) |&gt; mutate(village = gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname)) post_sum_S$village &lt;- factor(post_sum_S$village , levels = 1:65) ggplot(data = post_sum_S, aes(x = village)) + geom_pointrange(aes(ymin = `95%CI_low`, ymax = `95%CI_upp`, y = Mean)) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + scale_x_discrete( breaks = post_sum_S$village[seq(1, length(post_sum_S$village), by = 5)]) + theme_clear() + ylab(&quot;&quot;) + xlab(&quot;village&quot;) + ggtitle(&quot;Posterior summary spatial random effect by village&quot;) Stan data { int&lt;lower=1&gt; n; // number of children int&lt;lower=1&gt; k; // number of covariates int N; // total number of villages int y[n]; // tests matrix[N,N] dist_matrix; // distance matrix matrix[n, k] X; // matrix with covariates int index_village[n]; } parameters { real&lt;lower=0&gt; phi_inv; real&lt;lower=0&gt; sigma; vector[N] S; // spatial random effect vector[k] betas; real beta0; } transformed parameters { // vector[n] p = inv_logit(logit_p); real sigma_sq = square(sigma); real&lt;lower=0&gt; phi = 1/phi_inv; } model { matrix[N, N] L; matrix[N, N] Sigma; vector[N] zeroes; // vector of zeroes vector[n] logit_p; for(i in 1:(N-1)){ for(j in (i+1):N){ //Sigma[i,j] = sigma_sq*exp(-dist_matrix[i,j]/phi); Sigma[i,j] = sigma_sq*(1 + (sqrt(3)*dist_matrix[i,j])/phi) * exp(-sqrt(3)*dist_matrix[i,j] / phi); Sigma[j,i] = Sigma[i,j]; } } // diagonal elements covariances for(i in 1:N){ Sigma[i,i] = sigma_sq; } // sample spatial random effect L = cholesky_decompose(Sigma); zeroes = rep_vector(0, N); S ~ multi_normal_cholesky(zeroes, L); for(i in 1:n) { logit_p[i] = beta0 + X[i,]*betas + S[index_village[i]]; y[i] ~ binomial_logit(1, logit_p[i]); } beta0 ~ normal(0,10); betas ~ normal(0,10); phi_inv ~ gamma(5, 5); sigma ~ cauchy(0,1); } ex.data &lt;- list( n = nrow(gambia), # number of children k = ncol(X), # number of covariates N = N, # number of villages y = gambia$pos, # positive tests dist_matrix = obs_dist_mat, # distance matrix in km X = X, # altitude per village index_village = index_village ) Example10_14Stan &lt;- stan( file = &quot;functions/Example10_14.stan&quot;, data = ex.data, warmup = 15000, iter = 30000, chains = 2, thin = 10, pars = c(&quot;beta0&quot;, &quot;betas&quot;,&quot;sigma&quot;, &quot;phi&quot;, &quot;S&quot;), include = TRUE ) ## recompiling to avoid crashing R session #computing WAIC using the package loo rstan::traceplot(Example10_14Stan, pars = c(&quot;beta0&quot;,&quot;betas&quot;,&quot;sigma&quot;, &quot;phi&quot;)) # Extract samples summary_stan &lt;- summary( Example10_14Stan, pars = c(&quot;beta0&quot;,&quot;betas&quot;, &quot;sigma&quot;, &quot;phi&quot;), probs = c(0.025, 0.975) ) summary_stan$summary ## mean se_mean sd 2.5% 97.5% ## beta0 0.3522614091 4.887364e-03 0.2597556824 -0.159486460 0.8739034120 ## betas[1] 0.0006870215 2.271897e-06 0.0001250625 0.000437381 0.0009294606 ## betas[2] -0.4000057944 2.986040e-03 0.1616709103 -0.721534788 -0.0822872775 ## betas[3] -0.3713660306 3.861800e-03 0.2103334310 -0.766072431 0.0266838941 ## betas[4] 0.0438691000 3.747264e-04 0.0204191472 0.004266428 0.0842915620 ## betas[5] -0.2166475957 4.642501e-03 0.2462301730 -0.699125650 0.2676080843 ## betas[6] -0.5249790652 7.048292e-03 0.3763802333 -1.278164104 0.2065797214 ## betas[7] -1.3644709952 5.890083e-03 0.3104471638 -1.963698425 -0.7578417978 ## betas[8] -0.8136564973 8.465977e-03 0.4493777511 -1.708788734 0.0720395654 ## betas[9] 0.0227669796 7.775086e-03 0.4367271272 -0.841169969 0.8699352615 ## sigma 0.7496445667 1.975545e-03 0.1092771093 0.557609787 0.9885074690 ## phi 1.2480037159 1.102573e-02 0.5990597906 0.510696395 2.7821219891 ## n_eff Rhat ## beta0 2824.755 0.9996833 ## betas[1] 3030.241 0.9998661 ## betas[2] 2931.383 0.9999513 ## betas[3] 2966.450 0.9999363 ## betas[4] 2969.250 1.0009093 ## betas[5] 2813.057 1.0004112 ## betas[6] 2851.582 1.0018211 ## betas[7] 2778.002 1.0003460 ## betas[8] 2817.533 0.9999408 ## betas[9] 3155.077 0.9997243 ## sigma 3059.740 0.9996511 ## phi 2952.063 0.9998501 S_summary &lt;- summary(Example10_14Stan, pars = c(&quot;S&quot;))$summary S_summary_df &lt;- data.frame(S_summary) |&gt; tibble::rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;S[&quot;, 1:65, &quot;]&quot;)) |&gt; mutate(village = 1:65) |&gt; dplyr::select(mean, X2.5., X97.5., village) S_summary_df$village &lt;- factor(S_summary_df$village , levels = 1:65) ggplot(S_summary_df, aes(x = village, group = 1)) + geom_pointrange(aes(ymin = X2.5., ymax = X97.5., y = mean)) + geom_hline(yintercept = 0, linetype = &quot;dotted&quot;) + scale_x_discrete( breaks = S_summary_df$village[seq(1, length(S_summary_df$village), by = 5)]) + theme_clear() + ylab(&quot;&quot;) + xlab(&quot;village&quot;) + ggtitle(&quot;Posterior summary spatial random effect by village&quot;) "],["Time.html", "Chapter 11 Time-why it also matters Example 11.1 Ground level ozone concentrations Example 11.2 Low-pass filtering of carbon monoxide levels using the moving average Example 11.13 Forecasting ozone levels Example 11.14 Forecasting volcanic ash Example 11.16 Fitting a random walk model to PM10 concentrations in London Example 11.16 EXTRA Implementation of a dynamic linear model: UK ozone data Solutions to Selected Exercises Exercise 11.11 Exercise 11.13", " Chapter 11 Time-why it also matters The chapter contains the theory required for handling time series data. From this chapter, the reader will have gained an understanding of the following topics: That a temporal process consists of both low and high frequency components, the former playing a key role in determining long-term trends while the latter may be associated with shorter-term changes. Techniques for the exploratory analysis of the data generated by the temporal process, including the ACF (correlogram) and PACF (periodogram). Models for irregular (high frequency) components after the regular components (trend) have been removed. Methods for forecasting, including exponential smoothing and ARIMA modelling. The state space modelling approach, which sits naturally within a Bayesian setting and which provides a general framework for most of the classical time series models and many more besides. Implementing time series processes within a Bayesian hierarchical framework. Example 11.1 Ground level ozone concentrations Load the daily and hourly data for one site. library(ggplot2) # Load data for one site site_daily &lt;- read.csv(&quot;data/LA_ozone_daily.csv&quot;, header = TRUE) # change the format of the date column site_daily$date &lt;- as.Date(site_daily$date, &quot;%m/%d/%Y&quot;) # load hourly data from that same site site_hourly &lt;- read.csv(&quot;data/LA_ozone_hourly.csv&quot;, header = TRUE) Plot the daily and hourly time series. # Plot daily data ggplot(data = site_daily) + geom_line(aes(x = date, y = max.ozone, group = 1)) + # draw a line at the first data geom_vline(xintercept = as.Date(&quot;2013-01-01&quot;), color = &quot;grey&quot;) + # 8 hour regulatory standard of 0.075 (ppm) geom_hline(yintercept = 0.075, color = &quot;grey&quot;) + xlab(&quot;Day in 2013 at Los Angeles Site 060379033&quot;) + ylab(&quot;Max Daily 8hr Ozone Level (ppm)&quot;) + theme_classic() # Plot hourly data ggplot(data = site_hourly,) + geom_line(aes(x = time, y = ozone, group = 1)) + geom_vline(xintercept = 1, color = &quot;grey&quot;) + # 8 hour regulatory standard of 0.075 (ppm). geom_hline(yintercept = 0.075, color = &quot;grey&quot;) + xlab(&quot;Hour in 2013&#39;s ozone season&quot;) + ylab(&quot;Hourly Ozone Concentration (ppm)&quot;) + theme_classic() Example 11.2 Low-pass filtering of carbon monoxide levels using the moving average For this example we will use the TTR package (Technical Trading Rules). This allow us to manipulate objects like time series for forecasting. library(TTR) # Load daily data site_daily &lt;- read.csv(&quot;data/LA_ozone_daily.csv&quot;, header = TRUE) # add an identifier column for each day in the sample site_daily$day &lt;- 1:nrow(site_daily) # Add loess smooth # Single day ozone.loess &lt;- loess(max.ozone ~ day, span = 0.75, data = site_daily[,c(&quot;max.ozone&quot;, &quot;day&quot;)]) ozone.predict &lt;- predict(ozone.loess, data.frame(day = 1:nrow(site_daily))) ozone.predict_df &lt;- data.frame(day = 1:nrow(site_daily), ozone.prediction = ozone.predict) plot_SMA1 &lt;- ggplot(data = site_daily) + geom_line(aes(x = day, y = max.ozone, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Single day&quot;) + theme_classic() # Three days ozone_SMA3 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 3), day = 1:nrow(site_daily)) plot_SMA3 &lt;- ggplot(data = ozone_SMA3) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Three days&quot;) + theme_classic() # Six days ozone_SMA6 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 6), day = 1:nrow(site_daily)) plot_SMA6 &lt;- ggplot(data = ozone_SMA6) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Six days&quot;) + theme_classic() # Twelve days ozone_SMA12 &lt;- data.frame(sma = SMA(site_daily$max.ozone, n = 12), day = 1:nrow(site_daily)) plot_SMA12 &lt;- ggplot(data = ozone_SMA12) + geom_line(aes(x = day, y = sma, group = 1)) + geom_line(data = ozone.predict_df, aes(x = day, y = ozone.predict)) + xlab(&quot;Day in 2013 at LA Site 060379033&quot;) + ylab(&quot;Ozone Level (ppm)&quot;) + ggtitle(&quot;Twelve days&quot;) + theme_classic() cowplot::plot_grid(plot_SMA1, plot_SMA3, plot_SMA6, plot_SMA12, labels = &quot;auto&quot;) Example 11.13 Forecasting ozone levels library(TTR) library(forecast) # Load hourly data for site 060379033 site_hourly &lt;- read.csv(&quot;data/LA_ozone_hourly.csv&quot;, header = TRUE) # one night hour per day missing for instrument calibration - imputed for simplicity imputeNA &lt;- mean(site_hourly$ozone, na.rm = TRUE) site_hourly$ozone[is.na(site_hourly$ozone)] &lt;- imputeNA # Select the first seven days in july # days_pattern contains the dates for the first seven days days_pattern &lt;- paste0(&quot;^2013070&quot;, 1:7, collapse=&quot;|&quot;) # select all the rows that follow that pattern using grepl july_seven_days &lt;- site_hourly[grepl(days_pattern, site_hourly$datetime),] july_seven_days$hours &lt;- 1:nrow(july_seven_days) # Holt-Winters model fitting # Turn this into a time series object to use Holt-Winters forecast level_ts &lt;- ts(july_seven_days$ozone, frequency = 24, start = c(1)) ozone_forecast &lt;- HoltWinters(level_ts) # Plot using the default function plot( ozone_forecast, xlab = &quot;Hours - First Week -July 2013&quot;, ylab = &quot;O3 (ppm)&quot;, col.predicted = 1, col = &quot;black&quot;, bty = &quot;n&quot;, lty = 2 ) # Holt- Winters 24 ahead forast on Day 8 # no need to specify Holt-Winters forecast as the object is already HoltWinters class ozoneforecast_day8 &lt;- forecast(ozone_forecast, h=24) # Plot using default function plot(ozoneforecast_day8, bty = &quot;n&quot;) Example 11.14 Forecasting volcanic ash The data in this example consists of atmospheric levels of volcanic ash from 1500AD to 2000AD. library(ggplot2) library(forecast) library(TTR) # Load volcano dust data ## REVIEW: Data source: Hyn�d�man, R.J. Time Series Data Library, http://data.is/TSDLdemo # Data cover the period from 1500AD to 2000AD volcano_dust &lt;- scan(&quot;https://robjhyndman.com/tsdldata/annual/dvi.dat&quot;, skip = 1) The following figure shows the plot of the original time series. # Turn data into data frame to plot it in ggplot volcano_dust_df &lt;- data.frame(year = 1500:(1500+length(volcano_dust)-1), dust = volcano_dust) ggplot(data = volcano_dust_df) + geom_line(aes(x = year, y = dust, group = 1)) + xlab(&quot;Year&quot;) + ylab(&quot;Atmospheric levels of volcanic ash&quot;) + theme_classic() We can also plot the autocorrelogram (ACF) and partial autocorrelogram (PACF) to identify any autocorrelation in the time series. # convert data into a time series object volcano_dust_series &lt;- ts(volcano_dust, start = c(1500)) # Compute autocorrelogram with max lag 20 acf( volcano_dust_series, lag.max = 20, bty = &quot;n&quot;, main = &quot;Autocorrelogram volcano dust&quot; ) # Compute the partial autocorrelogram with max lag 20 pacf( volcano_dust_series, lag.max = 20, bty = &quot;n&quot;, main = &quot;Partial autocorrelogram volcano dust&quot; ) # Finding an ARIMA model auto.arima(volcano_dust_series, ic = &quot;aic&quot;) ## Series: volcano_dust_series ## ARIMA(1,0,2) with non-zero mean ## ## Coefficients: ## ar1 ma1 ma2 mean ## 0.4723 0.2694 0.1279 57.5178 ## s.e. 0.0936 0.0969 0.0752 8.4883 ## ## sigma^2 = 4897: log likelihood = -2661.84 ## AIC=5333.68 AICc=5333.81 BIC=5354.45 # fitting the ARIMA(2,0,0) model volcano_dust_arima &lt;- arima(volcano_dust_series, order = c(2, 0, 0)) # forecast 31 years with the ARIMA(2,0,0) model volcano_dust_forecast &lt;- forecast(volcano_dust_arima, h = 31) plot(volcano_dust_forecast, bty = &quot;n&quot;) Example 11.16 Fitting a random walk model to PM10 concentrations in London Nimble library(coda) library(ggplot2) library(nimble) library(tidybayes) library(tidyverse) source(&quot;functions/FFBS_functions_nimble.R&quot;) # load necessary functions for ffbs # Load data pm10 &lt;- source(&quot;data/11_13_data.txt&quot;)[[&quot;value&quot;]][[&quot;y&quot;]] |&gt; unlist() # Choose 250 days for example pm10_250 &lt;- pm10[385:634] Example11_16_PM10_Nimble &lt;- nimbleCode({ tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) Vt &lt;- tau^2 sqrt_Wt_diag ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) Wt &lt;- sqrt_Wt_diag^2 theta_mean[1] &lt;- m0 theta[1] ~ dnorm(theta_mean[1], var = C0) for(t in 1:J) { theta_mean[t+1] &lt;- Gt * theta[t] theta[t+1] ~ dnorm(theta_mean[t+1], var = Wt) yt_mean[t] &lt;- Ft[t] * theta[t+1] yt[t] ~ dnorm(yt_mean[t], var = Vt) } }) # Model specification J &lt;- length(pm10_250) yt &lt;-pm10_250 Ft &lt;- rep(1, J) Gt &lt;- 1 m0 &lt;- mean(yt, na.rm = TRUE) C0 &lt;- 10 const_list &lt;- list(J = J) dat_list &lt;- list( yt = yt, Ft = Ft, Gt = Gt, m0 = m0, C0 = C0 ) init_list &lt;- list(tau = 0.01, sqrt_Wt_diag = 0.1, theta = rep(0, J + 1)) Rmodel &lt;- nimbleModel( Example11_16_PM10_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;, &quot;yt&quot;)) conf$removeSampler(target = &quot;theta[]&quot;) conf$addSampler( target = &quot;theta&quot;, type = &quot;ffbs_uous&quot;, control = list( ytName = &quot;yt&quot;, FtName = &quot;Ft&quot;, VtName = &quot;Vt&quot;, GtName = &quot;Gt&quot;, WtName = &quot;Wt&quot;, m0Name = &quot;m0&quot;, C0Name = &quot;C0&quot; ) ) conf$printSamplers(byType = TRUE) conf$removeSampler(target = &quot;sqrt_Wt_diag&quot;) conf$addSampler(target = &quot;sqrt_Wt_diag&quot;, type = &quot;RW&quot;, control = list(log = TRUE)) conf$removeSampler(target = &quot;tau&quot;) conf$addSampler(target = &quot;tau&quot;, type = &quot;RW&quot;, control = list(log = TRUE)) conf$printSamplers(byType = TRUE) conf$printSamplers(executionOrder = TRUE) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = TRUE ) niter &lt;- 60000 nburnin &lt;- 0.5 * niter nchain &lt;- 2 nthin &lt;- 14 post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = nchain, samplesAsCodaMCMC = TRUE ) post_summary &lt;- nimSummary(post_samples) tidy_post_samples &lt;- post_samples |&gt; tidy_draws() # Traceplots tidy_post_samples |&gt; select(.chain, .iteration, .draw, &#39;tau&#39;) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free_y&quot;, nrow = 1) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) tidy_post_samples |&gt; select(.chain, .iteration, .draw, &#39;sqrt_Wt_diag&#39;) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free_y&quot;, nrow = 1) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) tidy_post_samples |&gt; select(.chain, .iteration, .draw, paste0(&#39;theta[&#39;, sample.int(J, size = 4, replace = FALSE),&quot;]&quot;)) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free_y&quot;, nrow = 2) + theme_bw() + theme(panel.grid = element_blank(), strip.background = element_blank()) head(post_summary) ## post.mean post.sd q2.5 q50 q97.5 f0 n.eff Rhat ## sqrt_Wt_diag 4.547 0.873 2.988 4.513 6.398 1 530.716 1.000 ## tau 6.765 0.684 5.372 6.773 8.057 1 736.766 1.000 ## theta[1] 26.086 2.832 20.642 26.032 31.613 1 3966.763 1.001 ## theta[2] 25.687 3.547 18.921 25.672 32.679 1 3983.608 1.000 ## theta[3] 22.765 3.688 15.613 22.837 29.896 1 4284.000 1.001 ## theta[4] 19.140 3.830 11.596 19.111 26.576 1 3575.991 1.000 post_sum_theta &lt;- as.data.frame(post_summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) |&gt; select(time, post.mean, post.sd, q2.5, q50, q97.5) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7) + geom_path(aes(y = q50), col = &quot;blue&quot;, linewidth = 0.6) + ylab(&quot;&quot;) + xlab(&quot;Time (days)&quot;) + ggtitle(expression(paste(&quot;Trend &quot;, PM[10], &quot; London site&quot;))) + theme_classic() npsample &lt;- floor((niter - nburnin)/nthin) Cnim_postfit_uous &lt;- compileNimble(nim_postfit_uous, showCompilerOutput = FALSE) ## Compiling ## [Note] This may take a minute. ## [Note] Use &#39;showCompilerOutput = TRUE&#39; to see C++ compilation details. post_tau &lt;- tidy_post_samples$tau post_sqrt_Wt_diag &lt;- tidy_post_samples$sqrt_Wt_diag # post_yt &lt;- tidy_post_samples$yt post_ft_list &lt;- lapply(1:(nchain * npsample), function(i) { post_Vt &lt;- post_tau[i] ^ 2 post_Wt &lt;- post_sqrt_Wt_diag[i] ^ 2 post_ft &lt;- Cnim_postfit_uous( yt = dat_list$yt, Ft = dat_list$Ft, Vt = post_Vt, Gt = dat_list$Gt, Wt = post_Wt, m0 = dat_list$m0, C0 = dat_list$C0 ) post_ft &lt;- data.frame(ft = post_ft) %&gt;% mutate(time = row_number()) return(post_ft) }) tidy_post_ft &lt;- do.call(&quot;rbind&quot;, post_ft_list) ## posterior summaries of ft post_sum_ft &lt;- tidy_post_ft |&gt; group_by(time) |&gt; summarise( post.mean = mean(ft), post.sd = sd(ft), q2.5 = quantile(ft, prob = 0.025), q50 = quantile(ft, prob = 0.50), q97.5 = quantile(ft, prob = 0.975)) |&gt; ungroup() ggplot(data = post_sum_ft, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;) + geom_path(aes(y = post.mean), linewidth = 0.25) + geom_point(aes(y = yt), size = 0.25) + ylab(bquote(&quot;PM&quot;[10]~&quot;Concentration&quot;)) + xlab(&quot;Time (days)&quot;) + ggtitle(expression(paste(&quot;Fitted values &quot;, PM[10], &quot; London site&quot;))) + theme_classic() Stan library(coda) library(ggplot2) library(rstan) library(tidyverse) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # TODO: change the name of this dataset cause it is used here and in the exercises pm10 &lt;- source(&quot;data/11_13_data.txt&quot;)[[&quot;value&quot;]][[&quot;y&quot;]] |&gt; unlist() pm10_250 &lt;- pm10[385:634] J &lt;- length(pm10_250) y &lt;- pm10_250 Ft &lt;- rep(1, J) Gt &lt;- 1 m0 &lt;- mean(y) C0 &lt;- 10 stan_data &lt;- list( Tt = J, y = y, Ft = Ft, G = Gt, m0 = m0, C0 = C0 ) functions { real uous_dlm_ldensity(array[] real y, array[] real Ft, real G, real V, real W, real m0, real C0, int Tt){ array[Tt+1] real a; array[Tt+1] real R; array[Tt] real lldata; a[1] = m0; R[1] = C0; for (i in 1:Tt) { real u; real Q; real A; real L; u = y[i] - Ft[i] * a[i]; Q = Ft[i] * R[i] * Ft[i] + V; A = G * R[i] * Ft[i] * inv(Q); L = G - A * Ft[i]; lldata[i] = normal_lpdf(u | 0, sqrt(Q)); a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L + W; } return sum(lldata); } array[] real uous_ffbs_rng(array[] real y, array[] real Ft, real G, real V, real W, real m0, real C0, int Tt){ array[Tt] real theta; array[Tt] real a; array[Tt] real R; array[Tt] real m; array[Tt] real C; // Kalman filtering real mt = m0; real Ct = C0; for(i in 1:Tt){ real ft; real Qt; real at; real Rt; real At; at = G * mt; Rt = G * Ct * G + W; ft = Ft[i] * at; Qt = Ft[i] * Rt * Ft[i] + V; At = Rt * Ft[i] * inv(Qt); mt = at + At * (y[i] - ft); Ct = Rt - At * Qt * At; //store for backward sampling a[i] = at; R[i] = Rt; m[i] = mt; C[i] = Ct; } // backward sampling array[Tt-1] int ind = sort_indices_desc(linspaced_int_array(Tt-1,1,Tt-1)); theta[Tt] = normal_rng(m[Tt], sqrt(C[Tt])); for(i in ind) { real Bt; real ht; real Ht; Bt = C[i] * G * inv(R[i+1]); ht = m[i] + Bt * (theta[i+1] - a[i+1]); Ht = C[i] - Bt * G * C[i]; theta[i] = normal_rng(ht, sqrt(Ht)); } return theta; } array[] real uous_dlm_one_step_ahead_rng(array[] real y, array[] real Ft, real G, real V, real W, real m0, real C0, int Tt){ array[Tt] real yfit; array[Tt+1] real a; array[Tt+1] real R; array[Tt] real lldata; a[1] = m0; R[1] = C0; for (i in 1:Tt) { real u; real Q; real A; real L; u = y[i] - Ft[i] * a[i]; Q = Ft[i] * R[i] * Ft[i] + V; //F[i]&#39; * R[i] * F[i] + V; A = G * R[i] * Ft[i] * inv(Q); L = G - A * Ft[i]; yfit[i] = normal_rng(Ft[i] * a[i], sqrt(Q)); // univariate a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L + W; } return yfit; } } data{ int Tt; array[Tt] real y; array[Tt] real Ft; real G; real m0; real&lt;lower=0&gt; C0; } parameters{ real&lt;lower=0&gt; tau; real&lt;lower=0&gt; sqrt_W; } model { real V = square(tau); real W = square(sqrt_W); tau ~ std_normal(); sqrt_W ~ std_normal(); target += uous_dlm_ldensity(y, Ft, G, V, W, m0, C0, Tt); } generated quantities{ array[Tt] real theta; array[Tt] real yfit; real V = square(tau); real W = square(sqrt_W); theta = uous_ffbs_rng(y, Ft, G, V, W, m0, C0, Tt); yfit = uous_dlm_one_step_ahead_rng(y, Ft, G, V, W, m0, C0, Tt); } Example11_16_PM10_Stan &lt;- stan( file = &quot;functions/Example11_16_PM10.stan&quot;, data = stan_data, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) rstan::traceplot(Example11_16_PM10_Stan, pars = c(&quot;tau&quot;,&quot;sqrt_W&quot;)) rstan::traceplot(Example11_16_PM10_Stan, pars = paste0(&quot;theta[&quot;, sample.int(J, size = 4, replace = FALSE), &quot;]&quot;)) stanfit_summary &lt;- summary(Example11_16_PM10_Stan, pars = c(&quot;tau&quot;,&quot;sqrt_W&quot;,&quot;theta&quot;, &quot;yfit&quot;)) head(stanfit_summary$summary) ## mean se_mean sd 2.5% 25% 50% ## tau 6.059572 0.006011225 0.4520659 5.173290 5.754283 6.060914 ## sqrt_W 4.268502 0.007901555 0.5789220 3.155335 3.873516 4.266370 ## theta[1] 25.772793 0.029473371 3.3465006 19.266802 23.494325 25.707770 ## theta[2] 22.723128 0.028054146 3.4480789 15.981407 20.404997 22.693075 ## theta[3] 19.086662 0.029104885 3.5356129 12.167884 16.697797 19.097985 ## theta[4] 17.434448 0.030008784 3.5659752 10.352259 15.027600 17.477404 ## 75% 97.5% n_eff Rhat ## tau 6.363179 6.932639 5655.586 0.9999911 ## sqrt_W 4.654269 5.431407 5368.031 1.0000244 ## theta[1] 28.042850 32.328176 12892.056 0.9998918 ## theta[2] 25.026313 29.478562 15106.376 0.9999013 ## theta[3] 21.503471 25.896446 14756.985 1.0000483 ## theta[4] 19.828534 24.325792 14120.818 1.0000220 stan_fit_df &lt;- data.frame(stanfit_summary$summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(data = stan_fit_df, aes(x = time)) + geom_path(aes(y = mean), col = &quot;blue&quot;, size = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + xlab(&quot;Time (days)&quot;) + ylab(&quot; &quot;) + ggtitle(expression(paste(&quot;Trend &quot;, PM[10], &quot; London site&quot;))) + theme_classic() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. yfit_summary_dt &lt;- data.frame(stanfit_summary$summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;yfit&quot;)) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(yfit_summary_dt, aes(x = time)) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + geom_point(aes(y = y)) + geom_path(aes(y = mean), linewidth = 0.1, size = 1) + xlab(&quot;Time (days)&quot;) + ylab(&quot; &quot;) + ggtitle(expression(paste(&quot;Fitted values &quot;, PM[10], &quot; London site&quot;))) + theme_classic() Example 11.16 EXTRA Implementation of a dynamic linear model: UK ozone data NOTE The code for the implementation of DLMs in Stan and Nimble was developed by Paritosh Kumar Roy. This code and other more complex DLM structures are available through his github. Nimble library(coda) library(dplyr) #library(sf) library(ggplot2) library(nimble, warn.conflicts = FALSE) library(nleqslv) library(tidybayes) library(tidyverse) source(&quot;functions/FFBS_functions_nimble.R&quot;) UK_ozone &lt;- read.csv(&quot;data/uk_ozone_one_site.csv&quot;) Example11_16_O3_Nimble &lt;- nimbleCode({ tau ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) for (t in 1:Tt) { Vt[t] &lt;- tau ^ 2 } for (j in 1:p) { sqrt_Wt_diag[j] ~ T(dt(mu = 0, sigma = 1, df = 1), 0, Inf) } Wt[1:p, 1:p] &lt;- nim_diag(x = sqrt_Wt_diag[1:p] ^ 2) mt[1:p, 1] &lt;- m0[1:p] Ct[1:p, 1:p, 1] &lt;- C0[1:p, 1:p] for (t in 1:Tt) { at[1:p, t] &lt;- (Gt[1:p, 1:p] %*% mt[1:p, t])[1:p, 1] Rt[1:p, 1:p, t] &lt;- Gt[1:p, 1:p] %*% Ct[1:p, 1:p, t] %*% t(Gt[1:p, 1:p]) + Wt[1:p, 1:p] ft[t] &lt;- (t(Ft[1:p, t]) %*% at[1:p, t])[1, 1] Qt[t] &lt;- (t(Ft[1:p, t]) %*% Rt[1:p, 1:p, t] %*% Ft[1:p, t] + Vt[t])[1, 1] yt[t] ~ dnorm(mean = ft[t], var = Qt[t]) At[1:p, t] &lt;- (Rt[1:p, 1:p, t] %*% Ft[1:p, t])[1:p, 1] / Qt[t] mt[1:p, t + 1] &lt;- at[1:p, t] + (At[1:p, t] * (yt[t] - ft[t])) Ct[1:p, 1:p, t + 1] &lt;- Rt[1:p, 1:p, t] - (At[1:p, t] %*% t(At[1:p, t])) * Qt[t] } theta[1:p, 1:(Tt + 1)] &lt;- nim_bsample( mt = mt[1:p, 1:(Tt + 1)], Ct = Ct[1:p, 1:p, 1:(Tt + 1)], at = at[1:p, 1:Tt], Gt = Gt[1:p, 1:p], Rt = Rt[1:p, 1:p, 1:Tt] ) }) Time-varying level model # Model specification yt &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(yt) p &lt;- 3 # (intercept, wind, temp) Ft &lt;- array(0, dim = c( p, Tt)) Ft[ 1,] &lt;- 1 Ft[ 2,] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[ 3,] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) m0 &lt;- c(mean(yt), 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) const_list &lt;- list(Tt = Tt, p = p) dat_list &lt;- list( yt = yt, Ft = Ft, Gt = Gt, m0 = m0, C0 = C0 ) init_list &lt;- list(tau = 0.01, sqrt_Wt_diag = sqrt(rep(0.1, p))) Rmodel &lt;- nimbleModel( Example11_16_O3_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Rmodel$calculate() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;, &quot;ft&quot;)) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = FALSE ) niter &lt;- 10000 nburnin &lt;- 0.5 * niter nthin &lt;- 1 nchains &lt;- 2 start_time &lt;- Sys.time() post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = nchains, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time post_summary &lt;- nimSummary(post_samples) tidy_post_samples &lt;- post_samples |&gt; tidy_draws() tidy_post_samples |&gt; dplyr::select( .chain, .iteration, .draw, &#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39; ) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(panel.grid = element_blank(), strip.background = element_blank()) post_summary[c(&#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;), ] ## post.mean post.sd q2.5 q50 q97.5 f0 n.eff Rhat ## tau 0.549 0.033 0.486 0.550 0.614 1 1141.190 1.003 ## sqrt_Wt_diag[1] 0.117 0.042 0.049 0.113 0.209 1 1191.359 1.009 ## sqrt_Wt_diag[2] 0.017 0.009 0.004 0.015 0.037 1 1823.198 1.003 ## sqrt_Wt_diag[3] 0.036 0.033 0.001 0.026 0.121 1 979.027 1.002 post_sum_theta &lt;- as.data.frame(post_summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; dplyr::select(rowname, q2.5, q50, q97.5) |&gt; separate(rowname, into = c(&quot;x1&quot;, &quot;x2&quot;), sep = &quot;,&quot;) |&gt; mutate(component = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x1))) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x2))) |&gt; dplyr::select(component, time, q2.5, q50, q97.5) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7) + geom_path(aes(y = q50), col = &quot;blue&quot;, linewidth = 0.4) + facet_wrap( ~ component, nrow = 2, scales = &quot;free&quot;, labeller = label_bquote(theta[.(component)]) ) + ylab(&quot;&quot;) + xlab(&quot;Time&quot;) + theme_classic() npsample &lt;- floor((niter - nburnin)/nthin) str(tidy_post_samples) ## tibble [10,000 × 1,110] (S3: tbl_df/tbl/data.frame) ## $ .chain : int [1:10000] 1 1 1 1 1 1 1 1 1 1 ... ## $ .iteration : int [1:10000] 1 2 3 4 5 6 7 8 9 10 ... ## $ .draw : int [1:10000] 1 2 3 4 5 6 7 8 9 10 ... ## $ ft[1] : num [1:10000] 8.15 8.15 8.15 8.15 8.15 ... ## $ ft[2] : num [1:10000] 8.97 8.97 8.97 8.96 8.96 ... ## $ ft[3] : num [1:10000] 8.97 8.97 8.96 8.95 8.95 ... ## $ ft[4] : num [1:10000] 8.69 8.69 8.66 8.66 8.66 ... ## $ ft[5] : num [1:10000] 9.29 9.29 9.19 9.17 9.18 ... ## $ ft[6] : num [1:10000] 8.87 8.87 8.9 8.9 8.9 ... ## $ ft[7] : num [1:10000] 8.95 8.95 8.97 8.96 8.96 ... ## $ ft[8] : num [1:10000] 8.49 8.49 8.87 8.85 8.8 ... ## $ ft[9] : num [1:10000] 8.43 8.43 8.6 8.59 8.57 ... ## $ ft[10] : num [1:10000] 8.69 8.69 8.73 8.72 8.71 ... ## $ ft[11] : num [1:10000] 8.06 8.06 8.21 8.21 8.19 ... ## $ ft[12] : num [1:10000] 7.66 7.65 7.84 7.85 7.81 ... ## $ ft[13] : num [1:10000] 8.18 8.18 8.26 8.26 8.24 ... ## $ ft[14] : num [1:10000] 8.65 8.65 8.51 8.52 8.53 ... ## $ ft[15] : num [1:10000] 8.77 8.77 8.73 8.72 8.73 ... ## $ ft[16] : num [1:10000] 8.13 8.13 8.35 8.33 8.3 ... ## $ ft[17] : num [1:10000] 8.36 8.36 8.52 8.5 8.46 ... ## $ ft[18] : num [1:10000] 8.55 8.55 8.56 8.55 8.54 ... ## $ ft[19] : num [1:10000] 8.49 8.49 8.35 8.36 8.37 ... ## $ ft[20] : num [1:10000] 8.85 8.85 8.75 8.75 8.77 ... ## $ ft[21] : num [1:10000] 9.15 9.15 9.08 9.08 9.09 ... ## $ ft[22] : num [1:10000] 8.93 8.93 8.87 8.88 8.89 ... ## $ ft[23] : num [1:10000] 8.99 8.99 8.71 8.73 8.78 ... ## $ ft[24] : num [1:10000] 9.22 9.23 8.89 8.92 8.98 ... ## $ ft[25] : num [1:10000] 8.82 8.82 8.55 8.58 8.64 ... ## $ ft[26] : num [1:10000] 8.98 8.98 8.71 8.74 8.79 ... ## $ ft[27] : num [1:10000] 9.09 9.09 8.86 8.89 8.94 ... ## $ ft[28] : num [1:10000] 8.79 8.79 8.75 8.77 8.79 ... ## $ ft[29] : num [1:10000] 8.79 8.79 8.77 8.78 8.8 ... ## $ ft[30] : num [1:10000] 8.64 8.64 8.53 8.55 8.58 ... ## $ ft[31] : num [1:10000] 8.33 8.32 8.39 8.4 8.4 ... ## $ ft[32] : num [1:10000] 8.57 8.57 8.56 8.57 8.57 ... ## $ ft[33] : num [1:10000] 8.66 8.66 8.68 8.68 8.69 ... ## $ ft[34] : num [1:10000] 8.85 8.86 8.83 8.85 8.86 ... ## $ ft[35] : num [1:10000] 8.76 8.76 8.71 8.72 8.73 ... ## $ ft[36] : num [1:10000] 8.96 8.96 8.86 8.87 8.9 ... ## $ ft[37] : num [1:10000] 9.02 9.01 8.77 8.79 8.83 ... ## $ ft[38] : num [1:10000] 8.9 8.89 8.66 8.68 8.72 ... ## $ ft[39] : num [1:10000] 8.97 8.97 8.84 8.86 8.89 ... ## $ ft[40] : num [1:10000] 9.06 9.06 8.9 8.93 8.96 ... ## $ ft[41] : num [1:10000] 9.34 9.34 9.14 9.17 9.23 ... ## $ ft[42] : num [1:10000] 8.96 8.96 8.88 8.89 8.92 ... ## $ ft[43] : num [1:10000] 9.05 9.05 9.02 9.03 9.06 ... ## $ ft[44] : num [1:10000] 9.04 9.04 9 9.02 9.04 ... ## $ ft[45] : num [1:10000] 8.99 8.99 8.96 8.97 8.99 ... ## $ ft[46] : num [1:10000] 9.09 9.09 9.13 9.14 9.15 ... ## $ ft[47] : num [1:10000] 9.01 9.01 9.02 9.03 9.04 ... ## $ ft[48] : num [1:10000] 9.36 9.37 9.28 9.3 9.33 ... ## $ ft[49] : num [1:10000] 9.31 9.32 9.25 9.26 9.28 ... ## $ ft[50] : num [1:10000] 9.15 9.15 9 9.01 9.04 ... ## $ ft[51] : num [1:10000] 8.97 8.96 8.82 8.83 8.86 ... ## $ ft[52] : num [1:10000] 8.77 8.76 8.84 8.84 8.83 ... ## $ ft[53] : num [1:10000] 9.1 9.1 9.14 9.14 9.15 ... ## $ ft[54] : num [1:10000] 9.04 9.04 9.03 9.04 9.04 ... ## $ ft[55] : num [1:10000] 9.23 9.23 9.27 9.28 9.28 ... ## $ ft[56] : num [1:10000] 9.28 9.28 9.36 9.36 9.36 ... ## $ ft[57] : num [1:10000] 8.96 8.97 9.06 9.06 9.04 ... ## $ ft[58] : num [1:10000] 8.92 8.91 8.97 8.97 8.96 ... ## $ ft[59] : num [1:10000] 8.98 9 9.05 9.04 9.03 ... ## $ ft[60] : num [1:10000] 9.17 9.17 9.06 9.07 9.08 ... ## $ ft[61] : num [1:10000] 9.01 8.95 8.93 8.94 8.93 ... ## $ ft[62] : num [1:10000] 9.24 9.25 9.03 9.05 9.08 ... ## $ ft[63] : num [1:10000] 9.44 9.46 9.11 9.14 9.2 ... ## $ ft[64] : num [1:10000] 9.61 9.62 9.28 9.31 9.38 ... ## $ ft[65] : num [1:10000] 9.81 9.82 9.39 9.43 9.52 ... ## $ ft[66] : num [1:10000] 9.77 9.77 9.47 9.51 9.58 ... ## $ ft[67] : num [1:10000] 9.75 9.74 9.47 9.51 9.58 ... ## $ ft[68] : num [1:10000] 9.89 9.86 9.51 9.57 9.65 ... ## $ ft[69] : num [1:10000] 9.79 9.79 9.56 9.6 9.66 ... ## $ ft[70] : num [1:10000] 9.6 9.64 9.54 9.56 9.61 ... ## $ ft[71] : num [1:10000] 9.59 9.6 9.57 9.59 9.62 ... ## $ ft[72] : num [1:10000] 9.27 9.27 9.38 9.39 9.39 ... ## $ ft[73] : num [1:10000] 9.32 9.31 9.36 9.37 9.37 ... ## $ ft[74] : num [1:10000] 9.34 9.34 9.32 9.33 9.34 ... ## $ ft[75] : num [1:10000] 8.79 8.79 9 8.99 8.96 ... ## $ ft[76] : num [1:10000] 8.94 8.94 9.16 9.15 9.12 ... ## $ ft[77] : num [1:10000] 8.07 8.04 8.48 8.44 8.37 ... ## $ ft[78] : num [1:10000] 8.49 8.52 8.89 8.86 8.79 ... ## $ ft[79] : num [1:10000] 8.5 8.52 8.88 8.84 8.77 ... ## $ ft[80] : num [1:10000] 8.57 8.6 8.93 8.89 8.82 ... ## $ ft[81] : num [1:10000] 8.5 8.48 8.76 8.72 8.65 ... ## $ ft[82] : num [1:10000] 8.57 8.57 8.75 8.72 8.66 ... ## $ ft[83] : num [1:10000] 8.27 8.27 8.58 8.53 8.46 ... ## $ ft[84] : num [1:10000] 8.23 8.24 8.32 8.3 8.26 ... ## $ ft[85] : num [1:10000] 8.2 8.21 8.16 8.15 8.15 ... ## $ ft[86] : num [1:10000] 8.2 8.2 8.29 8.27 8.24 ... ## $ ft[87] : num [1:10000] 8.46 8.46 8.4 8.39 8.39 ... ## $ ft[88] : num [1:10000] 8.83 8.83 8.59 8.59 8.62 ... ## $ ft[89] : num [1:10000] 8.88 8.86 8.55 8.56 8.6 ... ## $ ft[90] : num [1:10000] 9.12 9.12 8.88 8.9 8.93 ... ## $ ft[91] : num [1:10000] 8.78 8.79 8.66 8.67 8.7 ... ## $ ft[92] : num [1:10000] 8.79 8.8 8.78 8.79 8.79 ... ## $ ft[93] : num [1:10000] 8.71 8.73 8.73 8.73 8.73 ... ## $ ft[94] : num [1:10000] 8.45 8.44 8.49 8.49 8.49 ... ## $ ft[95] : num [1:10000] 8.47 8.47 8.68 8.67 8.64 ... ## $ ft[96] : num [1:10000] 8.37 8.38 8.57 8.55 8.52 ... ## [list output truncated] Cnim_postfit_uoms &lt;- compileNimble(nim_postfit_uoms, showCompilerOutput = FALSE) ## Compiling ## [Note] This may take a minute. ## [Note] Use &#39;showCompilerOutput = TRUE&#39; to see C++ compilation details. post_tau &lt;- tidy_post_samples$tau post_sqrt_Wt_diag &lt;- tidy_post_samples |&gt; select(starts_with(&quot;sqrt_Wt_diag&quot;)) |&gt; as.matrix()|&gt; unname() post_ft_list &lt;- lapply(1:(nchains*npsample), function(i) { post_Vt &lt;- post_tau[i]^2 post_Wt &lt;- nim_diag(post_sqrt_Wt_diag[i,]^2) post_ft &lt;- Cnim_postfit_uoms(yt = dat_list$yt, Ft = dat_list$Ft, Vt = post_Vt, Gt = dat_list$Gt, Wt = post_Wt, m0 = dat_list$m0, C0 = dat_list$C0) post_ft &lt;- data.frame(ft = post_ft) %&gt;% mutate(time = row_number()) return(post_ft) }) tidy_post_ft &lt;- do.call(&quot;rbind&quot;, post_ft_list) str(tidy_post_ft) ## &#39;data.frame&#39;: 2750000 obs. of 2 variables: ## $ ft : num 4.59 11.3 7.04 7.9 7.9 ... ## $ time: int 1 2 3 4 5 6 7 8 9 10 ... head(tidy_post_ft) ## ft time ## 1 4.592059 1 ## 2 11.300782 2 ## 3 7.037173 3 ## 4 7.901925 4 ## 5 7.895742 5 ## 6 10.799002 6 ## posterior summaries of ft head(tidy_post_ft) ## ft time ## 1 4.592059 1 ## 2 11.300782 2 ## 3 7.037173 3 ## 4 7.901925 4 ## 5 7.895742 5 ## 6 10.799002 6 post_sum_ft &lt;- tidy_post_ft %&gt;% group_by(time) %&gt;% summarise(post.mean = mean(ft), post.sd = sd(ft), q2.5 = quantile(ft, prob = 0.025), q50 = quantile(ft, prob = 0.50), q97.5 = quantile(ft, prob = 0.975)) %&gt;% ungroup() str(post_sum_ft) ## tibble [275 × 6] (S3: tbl_df/tbl/data.frame) ## $ time : int [1:275] 1 2 3 4 5 6 7 8 9 10 ... ## $ post.mean: num [1:275] 8.16 8.97 8.97 8.67 9.2 ... ## $ post.sd : num [1:275] 2.268 1.268 1.147 0.791 0.751 ... ## $ q2.5 : Named num [1:275] 3.69 6.43 6.71 7.12 7.7 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;2.5%&quot; &quot;2.5%&quot; &quot;2.5%&quot; &quot;2.5%&quot; ... ## $ q50 : Named num [1:275] 8.16 8.96 8.97 8.66 9.2 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;50%&quot; &quot;50%&quot; &quot;50%&quot; &quot;50%&quot; ... ## $ q97.5 : Named num [1:275] 12.6 11.4 11.2 10.2 10.7 ... ## ..- attr(*, &quot;names&quot;)= chr [1:275] &quot;97.5%&quot; &quot;97.5%&quot; &quot;97.5%&quot; &quot;97.5%&quot; ... post_sum_ft %&gt;% head() ## # A tibble: 6 × 6 ## time post.mean post.sd q2.5 q50 q97.5 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 8.16 2.27 3.69 8.16 12.6 ## 2 2 8.97 1.27 6.43 8.96 11.4 ## 3 3 8.97 1.15 6.71 8.97 11.2 ## 4 4 8.67 0.791 7.12 8.66 10.2 ## 5 5 9.20 0.751 7.70 9.20 10.7 ## 6 6 8.88 0.900 7.13 8.88 10.7 ggplot(data = post_sum_ft, aes(x = time)) + geom_ribbon(aes(ymin = q2.5,ymax = q97.5), fill = &quot;lightgray&quot;) + geom_path(aes(y = post.mean), size = 0.25) + geom_point(aes(y=yt), size = 0.25) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() Trend model # Model specification yt &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(yt) p &lt;- 4 # (intercept, trend, wind, temp) Ft &lt;- array(0, dim = c( p, Tt)) Ft[1,] &lt;- 1 Ft[2,] &lt;- 0 Ft[3,] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[4,] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) Gt[1, 2] &lt;- 1 m0 &lt;- c(mean(yt), 0, 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) const_list &lt;- list(Tt = Tt, p = p) dat_list &lt;- list( yt = yt, Ft = Ft, Gt = Gt, m0 = m0, C0 = C0 ) init_list &lt;- list(tau = 0.01, sqrt_Wt_diag = sqrt(rep(0.1, p))) Rmodel &lt;- nimbleModel( Example11_16_O3_Nimble, constants = const_list, data = dat_list, inits = init_list ) Rmodel$initializeInfo() Rmodel$calculate() Cmodel &lt;- compileNimble(Rmodel, showCompilerOutput = FALSE) conf &lt;- configureMCMC(Rmodel, monitors = c(&quot;tau&quot;, &quot;sqrt_Wt_diag&quot;, &quot;theta&quot;, &quot;ft&quot;)) Rmcmc &lt;- buildMCMC(conf) Cmcmc &lt;- compileNimble( Rmcmc, project = Cmodel, resetFunctions = TRUE, showCompilerOutput = FALSE ) niter &lt;- 10000 nburnin &lt;- 0.5 * niter nthin &lt;- 1 nchains &lt;- 2 start_time &lt;- Sys.time() post_samples &lt;- runMCMC( Cmcmc, niter = niter, nburnin = nburnin, thin = nthin, nchains = nchains, samplesAsCodaMCMC = TRUE ) end_time &lt;- Sys.time() run_time &lt;- end_time - start_time run_time post_summary &lt;- nimSummary(post_samples) tidy_post_samples &lt;- post_samples |&gt; tidy_draws() tidy_post_samples |&gt; dplyr::select( .chain, .iteration, .draw, &#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;, &#39;sqrt_Wt_diag[4]&#39; ) |&gt; gather(vars, value, -.chain, -.iteration, -.draw) |&gt; ggplot(aes(x = .iteration, y = value)) + geom_path(aes(color = factor(.chain)), linewidth = 0.25, show.legend = FALSE) + facet_wrap( ~ vars, scales = &quot;free&quot;, nrow = 2) + theme_classic() + theme(panel.grid = element_blank(), strip.background = element_blank()) post_summary[c(&#39;tau&#39;, &#39;sqrt_Wt_diag[1]&#39;, &#39;sqrt_Wt_diag[2]&#39;, &#39;sqrt_Wt_diag[3]&#39;, &#39;sqrt_Wt_diag[4]&#39;), ] ## post.mean post.sd q2.5 q50 q97.5 f0 n.eff Rhat ## tau 0.546 0.035 0.478 0.545 0.614 1 930.770 1.001 ## sqrt_Wt_diag[1] 0.127 0.053 0.028 0.126 0.237 1 1043.367 1.004 ## sqrt_Wt_diag[2] 0.002 0.002 0.000 0.001 0.007 1 927.014 1.000 ## sqrt_Wt_diag[3] 0.017 0.009 0.004 0.016 0.040 1 1560.133 1.011 ## sqrt_Wt_diag[4] 0.035 0.031 0.001 0.027 0.117 1 1017.542 1.000 post_sum_theta &lt;- as.data.frame(post_summary) |&gt; rownames_to_column() |&gt; filter(str_detect(rowname, &quot;theta&quot;)) |&gt; dplyr::select(rowname, q2.5, q50, q97.5) |&gt; separate(rowname, into = c(&quot;x1&quot;, &quot;x2&quot;), sep = &quot;,&quot;) |&gt; mutate(component = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x1))) |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, x2))) |&gt; dplyr::select(component, time, q2.5, q50, q97.5) ggplot(data = post_sum_theta, aes(x = time)) + geom_ribbon(aes(ymin = q2.5, ymax = q97.5), fill = &quot;lightgray&quot;, alpha = 0.7) + geom_path(aes(y = q50), col = &quot;blue&quot;, linewidth = 0.4) + facet_wrap( ~ component, nrow = 2, scales = &quot;free&quot;, labeller = label_bquote(theta[.(component)]) ) + ylab(&quot;&quot;) + xlab(&quot;Time&quot;) + theme_classic() npsample &lt;- floor((niter - nburnin)/nthin) Cnim_postfit_uoms &lt;- compileNimble(nim_postfit_uoms, showCompilerOutput = FALSE) ## Compiling ## [Note] This may take a minute. ## [Note] Use &#39;showCompilerOutput = TRUE&#39; to see C++ compilation details. post_tau &lt;- tidy_post_samples$tau post_sqrt_Wt_diag &lt;- tidy_post_samples |&gt; select(starts_with(&quot;sqrt_Wt_diag&quot;)) |&gt; as.matrix()|&gt; unname() post_ft_list &lt;- lapply(1:(nchains*npsample), function(i) { post_Vt &lt;- post_tau[i]^2 post_Wt &lt;- nim_diag(post_sqrt_Wt_diag[i,]^2) post_ft &lt;- Cnim_postfit_uoms(yt = dat_list$yt, Ft = dat_list$Ft, Vt = post_Vt, Gt = dat_list$Gt, Wt = post_Wt, m0 = dat_list$m0, C0 = dat_list$C0) post_ft &lt;- data.frame(ft = post_ft) %&gt;% mutate(time = row_number()) return(post_ft) }) tidy_post_ft &lt;- do.call(&quot;rbind&quot;, post_ft_list) ## posterior summaries of ft post_sum_ft &lt;- tidy_post_ft |&gt; group_by(time) |&gt; summarise(post.mean = mean(ft), post.sd = sd(ft), q2.5 = quantile(ft, prob = 0.025), q50 = quantile(ft, prob = 0.50), q97.5 = quantile(ft, prob = 0.975)) |&gt; ungroup() ggplot(data = post_sum_ft, aes(x = time)) + geom_ribbon(aes(ymin = q2.5,ymax = q97.5), fill = &quot;lightgray&quot;) + geom_path(aes(y = post.mean), size = 0.25) + geom_point(aes(y=yt), size = 0.25) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() Stan library(coda) library(ggplot2) library(gridExtra) library(rstan) library(tidyverse) options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) # Load data for RW case UK_ozone &lt;- read.csv(&quot;data/uk_ozone_one_site.csv&quot;) Time-varying level model y &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(y) p &lt;- 3 # (intercept, wind, temp) Ft &lt;- array(0, dim = c(Tt, p)) Ft[, 1] &lt;- 1 Ft[, 2] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[, 3] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) m0 &lt;- c(mean(y), 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) stan_data &lt;- list(T = Tt, p = p, y = y, F = Ft, G = Gt, m0= m0, C0 = C0) functions { // FFBS for DLM with univariate observation equation and univariate system equation array[] vector uoms_ffbs_rng(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T] vector[p] theta; array[T] vector[p] a; array[T] matrix[p,p] R; array[T] vector[p] m; array[T] matrix[p,p] C; // Kalman filtering vector[p] mt = m0; matrix[p,p] Ct = C0; for(i in 1:T){ real ft; real Qt; vector[p] at; matrix[p,p] Rt; vector[p] At; at = G * mt; Rt = G * Ct * G&#39; + W; ft = F[i]&#39; * at; Qt = quad_form(Rt, F[i]) + V; //F[i]&#39; * Rt * F[i] + V; At = Rt * F[i] * inv(Qt); mt = at + At * (y[i] - ft); Ct = Rt - At * Qt * At&#39;; //store for backward sampling a[i] = at; R[i] = Rt; m[i] = mt; C[i] = Ct; } // backward sampling array[T-1] int ind = sort_indices_desc(linspaced_int_array(T-1,1,T-1)); theta[T] = multi_normal_rng(m[T], C[T]); for(i in ind) { matrix[p,p] Bt; vector[p] ht; matrix[p,p] Ht; Bt = C[i] * G&#39; * inverse(R[i+1]); ht = m[i] + Bt * (theta[i+1] - a[i+1]); Ht = C[i] - Bt * R[i+1] * Bt&#39;; theta[i] = multi_normal_rng(ht, Ht); } return theta; } real uoms_dlm_ldensity(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T+1] vector[p] a; array[T+1] matrix[p, p] R; array[T] real lldata; a[1] = m0; R[1] = C0; for (i in 1:T) { real u; real Q; real Qinv; vector[p] A; matrix[p, p] L; u = y[i] - F[i]&#39; * a[i]; Q = quad_form(R[i],F[i]) + V; //F[i]&#39; * R[i] * F[i] + V; Qinv = inv(Q); // A = G * R[i] * F[i] * Qinv; L = G - A * F[i]&#39;; //lldata[i] = -0.5 * (log(2 * pi()) + log(Q) + Qinv*square(u)); lldata[i] = normal_lpdf(u | 0, sqrt(Q)); // univariate a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L&#39; + W; } return sum(lldata); } array[] real uoms_dlm_one_step_ahead_rng(array[] real y, array[] vector F, matrix G, real V, matrix W, vector m0, matrix C0, int T, int p){ array[T] real yfit; array[T+1] vector[p] a; array[T+1] matrix[p, p] R; array[T] real lldata; a[1] = m0; R[1] = C0; for (i in 1:T) { real u; real Q; real Qinv; vector[p] A; matrix[p, p] L; u = y[i] - F[i]&#39; * a[i]; Q = quad_form(R[i],F[i]) + V; //F[i]&#39; * R[i] * F[i] + V; Qinv = inv(Q); // A = G * R[i] * F[i] * Qinv; L = G - A * F[i]&#39;; yfit[i] = normal_rng(F[i]&#39; * a[i], sqrt(Q)); // univariate a[i+1] = G * a[i] + A * u; R[i+1] = G * R[i] * L&#39; + W; } return yfit; } } data{ int T; int p; array[T] real y; array[T] vector[p] F; matrix[p, p] G; vector[p] m0; cov_matrix[p] C0; } parameters{ real&lt;lower=0&gt; tau; vector&lt;lower=0&gt;[p] sqrt_W_diag; } model { real V = square(tau); matrix[p, p] W = diag_matrix(square(sqrt_W_diag)); tau ~ std_normal(); sqrt_W_diag ~ std_normal(); target += uoms_dlm_ldensity(y, F, G, V, W, m0, C0, T, p); } generated quantities{ array[T] vector[p] theta; array[T] real yfit; real V = square(tau); matrix[p, p] W = diag_matrix(square(sqrt_W_diag)); theta = uoms_ffbs_rng(y, F, G, V, W, m0, C0, T, p); yfit = uoms_dlm_one_step_ahead_rng(y, F, G, V, W, m0, C0, T, p); } Example11_16_UK_Stan &lt;- stan( file = &quot;functions/Example11_16_UK.stan&quot;, data = stan_data, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) rstan::traceplot(Example11_16_UK_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;, &quot;theta[1,1]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,1]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,2]&quot;)) rstan::traceplot(Example11_16_UK_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,3]&quot;)) fixedpars_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;))$summary fixedpars_summary ## mean se_mean sd 2.5% 25% ## tau 0.55010626 3.488395e-04 0.033468464 0.484432869 0.52797009 ## sqrt_W_diag[1] 0.11815950 4.656882e-04 0.042928733 0.050865809 0.08646848 ## sqrt_W_diag[2] 0.01689699 8.836672e-05 0.008904249 0.003430261 0.01068365 ## sqrt_W_diag[3] 0.03411268 3.029880e-04 0.030825578 0.001108826 0.01119069 ## 50% 75% 97.5% n_eff Rhat ## tau 0.55045820 0.5723100 0.61472703 9204.927 0.9999293 ## sqrt_W_diag[1] 0.11284435 0.1444072 0.21526998 8497.784 0.9999954 ## sqrt_W_diag[2] 0.01546892 0.0215257 0.03824708 10153.532 0.9999893 ## sqrt_W_diag[3] 0.02521080 0.0478548 0.11696704 10350.745 1.0002863 theta_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;theta&quot;))$summary yfit_summary &lt;- summary(Example11_16_UK_Stan, pars = c(&quot;yfit&quot;))$summary p1 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,1]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date, group = 1)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Intercept&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,2]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Wind&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p3 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,3]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Temperature&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1,p2,p3, ncol = 2) # Fitted values yfit_summary_dt &lt;- data.frame(yfit_summary) |&gt; rownames_to_column() |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(yfit_summary_dt, aes(x = time)) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + geom_point(aes(y = y)) + geom_path(aes(y = mean), linewidth = 0.1, size = 1) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() Trend model y &lt;- sqrt(UK_ozone$ozone) temp &lt;- UK_ozone$temp wind &lt;- UK_ozone$wind Tt &lt;- length(y) p &lt;- 4 # (intercept, trend, wind, temp) Ft &lt;- array(0, dim = c(Tt, p)) Ft[, 1] &lt;- 1 Ft[, 2] &lt;- 0 Ft[, 3] &lt;- (wind[1:Tt] - mean(wind[1:Tt])) / sd(wind[1:Tt]) Ft[, 4] &lt;- (temp[1:Tt] - mean(temp[1:Tt])) / sd(temp[1:Tt]) Gt &lt;- diag(x = 1, nrow = p, ncol = p) Gt[1, 2] &lt;- 1 m0 &lt;- c(mean(y), 0, 0, 0) C0 &lt;- diag(x = 1, nrow = p, ncol = p) stan_data_trend &lt;- list( T = Tt, p = p, y = y, F = Ft, G = Gt, m0 = m0, C0 = C0 ) Example11_16_UK_trend_Stan &lt;- stan( file = &quot;functions/Example11_16_UK.stan&quot;, data = stan_data_trend, warmup = 5000, iter = 10000, chains = 3, include = TRUE ) ## Warning in validityMethod(object): The following variables have undefined ## values: theta[1,1],The following variables have undefined values: theta[2,1],The ## following variables have undefined values: theta[3,1],The following variables ## have undefined values: theta[4,1],The following variables have undefined ## values: theta[5,1],The following variables have undefined values: theta[6,1],The ## following variables have undefined values: theta[7,1],The following variables ## have undefined values: theta[8,1],The following variables have undefined values: ## theta[9,1],The following variables have undefined values: theta[10,1],The ## following variables have undefined values: theta[11,1],The following ## variables have undefined values: theta[12,1],The following variables have ## undefined values: theta[13,1],The following variables have undefined values: ## theta[14,1],The following variables have undefined values: theta[15,1],The ## following variables have undefined values: theta[16,1],The following ## variables have undefined values: theta[17,1],The following variables have ## undefined values: theta[18,1],The following variables have undefined values: ## theta[19,1],The following variables have undefined values: theta[20,1],The ## following variables have undefined values: theta[21,1],The following ## variables have undefined values: theta[22,1],The following variables have ## undefined values: theta[23,1],The following variables have undefined values: ## theta[24,1],The following variables have undefined values: theta[25,1],The ## following variables have undefined values: theta[26,1],The following ## variables have undefined values: theta[27,1],The following variables have ## undefined values: theta[28,1],The following variables have undefined values: ## theta[29,1],The following variables have undefined values: theta[30,1],The ## following variables have undefined values: theta[31,1],The following ## variables have undefined values: theta[32,1],The following variables have ## undefined values: theta[33,1],The following variables have undefined values: ## theta[34,1],The following variables have undefined values: theta[35,1],The ## following variables have undefined values: theta[36,1],The following ## variables have undefined values: theta[37,1],The following variables have ## undefined values: theta[38,1],The following variables have undefined values: ## theta[39,1],The following variables have undefined values: theta[40,1],The ## following variables have undefined values: theta[41,1],The following ## variables have undefined values: theta[42,1],The following variables have ## undefined values: theta[43,1],The following variables have undefined values: ## theta[44,1],The following variables have undefined values: theta[45,1],The ## following variables have undefined values: theta[46,1],The following ## variables have undefined values: theta[47,1],The following variables have ## undefined values: theta[48,1],The following variables have undefined values: ## theta[49,1],The following variables have undefined values: theta[50,1],The ## following variables have undefined values: theta[51,1],The following ## variables have undefined values: theta[52,1],The following variables have ## undefined values: theta[53,1],The following variables have undefined values: ## theta[54,1],The following variables have undefined values: theta[55,1],The ## following variables have undefined values: theta[56,1],The following ## variables have undefined values: theta[57,1],The following variables have ## undefined values: theta[58,1],The following variables have undefined values: ## theta[59,1],The following variables have undefined values: theta[60,1],The ## following variables have undefined values: theta[61,1],The following ## variables have undefined values: theta[62,1],The following variables have ## undefined values: theta[63,1],The following variables have undefined values: ## theta[64,1],The following variables have undefined values: theta[65,1],The ## following variables have undefined values: theta[66,1],The following ## variables have undefined values: theta[67,1],The following variables have ## undefined values: theta[68,1],The following variables have undefined values: ## theta[69,1],The following variables have undefined values: theta[70,1],The ## following variables have undefined values: theta[71,1],The following ## variables have undefined values: theta[72,1],The following variables have ## undefined values: theta[73,1],The following variables have undefined values: ## theta[74,1],The following variables have undefined values: theta[75,1],The ## following variables have undefined values: theta[76,1],The following ## variables have undefined values: theta[77,1],The following variables have ## undefined values: theta[78,1],The following variables have undefined values: ## theta[79,1],The following variables have undefined values: theta[80,1],The ## following variables have undefined values: theta[81,1],The following ## variables have undefined values: theta[82,1],The following variables have ## undefined values: theta[83,1],The following variables have undefined values: ## theta[84,1],The following variables have undefined values: theta[85,1],The ## following variables have undefined values: theta[86,1],The following ## variables have undefined values: theta[87,1],The following variables have ## undefined values: theta[88,1],The following variables have undefined values: ## theta[89,1],The following variables have undefined values: theta[90,1],The ## following variables have undefined values: theta[91,1],The following ## variables have undefined values: theta[92,1],The following variables have ## undefined values: theta[93,1],The following variables have undefined values: ## theta[94,1],The following variables have undefined values: theta[95,1],The ## following variables have undefined values: theta[96,1],The following ## variables have undefined values: theta[97,1],The following variables have ## undefined values: theta[98,1],The following variables have undefined values: ## theta[99,1],The following variables have undefined values: theta[100,1],The ## following variables have undefined values: theta[101,1],The following ## variables have undefined values: theta[102,1],The following variables have ## undefined values: theta[103,1],The following variables have undefined values: ## theta[104,1],The following variables have undefined values: theta[105,1],The ## following variables have undefined values: theta[106,1],The following ## variables have undefined values: theta[107,1],The following variables have ## undefined values: theta[108,1],The following variables have undefined values: ## theta[109,1],The following variables have undefined values: theta[110,1],The ## following variables have undefined values: theta[111,1],The following ## variables have undefined values: theta[112,1],The following variables have ## undefined values: theta[113,1],The following variables have undefined values: ## theta[114,1],The following variables have undefined values: theta[115,1],The ## following variables have undefined values: theta[116,1],The following ## variables have undefined values: theta[117,1],The following variables have ## undefined values: theta[118,1],The following variables have undefined values: ## theta[119,1],The following variables have undefined values: theta[120,1],The ## following variables have undefined values: theta[121,1],The following ## variables have undefined values: theta[122,1],The following variables have ## undefined values: theta[123,1],The following variables have undefined values: ## theta[124,1],The following variables have undefined values: theta[125,1],The ## following variables have undefined values: theta[126,1],The following ## variables have undefined values: theta[127,1],The following variables have ## undefined values: theta[128,1],The following variables have undefined values: ## theta[129,1],The following variables have undefined values: theta[130,1],The ## following variables have undefined values: theta[131,1],The following ## variables have undefined values: theta[132,1],The following variables have ## undefined values: theta[133,1],The following variables have undefined values: ## theta[134,1],The following variables have undefined values: theta[135,1],The ## following variables have undefined values: theta[136,1],Th rstan::traceplot(Example11_16_UK_trend_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;, &quot;theta[1,1]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,1]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,2]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,3]&quot;)) rstan::traceplot(Example11_16_UK_trend_Stan, pars = paste0(&quot;theta[&quot;, sample.int(Tt, size = 4, replace = FALSE), &quot;,4]&quot;)) fixedpars_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;tau&quot;, &quot;sqrt_W_diag&quot;))$summary fixedpars_summary ## mean se_mean sd 2.5% 25% ## tau 0.543642607 3.914308e-04 0.034375997 4.772720e-01 0.5201395929 ## sqrt_W_diag[1] 0.131536090 6.375197e-04 0.051581535 3.457818e-02 0.0958162138 ## sqrt_W_diag[2] 0.002059789 1.722597e-05 0.001898505 7.153768e-05 0.0007138005 ## sqrt_W_diag[3] 0.016773427 8.278251e-05 0.009020480 3.348869e-03 0.0104632621 ## sqrt_W_diag[4] 0.034660473 2.839305e-04 0.030739697 1.150335e-03 0.0117165930 ## 50% 75% 97.5% n_eff Rhat ## tau 0.543274715 0.566861458 0.612240194 7712.597 1.0010600 ## sqrt_W_diag[1] 0.129392438 0.165679335 0.236533778 6546.380 1.0010844 ## sqrt_W_diag[2] 0.001559833 0.002842681 0.007096176 12146.628 1.0006294 ## sqrt_W_diag[3] 0.015253811 0.021381703 0.038668290 11873.590 1.0001375 ## sqrt_W_diag[4] 0.026333330 0.048433147 0.115248048 11721.283 0.9998328 theta_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;theta&quot;))$summary yfit_summary &lt;- summary(Example11_16_UK_trend_Stan, pars = c(&quot;yfit&quot;))$summary p1 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,1]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date, group = 1)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Intercept&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p2 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,4]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Trend&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p3 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,2]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Wind&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) p4 &lt;- data.frame(theta_summary) |&gt; rownames_to_column() |&gt; filter(rowname %in% paste0(&quot;theta[&quot;, 1:Tt, &quot;,3]&quot;)) |&gt; mutate(date = as.Date(UK_ozone$date[1:Tt])) |&gt; ggplot(aes(x = date)) + geom_path(aes(y = mean), color = &quot;blue&quot;, linewidth = 0.6) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;lightgray&quot;) + ggtitle(&quot;Temperature&quot;) + theme_classic() + theme(plot.title = element_text(hjust = 0.5)) grid.arrange(p1,p2,p3, p4, ncol = 2) # Fitted values yfit_summary_dt &lt;- data.frame(yfit_summary) |&gt; rownames_to_column() |&gt; mutate(time = as.numeric(gsub(&quot;.*?([0-9]+).*&quot;, &quot;\\\\1&quot;, rowname))) ggplot(yfit_summary_dt, aes(x = time)) + geom_ribbon(aes(ymin = X2.5., ymax = X97.5.), alpha = 0.25, col = &quot;gray99&quot;) + geom_point(aes(y = y)) + geom_path(aes(y = mean), linewidth = 0.1, size = 1) + ylab(&quot;Ozone Level (ppm)&quot;) + xlab(&quot;Time&quot;) + ggtitle(&quot;Fitted values Ozone level UK&quot;) + theme_classic() Solutions to Selected Exercises Exercise 11.11 The joint distribution of \\(\\mathbf{\\gamma} = \\{\\gamma_1, \\gamma_2, \\dots, \\gamma_{N_T}\\}\\) can be written as: \\[ p(\\mathbf{\\gamma}) = p(\\gamma_1) \\prod_{t=1}^{N_T-1}p(\\gamma_{t+1}\\mid\\gamma)\\\\ = p(\\gamma_1)(2\\pi\\sigma_w^2)^{-(N_T-1)/2}\\exp\\{- \\frac{1}{2\\sigma_w^2} \\sum_{t=1}^{N_T-1} (\\gamma_{t+1} - \\gamma)^2) \\}\\\\ \\propto p(\\gamma_1) \\exp\\{-\\frac{1}{2\\sigma_w^2}\\mathbf{\\gamma}^\\top M \\mathbf{\\gamma}\\}, \\] where the entries of \\(M\\) are: \\[ M_{ij} = \\begin{cases} 1, &amp; i = j = 1, N_T\\\\ 2, &amp; i = j = 2, 3, \\dots, N_T - 1 \\\\ -1 &amp; i = (j-1), \\text{where } j = 2, 3, \\dots, N_T\\\\ -1 &amp; i = (j+1), \\text{where } j = 1, 2, \\dots, N_T - 1 \\\\ 0 &amp; \\text{else} \\end{cases}. \\] Now, if we assume that \\(p(\\gamma_1) \\propto 1\\), i.e., an uninformative, improper prior, we have that \\[ p(\\mathbf{\\gamma}) \\propto \\exp\\{-\\frac{1}{2\\sigma_w^2}\\mathbf{\\gamma}^\\top M \\mathbf{\\gamma}\\}, \\] which implies that, for any \\(t\\), by Bayes rule, \\[ p(\\gamma_t|\\mathbf{\\gamma}_{-t}) = \\frac{p(\\mathbf{\\gamma})}{p(\\mathbf{\\gamma}_{-t})} \\propto p(\\mathbf{\\gamma}). \\] Eliminating terms in the joint distribution \\(P(\\gamma)\\) not related to \\(\\gamma_t\\), we obtain \\[ p(\\gamma_t|\\mathbf{\\gamma}_{-t}) \\propto \\begin{cases} \\exp\\{-\\frac{1}{2\\sigma_w^2}(\\gamma_t^2 - 2\\gamma_t\\gamma_{t+1})\\}, &amp; t = 1\\\\ \\exp\\{-\\frac{1}{2\\sigma_w^2}(2\\gamma_t^2 - 2\\gamma_t\\gamma_{t+1} - 2\\gamma_{t-1}\\gamma_t)\\}, &amp; t = 2, 3, \\dots, N_T-1 \\\\ \\exp\\{-\\frac{1}{2\\sigma_w^2}(\\gamma_t^2 - 2\\gamma_{t-1}\\gamma_{t})\\}, &amp; t = N_T \\end{cases}. \\] The proportional form implies that these are Normal densities. By completing the square, we can obtain \\[ p(\\gamma|\\gamma_{-t}) \\propto \\begin{cases} N(\\gamma_{t+1}, \\sigma_w^2), &amp; t = 1 \\\\ N(\\frac{\\gamma_{t-1}+\\gamma_{t+1}}{2}, \\sigma^2_w/2), &amp; t = 2, 3 \\dots, N_T-1 \\\\ N(\\gamma_{t-1}, \\sigma_w^2), &amp; t = N_T \\end{cases}, \\] as required. A necessary assumption for this conclusion was that \\(p(\\gamma_1) \\propto 1\\). ## Exercise 11.12 {-} The joint density of \\(\\gamma_t\\) and \\(\\tau_w\\), given \\(\\gamma_{t-1}\\) is given by \\[ p(\\gamma_t, \\tau_w \\mid \\gamma_{t-1}) = \\frac{\\tau_w^{1/2}}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{\\tau(\\gamma_t - \\gamma_{t-1})^2}{2} \\right) \\frac{b^a}{\\Gamma(a)}\\tau_w^{a-1}\\exp(-b\\tau_w). \\] Marginalizing \\(\\tau_w\\) gives \\[ p(\\gamma_t \\mid \\gamma_{t-1}) = \\int_{0}^{\\infty} \\frac{\\tau_2^{1/2}}{\\sqrt{2\\pi}}\\exp \\left(-\\frac{\\tau(\\gamma_t - \\gamma_{t-1})^2}{2} \\right) \\frac{b^a}{\\Gamma(a)}\\tau_w^{a-1}\\exp(-b\\tau_w) d\\tau_w \\\\ = \\frac{b^2}{\\Gamma(a)\\sqrt{2\\pi}} \\int_{0}^\\infty \\tau_w^{a-0.5} \\exp \\left(-\\tau_w \\left[ \\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b\\right] \\right) d\\tau_w \\\\ = \\frac{b^a \\Gamma(a+0.5)}{\\gamma(a)\\sqrt{2\\pi} ((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}} \\int_{0}^\\infty \\frac{(\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}{\\Gamma(a+0.5)} \\tau_w^{(a +0.5) - 1} \\exp\\left( -\\tau_w \\left[ \\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b\\right]\\right)\\\\ = \\frac{b^a \\Gamma(a+0.5)}{\\gamma(a)\\sqrt{2\\pi} ((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}, \\] where the last equality follows because the integrand was the density of a Gamma distribution. Now, by Bayes rule, we have the following expression for the posterior: \\[ p(\\tau_{w}|\\gamma_t, \\gamma_{t-1}) = \\frac{p(\\gamma_t, \\tau_w \\mid \\gamma_{t-1})}{p(\\gamma_t \\mid \\gamma_{t-1}) } = \\frac{((\\gamma_t - \\gamma_{t-1})^2/2+b)^{a+0.5}}{\\Gamma(a + 0.5)}\\tau^{(a+0.5)-1} \\exp \\left(-\\tau_w \\left[\\frac{(\\gamma_t - \\gamma_{t-1})^2}{2} + b \\right] \\right), \\] which is the density function of the \\(Ga(a + 0.5, (\\gamma_t - \\gamma_{t-1})^2/2 + b)\\) distribution. The shape parameter of the posterior is the prior shape parameter shifted by \\(0.5\\), and the rate parameter is the prior rate parameter shifted by the term \\((\\gamma_t - \\gamma_{t-1})^2/2\\). Exercise 11.13 We use the rjags package here, as it most closely resembles WinBUGS, but is compatible with all operating systems. library(rjags) ## Warning: package &#39;rjags&#39; was built under R version 4.2.3 library(tidyverse) data &lt;- source(&quot;data/11_13_data.txt&quot;) inits1 &lt;- source(&quot;data/11_13_model1_inits1.txt&quot;) model_ar &lt;- jags.model(file = &quot;data/11_13_model1.txt&quot;, data = data$value, inits = inits1$value, n.chains = 3) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 1410 ## Unobserved stochastic nodes: 1514 ## Total graph size: 2934 ## ## Initializing model update(model_ar, 5000) ## burn in samples_sigmas &lt;- coda.samples(model_ar, variable.names = c( &quot;sigma.v&quot;, &quot;sigma.w&quot; ), n.iter = 100, thin = 2) plot(samples_sigmas, bty = &quot;n&quot;) apply(samples_sigmas[[1]], 2, quantile) ## sigma.v sigma.w ## 0% 0.009788876 0.3944185 ## 25% 0.010050774 0.4348300 ## 50% 0.010248967 0.4414875 ## 75% 0.010388581 0.4527602 ## 100% 0.010778813 0.4778964 samples_gamma &lt;- coda.samples(model_ar, variable.names = c( &quot;gamma&quot;, &quot;y&quot; ), n.iter = 2000, thin = 1) missings &lt;- which(is.na(data$value$y)) nonmissing &lt;- tibble(var = &quot;y&quot;, time = as.double(seq(1, length(data$value$y))), value = data$value$y) |&gt; filter(!(time %in% missings)) plot_df &lt;- data.frame(samples_gamma[[1]]) plot_df &lt;- plot_df |&gt; pivot_longer(cols = everything(), names_to = &quot;variable&quot;) |&gt; separate(variable, c(&quot;var&quot;, &quot;time&quot;)) |&gt; mutate(time = as.numeric(time)) |&gt; filter(!(var == &quot;y&quot; &amp; !(time %in% missings))) |&gt; rbind(nonmissing) |&gt; group_by(var, time) |&gt; summarise(q025 = quantile(value, prob = 0.025), q975 = quantile(value, prob = 0.975), median = median(value))|&gt; mutate(range = q975-q025) ## Warning: Expected 2 pieces. Additional pieces discarded in 5844000 rows [1, 2, ## 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...]. ## `summarise()` has grouped output by &#39;var&#39;. You can override using the `.groups` ## argument. plot_scatter_df &lt;- dplyr::select(plot_df, var, time, median) |&gt; pivot_wider(names_from = var, values_from = median) plot_scatter &lt;- ggplot(plot_scatter_df, aes(x = y, y = gamma)) + geom_point(alpha = 0.5) + theme_classic() plot_scatter plot &lt;- ggplot(filter(plot_df, var == &quot;gamma&quot;), aes(x = time)) + geom_line(aes(y = median), alpha = 0.5) + geom_line(aes(y = q025), alpha = 0.5, linetype = &quot;dashed&quot;) + geom_line(aes(y = q975), alpha = 0.5, linetype = &quot;dashed&quot;) + geom_point(data = filter(plot_df, (var == &quot;y&quot; &amp; time %in% missings)), aes(x = time, y = 0), color = &quot;blue&quot;) + theme_classic() plot "],["exposure.html", "Chapter 12 Exposure assessment-over space and time", " Chapter 12 Exposure assessment-over space and time In this chapter we have seen the many ways in which the time can be added to space in order to characterize random exposure fields. From this chapter, the reader will have gained an understanding of the following topics: Additional power that can be gained in an epidemiological study by combining the contrasts in the process over both time and space while characterizing the stochastic dependencies across both space and time for inferential analysis. Criteria that good approaches to spatio–temporal modelling should satisfy. General strategies for developing such approaches. Separability and non-separability in spatio–temporal models, and how these could be characterize using the Kronecker product of correlation matrices. Examples of the use of spatio–temporal models in modelling environmental exposures. "],["causality.html", "Chapter 13 Causality-roadblocks on the way to it Solutions to Selected Exercises", " Chapter 13 Causality-roadblocks on the way to it This chapter contains a discussion of the differences between causality and association. It also covers specific issues that may be encountered in this area when investigating the effects of environmental hazards on health. From this chapter, the reader will have gained an understanding of the following topics: Issues with causality in observational studies. The Bradford–Hill criteria which are a group of minimal conditions necessary to provide adequate evidence of a causal relationship. Ecological bias which may occur when inferences about the nature of individuals are made using aggregated data. The role of exposure variability in determining the extent of ecological bias. Approaches to acknowledging ecological bias in ecological studies. Concentration and exposure response functions. Models for estimating personal exposures including micro-environments. Solutions to Selected Exercises Question 13.16: Load the Italy and China data, described in Example 13.3, and included in the supplementary Bookdown material into R. The data for this exercise follows that of Exercise 13.3, we can load it as follows. library(tidyverse) library(janitor) vonK_Data &lt;- read_csv(&quot;data/vonK_China_Italy.csv&quot;) # Aggregate these datasets to have just two age ranges---$&lt;70$ and $\\geq 70$. vonK_Data_70 &lt;- mutate( vonK_Data, .keep = &quot;unused&quot;, Age_Range = fct_collapse( `Age group`, Over_70 = c(&quot;70-79&quot;, &quot;80+&quot;), other_level = &quot;Under_70&quot; ) ) # Sum up all death data grouped by age range and country vonK_Data_70 &lt;- vonK_Data_70 |&gt; group_by(Age_Range, Country) |&gt; summarise( Fatalities = sum(`Confirmed fatalities`), Total_Cases = sum(`Confirmed cases`), .groups = &quot;drop_last&quot; ) |&gt; mutate(Survived = Total_Cases - Fatalities, .before = Total_Cases) |&gt; ## Convert from fatalities to survivals arrange(Country) |&gt; ungroup() i): Display a 2 by 2 contingency table of deaths/survivors vs. China/Italy aggregated over all age groups. ## Sum up all deaths by country vonK_Aggregated_Table &lt;- vonK_Data_70 |&gt; dplyr::select(-Age_Range) |&gt; group_by(Country) |&gt; summarise_all(.funs = c(sum)) |&gt; adorn_totals(&quot;row&quot;) Table 13.1: Aggregated Contingency Table Country Fatalities Survived Total_Cases China 1023 43649 44672 Italy 357 7669 8026 Total 1380 51318 52698 ii): Display 2 by 2 contingency tables stratified by the age groups \\(&lt;70\\) and \\(\\geq 70\\). ## Filter by age range and then sum up deaths vonK_Under70_Table &lt;- vonK_Data_70 |&gt; filter(Age_Range == &quot;Under_70&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) vonK_Over70_Table &lt;- vonK_Data_70 |&gt; filter(Age_Range == &quot;Over_70&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) Table 13.2: Under 70 Contingency Table Country Fatalities Survived Total_Cases China 503 38843 39346 Italy 41 4668 4709 Total 544 43511 44055 Table 13.3: Over 70 Contingency Table Country Fatalities Survived Total_Cases China 520 4806 5326 Italy 316 3001 3317 Total 836 7807 8643 iii): Does a Simpson’s disaggregation (SDis) empirically occur in these two tables? Following the notation of the textbook (Example 13.3), let \\(S\\) denote alive and \\(\\bar{S}\\) denote death. Let \\(T\\) and \\(\\bar{T}\\) denote China and Italy. Finally, let \\(C\\) and \\(\\bar{C}\\) denote the binarized age groups under 70 and over 70. Recall that \\[ P_1 = Pr(S|T), \\; P_2 = Pr(S|\\tilde{T}), \\; R_1 = Pr(S|\\bar{C}T), \\\\ R_2 = Pr(S|\\bar{C}\\bar{T}), \\; Q_1 = Pr(S|CT), \\; Q_2 = Pr(S|C\\bar{T}). \\] A Simpson’s disaggregation occurs when \\(P_1 &gt; P_2\\), while \\(R_1 &lt; R_2\\) and \\(Q_1 &lt; Q_2\\). We compute the empirical values denoted \\(\\hat{P}_1, \\hat{P}_2, \\hat{Q}_1, \\hat{Q}_2, \\hat{R}_1, \\hat{R}_2\\): P_hats = head(vonK_Aggregated_Table$Survived / vonK_Aggregated_Table$Total_Cases, -1) Q_hats = head(vonK_Under70_Table$Survived / vonK_Under70_Table$Total_Cases, -1) R_hats = head(vonK_Over70_Table$Survived / vonK_Over70_Table$Total_Cases, -1) Solution &lt;- tibble( index = c(1, 2), P_hat = P_hats, Q_hat = Q_hats, R_hat = R_hats ) knitr::kable(Solution) index P_hat Q_hat R_hat 1 0.9770997 0.9872160 0.9023658 2 0.9555196 0.9912933 0.9047332 Indeed, we see that \\(\\hat{P}_1 = 0.977 &gt; \\hat{P}_2 = 0.956\\), while \\(\\hat{Q}_1 = 0.987 &lt; \\hat{Q}_2 = 0.991\\) and \\(\\hat{R}_1 = 0.902 &lt; \\hat{R}_2 = 0.905\\), hence a Simpson’s disaggregation has occured. Question 13.17: Using the Italy and China data stratified by the age groups \\(&lt;70\\) and \\(\\geq 70\\), obtain empirical estimates of \\(P_1\\) and \\(P_2\\), as defined in an SDis. In the following questions, assume that \\(P_1\\) and \\(P_2\\) are the population values. i): What are the possible ranges of \\(\\gamma_1\\) and \\(\\gamma_2\\) for a controlled SDis to exist (can you visualize it in the rectangle \\([0,1]^2\\))? Keep all intermediate computations to three decimal precision. We compute \\(R\\) as defined in Theorem 2, with \\(P_i = \\hat{P}_i\\). \\[ R = \\frac{P_2/(1-P_2)}{P_1/(1-P_1)} = \\frac{0.956/0.044}{0.977/0.023} = 0.511. \\] The allowable region defined by Theorem 2 is hence: \\[ \\Gamma &lt; 0.511 \\implies \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] One can plug the above inequality into freeware such as (Desmos)[https://www.desmos.com/] or (Wolfram-Alpha)[https://www.wolframalpha.com/] to visualize the region. ii): Suppose a centered interval for the \\(\\gamma\\)’s are desired, i.e., \\(\\gamma_1 = 1 - \\gamma_2\\). What are the allowable values of \\(\\gamma_1\\) and \\(\\gamma_2\\)? Furthermore, what choices (considering three decimal points) yield the tightest interval for \\(\\beta_i\\)? We can re-write the above by substituting \\(\\gamma_1 = 1 - \\gamma_2\\) as \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{(1-\\gamma_2)(1-(1-\\gamma_2))} = \\left(\\frac{\\gamma_2}{1-\\gamma_2}\\right)^2 &lt; 0.511, \\] which, taking only the positive solution of the square root, yields the allowable interval for \\(\\gamma_2\\) \\[ \\gamma_2 &lt; 0.417, \\] implying that \\(\\gamma_1 &gt; 0.583\\). Being conservative up to three decimal points, the smallest interval is hence \\([0.416, 0.584]\\). iii): Compute empirical estimates of \\(\\beta_1\\) and \\(\\beta_2\\), and interpret these values. Recall that \\(\\beta_1 = Pr(T|\\bar{C}), \\beta_2 = Pr(T|C)\\). In our context, an empirical estimate of \\(\\beta_1\\) is hence the proportion of Chinese patients over 70, and \\(\\beta_2\\) the proportion of Chinese patients under 70. beta_hat_1 &lt;- vonK_Over70_Table$Total_Cases[1] / vonK_Over70_Table$Total_Cases[3] beta_hat_2 &lt;- vonK_Under70_Table$Total_Cases[2] / vonK_Over70_Table$Total_Cases[3] beta_hats &lt;- c(beta_hat_1, beta_hat_2) Solution &lt;- Solution |&gt; add_column(beta_hat = beta_hats) knitr::kable(Solution) index P_hat Q_hat R_hat beta_hat 1 0.9770997 0.9872160 0.9023658 0.6162212 2 0.9555196 0.9912933 0.9047332 0.5448340 We obtain \\(\\hat{\\beta}_1 = 0.616\\) and \\(\\hat{\\beta}_2 = 0.545\\). iv) (Bonus): What is the tightest centered interval at \\(\\frac{1}{2}(\\hat{\\beta}_1 - \\hat{\\beta}_2)\\) for \\(\\gamma_1\\) and \\(\\gamma_2\\) such that a controlled SDis can theoretically exist (assuming population parameters)? How well does it cover the estimated \\(\\hat{\\beta}_i\\) (if at all)? Was the empirically observed SDis surprising given these results? We compute the average: mean(c(beta_hat_1, beta_hat_2)) ## [1] 0.5805276 We wish to have the tightest interval centered at \\(0.581\\) such that \\(\\gamma_1, \\gamma_2\\) satisfy \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] We can re-write this as follows. The desired interval is \\([0.581-\\gamma, 0.581+\\gamma]\\), setting \\(\\gamma_2 = 0.581-\\gamma, \\gamma_1 = 0.581+\\gamma\\). Using numerical methods accessed via (Wolfram-Alpha)[https://www.wolframalpha.com/], we obtain the inequality \\[ \\gamma &gt; 0.081 \\] This indicates that \\(\\gamma = 0.080\\) is a boundary value (at 3 decimal points), and hence the tightest interval is \\([0.501, 0.661]\\), which well-covers the estimated \\(\\hat{\\beta}\\). This indicates that perhaps the empirical existence of an SDis is not so surprising. Question 13.18: Stratify the Italy and China data by the age groups \\(&lt;50\\) and \\(\\geq 50\\). Display the corresponding contingency tables. Re-loading the data and re-stratifying: vonK_Data_50 &lt;- mutate( vonK_Data, .keep = &quot;unused&quot;, Age_Range = fct_collapse( `Age group`, Over_50 = c(&quot;50-59&quot;, &quot;60-69&quot;, &quot;70-79&quot;, &quot;80+&quot;), other_level = &quot;Under_50&quot; ) ) i): Does an SDis still occur? Why or why not? We reproduce the contingency tables of Exercise 13.16. vonK_Data_50 &lt;- vonK_Data_50 |&gt; group_by(Age_Range, Country) |&gt; summarise( Fatalities = sum(`Confirmed fatalities`), Total_Cases = sum(`Confirmed cases`), .groups = &quot;drop_last&quot; ) |&gt; mutate(Survived = Total_Cases - Fatalities, .before = Total_Cases) |&gt; arrange(Country) |&gt; ungroup() vonK_Aggregated_Table &lt;- vonK_Data_50 |&gt; dplyr::select(-Age_Range) |&gt; group_by(Country) |&gt; summarise_all(.funs = c(sum)) |&gt; adorn_totals(&quot;row&quot;) vonK_Under50_Table &lt;- vonK_Data_50 |&gt; filter(Age_Range == &quot;Under_50&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) vonK_Over50_Table &lt;- vonK_Data_50 |&gt; filter(Age_Range == &quot;Over_50&quot;) |&gt; dplyr::select(-Age_Range) |&gt; adorn_totals(&quot;row&quot;) Displaying the tables: Table 13.4: Aggregated Contingency Table Country Fatalities Survived Total_Cases China 1023 43649 44672 Italy 357 7669 8026 Total 1380 51318 52698 Table 13.5: Under 50 Contingency Table Country Fatalities Survived Total_Cases China 64 20691 20755 Italy 1 1784 1785 Total 65 22475 22540 Table 13.6: Over 50 Contingency Table Country Fatalities Survived Total_Cases China 959 22958 23917 Italy 356 5885 6241 Total 1315 28843 30158 Now, checking if an SDis still occurs. P_hats &lt;- head(vonK_Aggregated_Table$Survived / vonK_Aggregated_Table$Total_Cases, -1) Q_hats &lt;- head(vonK_Under50_Table$Survived / vonK_Under50_Table$Total_Cases, -1) R_hats &lt;- head(vonK_Over50_Table$Survived / vonK_Over50_Table$Total_Cases, -1) Solution &lt;- tibble( index = c(1, 2), P_hat = P_hats, Q_hat = Q_hats, R_hat = R_hats ) knitr::kable(Solution) index P_hat Q_hat R_hat 1 0.9770997 0.9969164 0.9599030 2 0.9555196 0.9994398 0.9429579 An SDis no longer occurs. \\(\\hat{P}_1 = 0.977 &gt; \\hat{P}_2 = 0.956\\), but \\(\\hat{R}_1 = 0.960 &gt; \\hat{R}_2 = 0.943\\) also. Probabilistically: different definitions of the random variables involved change the probabilities (empirical or population), and an SDis is not at all guaranteed to appear, even under the same dataset. Conceptually: see Figure 1 in Von Kügelen et al. Although each 10-year range results in a higher CFR for China, the aggregated data shows a higher CFR for Italy. This is exactly Simpson’s paradox. The same aggregation phenomenon happens in our binarization, resulting in a smaller version of Simpson’s paradox. By aggregating the bins 50-80+, we have produced an estimate where the Italian CFR is higher than the Chinese, resulting in a similar bin to the “Total” one given in Figure 1. ii): Compute \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\), and repeat the bonus question of Exercise 13.17. What has changed? Do the changes match your observations? Note that because the aggregated data does not change from Exercise 13.17, our estimates for \\(P_1\\) and \\(P_2\\) also remain the same. Therefore, the allowable region for \\(\\gamma_i\\) does not change: \\[ \\frac{\\gamma_2/(1-\\gamma_2)}{\\gamma_1/(1-\\gamma_1)} &lt; 0.511, \\; \\gamma_2 \\leq \\gamma_1. \\] We first compute \\(\\hat{\\beta_i}\\) again. beta_hat_1 &lt;- vonK_Over50_Table$Total_Cases[1] / vonK_Over50_Table$Total_Cases[3] beta_hat_2 &lt;- vonK_Under50_Table$Total_Cases[1] / vonK_Over50_Table$Total_Cases[3] beta_hats &lt;- c(beta_hat_1, beta_hat_2) print(beta_hats) ## [1] 0.7930566 0.6882088 We obtain \\(\\hat{\\beta}_1 = 0.793\\), while \\(\\hat{\\beta}_2 = 0.688\\). We compute their mid-point to be \\(0.741\\). Hence, we look for an interval of the form \\([0.741 - \\gamma, 0.741 + \\gamma]\\). Repeating the exercise, we obtain \\[ \\gamma &gt; 0.063. \\] Taking \\(\\gamma = 0.064\\), this results in the interval \\([0.677, 0.805]\\). our estimated \\(\\hat{\\beta}_i\\) does still fall in this interval, but it is near the boundary values. This perhaps suggests that an SDis was less likely to occur in this stratification (and indeed, it does not occur empirically). "],["monitoring.html", "Chapter 14 Better monitoring network design-getting better measurements", " Chapter 14 Better monitoring network design-getting better measurements This chapter looks at the emergence of a central purpose; to explore or reduce uncertainty about aspects of the environmental processes of interest. One form of uncertainty, aleatory, cannot be reduced by definition whereas with the other, epistemic, where uncertainty can be reduced (see Chapter 3). However that reduction does not stop the original network from becoming sub-optimal over time, pointing to the need to regularly reassess its performance. From that perspective we see that the design criteria must allow for the possibility of ‘gauging’ (adding monitors to) sites that Maximally reduce uncertainty at their space–time points (measuring their responses eliminates their uncertainty); Best minimise uncertainty at other locations; Best inform about process parameters; Best detect non-compliers. From this chapter, the reader will have gained an understanding of many of the challenges that the network designer may face. These involve the following topics: A multiplicity of valid design objectives. Unforeseen and changing objectives. A multiplicity of responses at each site, i.e. which should be monitored. A need to use prior knowledge and to characterise prior uncertainty. A need to formulate realistic process models. A requirement to take advantage of, and integrate with, existing networks. The need to be realistic, meaning to contend with economic as well as administrative demands and constraints. "],["frontiers.html", "Chapter 15 Current frontiers", " Chapter 15 Current frontiers In this chapter the reader will have encountered a selection of new frontiers in spatio– temporal epidemiology including the following: - A number of areas that are currently under active development. - Two modern approaches to addressing the problem of non-stationarity in random spatio– temporal fields; warping and dimension expansion. - How dimension expansion can be used to dramatically reduce non-stationarity and suggest its possible causes. - A powerful approach combining both physical and statistical modelling within a single framework. "],["course.html", "Course", " Course The following is an example of a structure for a course that might be delivered to epidemiologists with an intermediate level of statistics or statistics students who had an interest in epidemiological analysis. Reference is given to the material in the chapters in the book together with suggested times that might be dedicated to that material. Chapter Sections Suggested timing Chapter 1: Why spatio-temporal epidemiology? All 0.5 Week plus background reading Chapter 2: Modelling health risks All excluding 2.6-2.7 1 Week Chapter 3: The importance of uncertainty 3.1-3.4 inclusive 0.5 Week Chapter 4: Embracing uncertainty: the Bayesian approach 4.1-4.5 inclusive 0.5 Week Chapter 5: The Bayesian approach in practice 5.1-5.3 5.7, 5.8 2 Weeks "],["references.html", "References", " References print(sessionInfo()) ## R version 4.2.2 (2022-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 22621) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=English_Canada.utf8 LC_CTYPE=English_Canada.utf8 ## [3] LC_MONETARY=English_Canada.utf8 LC_NUMERIC=C ## [5] LC_TIME=English_Canada.utf8 ## ## attached base packages: ## [1] parallel stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] janitor_2.1.0 rjags_4-13 gridExtra_2.3 ## [4] nleqslv_3.3.3 forecast_8.19 TTR_0.24.3 ## [7] viridis_0.6.2 viridisLite_0.4.1 tidybayes_3.0.2 ## [10] forcats_0.5.2 stringr_1.4.1 dplyr_1.0.10 ## [13] purrr_0.3.5 readr_2.1.3 tidyr_1.2.1 ## [16] tibble_3.1.8 tidyverse_1.3.2 magrittr_2.0.3 ## [19] coda_0.19-4 gstat_2.1-0 cowplot_1.1.1 ## [22] CARBayes_5.3 Rcpp_1.0.9 MASS_7.3-58.1 ## [25] spdep_1.2-7 spData_2.2.0 loo_2.5.1 ## [28] reshape2_1.4.4 geoR_1.9-2 boot_1.3-28 ## [31] mvtnorm_1.1-3 INLA_22.12.16 foreach_1.5.2 ## [34] Matrix_1.5-1 rstan_2.26.13 StanHeaders_2.26.13 ## [37] nimble_0.12.2 sf_1.0-9 rgdal_1.6-2 ## [40] sp_1.5-0 ggmap_3.0.1 ggplot2_3.4.1 ## ## loaded via a namespace (and not attached): ## [1] utf8_1.2.2 tidyselect_1.2.0 htmlwidgets_1.5.4 ## [4] grid_4.2.2 munsell_0.5.0 codetools_0.2-18 ## [7] units_0.8-0 withr_2.5.0 colorspace_2.0-3 ## [10] highr_0.9 knitr_1.40 rstudioapi_0.14 ## [13] stats4_4.2.2 wk_0.7.0 labeling_0.4.2 ## [16] RgoogleMaps_1.4.5.3 splancs_2.01-43 mnormt_2.1.1 ## [19] MCMCpack_1.6-3 bit64_4.0.5 farver_2.1.1 ## [22] vctrs_0.5.0 generics_0.1.3 xfun_0.34 ## [25] R6_2.5.1 fields_14.1 bitops_1.0-7 ## [28] cachem_1.0.6 reshape_0.8.9 assertthat_0.2.1 ## [31] vroom_1.6.0 scales_1.2.1 nnet_7.3-18 ## [34] googlesheets4_1.0.1 gtable_0.3.1 processx_3.8.0 ## [37] mcmc_0.9-7 spam_2.9-1 timeDate_4021.107 ## [40] rlang_1.0.6 MatrixModels_0.5-1 splines_4.2.2 ## [43] gargle_1.2.1 broom_1.0.1 checkmate_2.1.0 ## [46] inline_0.3.19 s2_1.1.0 yaml_2.3.6 ## [49] abind_1.4-5 modelr_0.1.9 crosstalk_1.2.0 ## [52] backports_1.4.1 quantmod_0.4.20 CARBayesdata_3.0 ## [55] tensorA_0.36.2 tools_4.2.2 tcltk_4.2.2 ## [58] bookdown_0.29 ellipsis_0.3.2 jquerylib_0.1.4 ## [61] posterior_1.3.1 RColorBrewer_1.1-3 proxy_0.4-27 ## [64] plyr_1.8.7 classInt_0.4-8 ps_1.7.2 ## [67] prettyunits_1.1.1 deldir_1.0-6 fracdiff_1.5-2 ## [70] zoo_1.8-11 haven_2.5.1 fs_1.5.2 ## [73] ggdist_3.2.0 spacetime_1.2-8 SparseM_1.81 ## [76] lmtest_0.9-40 reprex_2.0.2 googledrive_2.0.0 ## [79] truncnorm_1.0-8 matrixStats_0.62.0 hms_1.1.2 ## [82] evaluate_0.17 arrayhelpers_1.1-0 leaflet_2.1.1 ## [85] jpeg_0.1-9 readxl_1.4.1 compiler_4.2.2 ## [88] maps_3.4.1 KernSmooth_2.23-20 V8_4.2.1 ## [91] crayon_1.5.2 htmltools_0.5.3 tzdb_0.3.0 ## [94] RcppParallel_5.1.5 lubridate_1.8.0 DBI_1.1.3 ## [97] dbplyr_2.2.1 cli_3.4.1 quadprog_1.5-8 ## [100] dotCall64_1.0-2 igraph_1.3.5 pkgconfig_2.0.3 ## [103] sn_2.1.0 numDeriv_2016.8-1.1 xml2_1.3.3 ## [106] svUnit_1.0.6 bslib_0.4.0 rvest_1.0.3 ## [109] snakecase_0.11.0 distributional_0.3.1 callr_3.7.3 ## [112] digest_0.6.30 rmarkdown_2.17 cellranger_1.1.0 ## [115] intervals_0.15.2 curl_4.3.3 urca_1.3-3 ## [118] quantreg_5.94 lifecycle_1.0.3 nlme_3.1-160 ## [121] jsonlite_1.8.3 tseries_0.10-52 fansi_1.0.3 ## [124] pillar_1.8.1 lattice_0.20-45 GGally_2.1.2 ## [127] fastmap_1.1.0 httr_1.4.4 pkgbuild_1.3.1 ## [130] survival_3.4-0 glue_1.6.2 xts_0.12.2 ## [133] FNN_1.1.3.1 png_0.1-7 iterators_1.0.14 ## [136] bit_4.0.4 class_7.3-20 stringi_1.7.8 ## [139] sass_0.4.2 e1071_1.7-12 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
